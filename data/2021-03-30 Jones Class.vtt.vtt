WEBVTT

1
00:00:02.340 --> 00:00:02.700
MS Analytics Class 2021: yeah.

2
00:00:09.179 --> 00:00:15.450
Capstone Coach 1: Okay, I went to Austin for this last weekend was the first time in many months normally go there.

3
00:00:16.590 --> 00:00:28.920
Capstone Coach 1: Just you know hang out in the restaurants, what have you, I was surprised that how many people how many the restaurants were closed and the hotels and everything it was just.

4
00:00:30.120 --> 00:00:33.150
Capstone Coach 1: sort of a different different atmosphere than right around local.

5
00:00:36.150 --> 00:00:39.720
Capstone Coach 1: But I understand, most of the students on campus.

6
00:00:44.010 --> 00:00:44.910
Capstone Coach 1: somebody's talking.

7
00:00:50.370 --> 00:00:50.850
Capstone Coach 1: Okay.

8
00:00:53.100 --> 00:00:57.120
Capstone Coach 1: All right, we're going to get started here the.

9
00:00:58.800 --> 00:01:00.270
Capstone Coach 1: i'm going to share my screen.

10
00:01:01.740 --> 00:01:03.300
Capstone Coach 1: Try to okay.

11
00:01:05.670 --> 00:01:06.300
Capstone Coach 1: All right.

12
00:01:07.080 --> 00:01:08.730
MS Analytics Class 2021: Let me know if it doesn't work that the Jones.

13
00:01:09.270 --> 00:01:10.740
Capstone Coach 1: You can all do that.

14
00:01:11.850 --> 00:01:14.250
Capstone Coach 1: You think by now i'd like to get the hang of this right.

15
00:01:17.220 --> 00:01:17.640
Okay.

16
00:01:20.370 --> 00:01:22.380
Capstone Coach 1: All right, do you do you see my screen anyone.

17
00:01:24.750 --> 00:01:25.260
Swanson J, Charles: Yes, sir.

18
00:01:25.440 --> 00:01:27.000
Fireman, Karen: Yes, we did rice, we see it.

19
00:01:27.810 --> 00:01:29.100
Capstone Coach 1: Good okay good.

20
00:01:31.980 --> 00:01:34.980
Capstone Coach 1: Okay, I guess i'll just hide that for my.

21
00:01:36.420 --> 00:01:38.520
Capstone Coach 1: Probably hide this that's.

22
00:01:43.860 --> 00:01:52.680
Capstone Coach 1: Right okay so as usual we're going to start with, by going to the campus see if I can find that here.

23
00:01:54.030 --> 00:01:54.690
Capstone Coach 1: Here it is.

24
00:01:57.840 --> 00:01:58.110
Capstone Coach 1: Oh.

25
00:01:59.730 --> 00:02:00.240
Capstone Coach 1: OK.

26
00:02:04.620 --> 00:02:08.850
Capstone Coach 1: So if you'll go to the week number 11 on e campus.

27
00:02:11.280 --> 00:02:13.020
Capstone Coach 1: The last day of March.

28
00:02:14.070 --> 00:02:16.860
Capstone Coach 1: And there's not a lot there but it's pretty.

29
00:02:19.170 --> 00:02:26.970
Capstone Coach 1: involved we're going to be talking about this tonight so tonight we're talking about neural networks, and in particular we're talking about.

30
00:02:27.390 --> 00:02:44.430
Capstone Coach 1: How to arrange your networks or how to use them to process images and some of you may have seen this before and it will be looking after a couple of different new types of neural networks, the neural networks that we've been looking at in psychic learn.

31
00:02:45.660 --> 00:02:48.390
Capstone Coach 1: have been used to basically.

32
00:02:49.710 --> 00:02:58.950
Capstone Coach 1: Do forecasting and classification, for you know structured models and where you have a target that you're trying to forecast.

33
00:02:59.700 --> 00:03:09.450
Capstone Coach 1: Those are called nlp neural networks that's the the word that's commonly used now I see psychic learn uses nlp.

34
00:03:10.110 --> 00:03:19.920
Capstone Coach 1: and tense or charisse uses nlp as their jargon it stands for multi layer precept on network.

35
00:03:20.700 --> 00:03:31.170
Capstone Coach 1: There are two other major categories of networks and we'll talk about those here in just a minute so i'm going to start by opening up this PDF file, which is the slides for tonight.

36
00:03:31.710 --> 00:03:44.850
Capstone Coach 1: And we're going to go through that and then, as we get down through that we're going to open up these Python files that you see here, these are code files and we'll be working through that.

37
00:03:46.110 --> 00:04:01.050
Capstone Coach 1: You know, making some alterations to see how they perform, and this sort of thing, so if you could get a chance, please download these code files, the dot P, why files or you may also want to download the other two as well.

38
00:04:02.100 --> 00:04:04.260
Capstone Coach 1: Right and so.

39
00:04:05.700 --> 00:04:07.530
Capstone Coach 1: We get this going.

40
00:04:17.640 --> 00:04:18.780
Capstone Coach 1: Okay, there we go.

41
00:04:21.090 --> 00:04:21.690
Capstone Coach 1: Okay.

42
00:04:24.330 --> 00:04:34.230
Capstone Coach 1: So some of some of these slides are going to be a review of what we did in 656 last summer last summer You might recall, we had one lecture.

43
00:04:34.800 --> 00:04:43.710
Capstone Coach 1: I mean the middle of the Semester on neural networks we didn't really spend a whole lot of time on that, but we're going to go back and look at this we're going to look at.

44
00:04:44.130 --> 00:04:52.260
Capstone Coach 1: What is the neural network again just to refresh your memory what it looks like and then we're going to talk about tensorflow and charisse and how they.

45
00:04:52.680 --> 00:04:59.850
Capstone Coach 1: They operate on on network will also talk about psychic learn very early on how they work within that work.

46
00:05:00.810 --> 00:05:11.070
Capstone Coach 1: They will have a what I call a code break where we're going to bring some code in that you saw and we're going to look at that and go through that code and talk about it and.

47
00:05:11.700 --> 00:05:30.150
Capstone Coach 1: manipulated and change it, and you know that sort of thing right now there are no bugs in that code, I think, so if you do have, if you do get a temptation to get ahead, you should be able to run that code without run into a wall, and if you if you do run into a wall, let me know.

48
00:05:31.650 --> 00:05:41.670
Capstone Coach 1: You do need to have you know, like the tensorflow and charisse downloaded inside of your your spider environment, to make sure that's there.

49
00:05:42.540 --> 00:05:51.120
Capstone Coach 1: we're going to talk about how the activation is used in neural networks and, in particular, how it's used with respect to deep learning.

50
00:05:51.900 --> 00:05:59.190
Capstone Coach 1: The the phrase deep learning really refers to the use of neural networks in a very large sets.

51
00:05:59.610 --> 00:06:09.210
Capstone Coach 1: Deep learning is basically involves training some complicated newer networks large girl that works to perform some activities that.

52
00:06:09.810 --> 00:06:16.200
Capstone Coach 1: That can be pretty extraordinary like image recognition and text recognition and things like that.

53
00:06:17.130 --> 00:06:32.340
Capstone Coach 1: email we're going to talk about we're going to embed look at this and very much steep Hill, this is the Multi layer precept run that work that you're familiar with with to hidden layers and a binary target and we'll talk about how that's done charisse again.

54
00:06:34.770 --> 00:06:35.370
Capstone Coach 1: Okay.

55
00:06:36.000 --> 00:06:48.480
Capstone Coach 1: And I keep thinking like to play like i'm one PowerPoint Okay, so today there are basically three different types of deep learning and neural networks and these three are.

56
00:06:49.020 --> 00:07:00.750
Capstone Coach 1: promoted by charisse tensorflow the psychic learn people they're trying to catch up, I think, but they of course psychic learn has a very good repertoire of the first one.

57
00:07:01.170 --> 00:07:19.650
Capstone Coach 1: The first one is called a multi layer precept on network, these are also called fast forward neural networks, they are often used to model supervised targets it's forecasting of interval targets or classification of binary categorical nominal targets.

58
00:07:20.700 --> 00:07:37.800
Capstone Coach 1: The other two convolution neural networks you're going to get familiar with tonight convolution neural network is primarily used for processing images and normally it's not unusual for image processing to use both a CNN.

59
00:07:39.150 --> 00:07:48.420
Capstone Coach 1: convolution neural network, together with our ordinary a multi layer preceptor on network stack them one, on the other.

60
00:07:49.260 --> 00:07:57.270
Capstone Coach 1: The last one recurrent neural network is a neural network that was originally developed for handling time series data.

61
00:07:57.990 --> 00:08:10.050
Capstone Coach 1: And so it has a unique capability and that capability is that it can actually take the error that it sees in the output.

62
00:08:10.590 --> 00:08:27.330
Capstone Coach 1: As it's running it'll look at the predictive value versus the actual value and it'll see the difference there and feed that back into the front of the network, now that has a very special name and time series and time series, you know it's called.

63
00:08:28.890 --> 00:08:42.000
Capstone Coach 1: not moving average, but the feedback loop on a neural network and it turns out that this feedback loop for neural networks is very important for certain types of data.

64
00:08:42.420 --> 00:08:51.480
Capstone Coach 1: And it works, not just for neural networks, but also, not just for time series data but also for language, for example.

65
00:08:52.080 --> 00:08:57.420
Capstone Coach 1: One of the things that you're going to see you see you're seeing more and more of in language translations is.

66
00:08:57.780 --> 00:09:09.090
Capstone Coach 1: The ability of the computer to look ahead as to what is the next word or what is the next group of words, going to be and that helps the gun the cube computer do a better job with the translation.

67
00:09:09.900 --> 00:09:14.280
Capstone Coach 1: it's one thing, just to translate everything, word for word, according to the dictionary.

68
00:09:14.730 --> 00:09:20.370
Capstone Coach 1: You might get in the ballpark but often you don't because different words have multiple meanings.

69
00:09:20.760 --> 00:09:34.080
Capstone Coach 1: And so to really get the translation accurate, you have to think about the context, what came before and what's likely to come after and recurrent neural network allows you to do that a little bit more sophisticated way.

70
00:09:36.630 --> 00:09:37.350
Capstone Coach 1: Okay.

71
00:09:39.480 --> 00:09:49.200
Capstone Coach 1: Now you've seen this, this is the mcculloch pips per sub tron this came in to being in the 1943 there was a paper that was written about.

72
00:09:49.830 --> 00:10:02.310
Capstone Coach 1: Mathematical bio physics and the the intention here was to try to come up with a way to model, the human brain, the way the brain thinks mathematically, and this is what they came up with and.

73
00:10:02.880 --> 00:10:16.500
Capstone Coach 1: It was kind of a nice idea that you have a series of inputs that are coming into stimulate stimulate the brain or your thinking process those inputs inputs are.

74
00:10:17.220 --> 00:10:27.270
Capstone Coach 1: The Goethe become then a linear potential with, that is to say you take the inputs and you add them together with weights we multiply each one by weight.

75
00:10:27.900 --> 00:10:38.640
Capstone Coach 1: So you can think of each of these lines as being a separate wait, the input is going to be multiplied by that weight and then averaged added together.

76
00:10:39.000 --> 00:10:44.430
Capstone Coach 1: And that's called the see here or the formal word board is linear potential.

77
00:10:45.210 --> 00:10:59.760
Capstone Coach 1: And then some ways it's just like a regression, the statement, so you can think of Z as the regression it's a statement or model that's developed from the inputs, in conjunction with the weights that are assigned to the each input.

78
00:11:00.240 --> 00:11:09.660
Capstone Coach 1: And then there is an intercept you see it here it's labeled wc row, and these templates are what's calculated in a neural network.

79
00:11:10.020 --> 00:11:26.160
Capstone Coach 1: When you fit a neural network to data those weights, then, are are initialized but then they are improved, they are estimated to their to their values are changed to provide a better fit or a better forecast from the model.

80
00:11:27.240 --> 00:11:35.940
Capstone Coach 1: Now, the way the neural network works, how you know is that it, it has a series of perceptions, represented here by this bubble.

81
00:11:36.480 --> 00:11:40.530
Capstone Coach 1: And what happens is when the linear potential comes into that bubble.

82
00:11:41.340 --> 00:11:51.360
Capstone Coach 1: The bubble and call the perception or neuron some people call it a neuron it will fire off a one or a zero depending upon the value of that potential.

83
00:11:52.320 --> 00:12:03.900
Capstone Coach 1: This is a binary precept tron, and this is what this was invented back in 1943 So you can see the output here becomes a zero or one.

84
00:12:04.440 --> 00:12:19.620
Capstone Coach 1: The step function is called the activation functions, so you can see, this one is fairly simple it says the activation is going to be zero if the value of Z is less than some number here in the middle.

85
00:12:20.280 --> 00:12:28.830
Capstone Coach 1: And if it's greater than that number than the activation is going to be one that will be the value that is actually produced this and output from the sky.

86
00:12:29.640 --> 00:12:40.650
Capstone Coach 1: So the idea here is that you have a whole bunch of these in a neural network and they're all interconnected in some way in a multi layer preceptor on network, they are always.

87
00:12:41.610 --> 00:12:52.140
Capstone Coach 1: forward looking and backward looking each precept tron receives inputs, as you see here and then outputs that go to the next layer of neurons.

88
00:12:55.050 --> 00:13:03.870
Capstone Coach 1: And so nlp the email P network that we've been looking at for some time is characterized by several things number one.

89
00:13:04.350 --> 00:13:16.260
Capstone Coach 1: It is almost always associated with the supervisor target the target is either an interval variable you want a podcast or a criteria categorical variable that you want to classify.

90
00:13:17.550 --> 00:13:31.500
Capstone Coach 1: It always has layers, and these are the hidden layers and the output layer and the input layer the input layer contains basically all of the features that you have in your data that you want to use to build this model.

91
00:13:32.010 --> 00:13:50.430
Capstone Coach 1: The number of inputs can be in the hundreds or thousands and so these neural network, it has the capability that especially once it is more fit to the data it can produce sophisticated responses sophisticated forecasts.

92
00:13:51.540 --> 00:14:05.190
Capstone Coach 1: Each the input layer is then followed by a hidden layer and that hidden layer maybe alone or it may be multiple hidden layers and then in the very back in the very end, you have this output layer

93
00:14:06.210 --> 00:14:21.840
Capstone Coach 1: Every layer is indicated connected to the previous and the following layers that's called fast forward connections fast forward means the you know the signal comes into the layer and then it it's immediately forward down to the next layer

94
00:14:22.860 --> 00:14:33.570
Capstone Coach 1: The word dense is used to acknowledge the fact that usually all precept tron so connected to all of the previous ones, and all of the ones in the following layer

95
00:14:34.110 --> 00:14:54.390
Capstone Coach 1: that's where dense mathematically and conceptually you don't have to have a dense random I you know neural network, the neurons don't have to be connected to everything else around them, but by default, in most cases, that is, the arrangement and it's referred to as a dense dense network.

96
00:14:56.670 --> 00:15:07.350
Capstone Coach 1: let's see what we have here, this is a an example, so we have three layers one input layer one hidden layer and then whoops what happened okay.

97
00:15:08.550 --> 00:15:08.880
Capstone Coach 1: Oh.

98
00:15:10.230 --> 00:15:16.650
Capstone Coach 1: There we go and then one output layer now the output layer is just a single neuron you can see, by itself.

99
00:15:17.760 --> 00:15:34.710
Capstone Coach 1: The input so i've got four inputs X one through X four and then i've got 4% 3% trans H one, two and three, there is the output layer that little blue circle is also a perception but that's in the output layer

100
00:15:35.760 --> 00:15:41.100
Capstone Coach 1: Now, this can be used to represent two basic kinds of data one.

101
00:15:42.180 --> 00:15:50.760
Capstone Coach 1: It if the output here, it could be an interval variable so it could be like a regression model where you're trying to forecast something.

102
00:15:51.360 --> 00:16:04.620
Capstone Coach 1: In a regression model, the difference between the regression model is that because of these interconnections this bottle is highly can be highly nonlinear.

103
00:16:05.280 --> 00:16:13.530
Capstone Coach 1: If the activation function in here is basically linear, then this is going to look like an ordinary regression model.

104
00:16:14.310 --> 00:16:25.260
Capstone Coach 1: because all of the weight, so all of the inputs rather are being multiplied by times coefficients and then added together so simple linear addition.

105
00:16:25.890 --> 00:16:38.730
Capstone Coach 1: So that means that conceptually if the weights are good, you should get the exact same answers you'd get with ordinary regression if you're if you're looking at minimizing some of squares and.

106
00:16:39.360 --> 00:16:47.220
Capstone Coach 1: The neural network would that will do a does operate on that basis when you have a you know, an output that's interval.

107
00:16:48.240 --> 00:17:03.240
Capstone Coach 1: So it could, if you wanted it to just mimic an ordinary regression model but that's not what we normally do so, what we normally do is we add to the activation in these.

108
00:17:03.780 --> 00:17:18.420
Capstone Coach 1: perceptions and we add that activation is a non linear activation which means, then, that this guy can fit not just straight lines, but it can also do a good job of fitting.

109
00:17:18.990 --> 00:17:29.310
Capstone Coach 1: Multi dimensional nonlinear arrangements for the opposite, so you would expect it then to do better in terms of the.

110
00:17:29.850 --> 00:17:41.850
Capstone Coach 1: sons of squares of residuals the R squared and things like that that doesn't always happen and the unfortunate thing is that if you're not careful, with the way the magnetic work is.

111
00:17:42.810 --> 00:17:55.380
Capstone Coach 1: constructed, you have a couple of problems that you may run into one is overfitting I neural network can can re read reproduce the training data exactly.

112
00:17:55.890 --> 00:18:08.280
Capstone Coach 1: Most of the time, and so that means it's a prime candidate for overfitting the you fit the training data perfectly, but then, then the validation did, and not so good that's overfitting.

113
00:18:10.410 --> 00:18:25.170
Capstone Coach 1: So that's one problem, the other problem is the actual optimization or the calculating the weights estimating the weights that go into this model is a little bit computation well, it is not a little bit it's computationally complicated.

114
00:18:25.770 --> 00:18:35.100
Capstone Coach 1: And sometimes the something like psychic learn or some of the software may not do a good job at actually estimating the weights.

115
00:18:35.580 --> 00:18:45.720
Capstone Coach 1: But this so this puts a burden on the person who's writing the code is running this to make sure that they have looked at the optimizer.

116
00:18:46.230 --> 00:18:58.320
Capstone Coach 1: All of these neural networks have what's called an optimizer, which is an admission a mathematical algorithm for estimating those weights finding the best weights, to give you a better fit.

117
00:18:58.830 --> 00:19:07.230
Capstone Coach 1: And there are about a half a dozen or more optimized and charisse and the same number and psychic learn as well.

118
00:19:07.620 --> 00:19:14.070
Capstone Coach 1: So you have to be careful to choose a an optimizer that works well with the data, you have and the network.

119
00:19:14.430 --> 00:19:26.970
Capstone Coach 1: You have to be careful to set up the network, so that the calculation of the weights is not too complicated, you have to be careful to look for overfitting and so forth, and so on.

120
00:19:27.780 --> 00:19:36.780
Capstone Coach 1: Now, with psychic learn honestly it's all it's all up to you, they just give you the algorithm they give you a you know the code.

121
00:19:37.710 --> 00:19:48.960
Capstone Coach 1: You pluck this plug everything in and you'll get a fit to your neural network now then it's up to you to see whether or not that fit is worth anything or not, whether it's valuable.

122
00:19:49.530 --> 00:20:11.190
Capstone Coach 1: Meaningful like anything now with charisse it's a little bit different it does what's called deep learning they refer to it as deep learning the number of weights are usually huge and the amount of time that it takes to fit the network can be huge as well, depending upon the configuration.

123
00:20:14.250 --> 00:20:23.010
Capstone Coach 1: And this is this is psychic learn, and these are the inputs that input or input know import import statements.

124
00:20:24.690 --> 00:20:36.300
Capstone Coach 1: Import statements that psychic learn uses and so you have one here SK alert neural network import him LP regress or.

125
00:20:36.900 --> 00:20:51.120
Capstone Coach 1: Multi layer precept precept tron regress R is for basically forecasting interval variables, so this guy is used anywhere, you would ordinarily use linear regression.

126
00:20:51.510 --> 00:21:03.570
Capstone Coach 1: or where you can use linear or not only regression, for that matter, you could use him LP regress or as your model to drive your model.

127
00:21:04.260 --> 00:21:13.500
Capstone Coach 1: This, of course, would also be competitive to decision trees and random forest that will also fit a you know interval variable.

128
00:21:14.070 --> 00:21:34.680
Capstone Coach 1: The second one that nlp classifier is for binary and nominal categorical data, and it does a great job of that so if they've separated these two because the mathematics behind the two are very different and we'll look at that here, in just a minute.

129
00:21:36.540 --> 00:21:54.810
Capstone Coach 1: So if you were going to actually use psychic learn you'll want to do some one hot encoding normally on your your nominal variables and you can use advanced analytics replacing puting code for that and it nominal encoding is is an option there.

130
00:21:56.130 --> 00:22:05.280
Capstone Coach 1: Basically what you're doing is you're taking each nominal variable and if that nominal variable has five levels you're going to create five new variables.

131
00:22:05.760 --> 00:22:20.730
Capstone Coach 1: binary zeros each one of them to zero, and one which represents each of the five levels that are in that nominal variable once you make that encoding you normally drop or get rid of the nominal variable of course.

132
00:22:23.460 --> 00:22:26.640
Capstone Coach 1: Now this is the pre processing I think i've just.

133
00:22:27.930 --> 00:22:35.220
Capstone Coach 1: talked about that this is the actual model statement and psychic learn, so you would say.

134
00:22:36.540 --> 00:22:54.690
Capstone Coach 1: That stored neural network FN n is equal to nlp regress or so once you when you see male P regress are you knew right off that the target is interval the hidden layer now, this is one hidden layer is five as 5% tron Center.

135
00:22:55.920 --> 00:23:12.180
Capstone Coach 1: You could write this for two or three hidden layers if you had to hidden layers you just say five, and that would be the number in the first layer comma, and then you give it another number, like, for that would be the number of precept transcend the second layer

136
00:23:13.470 --> 00:23:24.030
Capstone Coach 1: The solver the optimizer is lb fgs, which is one of the oldest ones, been around forever and, by the way, and psychic learn it does a fabulous job.

137
00:23:24.510 --> 00:23:32.400
Capstone Coach 1: And normally, this is the one that I use, at least, to start with, and then, if I have trouble I may switch over to something else.

138
00:23:33.300 --> 00:23:47.820
Capstone Coach 1: activation you've got several options there we're going to talk about that tonight, one of them is hyperbolic tangent and that's the default and SAS it's very popular often used also here with a psychic learn.

139
00:23:49.470 --> 00:24:01.470
Capstone Coach 1: The maximum number of iterations is how many the optimizer here lb fgs is going to be pushing that pushing that front of that.

140
00:24:01.950 --> 00:24:16.920
Capstone Coach 1: Of that function trying to get the perfect fit to all of the data and it will make minutia changes small changes in the weeds to try to make that to get 100% classification or perfect accuracy.

141
00:24:18.030 --> 00:24:27.420
Capstone Coach 1: Well, at some point, you have to tell the optimizer that that's enough for tonight, thank you and this Max iteration control slap.

142
00:24:27.810 --> 00:24:38.160
Capstone Coach 1: If the number of the durations the number of attempts to change the weights to move the weights towards the optimum exceeds 1000 it shuts down quits gives you the output.

143
00:24:39.540 --> 00:24:48.510
Capstone Coach 1: And now on it gives you the output it doesn't tell you that there will Oh, by the way, I hit the maximum number of iterations.

144
00:24:49.110 --> 00:24:56.130
Capstone Coach 1: So be careful there I always you always have always a good idea to check to see how many iterations were actually used.

145
00:24:56.550 --> 00:25:10.680
Capstone Coach 1: And if it's 1000 then maybe you want to rerun it with a larger number or you might want to look at a different may a different structure for the network apparently it's having problems finding the best value for the weights.

146
00:25:11.940 --> 00:25:20.070
Capstone Coach 1: The last one random state well, will you know what that's about that just about having the ability to reproduce your solution.

147
00:25:21.180 --> 00:25:39.960
Capstone Coach 1: as often as you want so as long as this is set, then your next solution with the same conditions and the same data will be exactly the same if you don't have this in your call to you know psychic learn then every solution is going to be different.

148
00:25:41.130 --> 00:25:53.070
Capstone Coach 1: And finally, the once you describe what you want the network to look like here, then we call the optimizer by using the fit function happen in or model dot fit.

149
00:25:53.550 --> 00:26:05.010
Capstone Coach 1: X, of course, of the features it's a some kind of a lumpy matrix usually or data frame and why is just usually it's a column, and although it can be another matrix as well.

150
00:26:06.750 --> 00:26:18.120
Capstone Coach 1: display metrics as a function that's in advanced advanced analytics and what it does, is it gives you this, the classical metrics for.

151
00:26:18.630 --> 00:26:32.640
Capstone Coach 1: This fit that would be the you know, in this case we have an interval variable so it's going to give you the side, the average me squared error, the square root of that and some other information that you normally see.

152
00:26:35.940 --> 00:26:45.480
Capstone Coach 1: Now binary classification, it looks the same but it's not So here we have nlp classifier instead of internal P regress or.

153
00:26:46.320 --> 00:26:57.750
Capstone Coach 1: The hidden layers, though, are still described in the same way we work, we have a single hidden layer, which is what we have here with five neurons.

154
00:26:58.260 --> 00:27:09.930
Capstone Coach 1: And we just have the five and then, if you put another number after that then you're describing is to layer or three later, or what have you in your network notice the optimizer is the same.

155
00:27:13.440 --> 00:27:21.960
Capstone Coach 1: activation is hyperbolic tangent again so everything looks the same, but because it says classifier here it's not the same.

156
00:27:22.560 --> 00:27:30.090
Capstone Coach 1: The behavior of the optimizer is are going to be a little better tuned for the fact that they have a target.

157
00:27:30.450 --> 00:27:38.670
Capstone Coach 1: And in the case of a classifier what we're actually calculating in this model is the probability.

158
00:27:39.030 --> 00:27:56.520
Capstone Coach 1: That each have a certain states or if it's binary you get the probability that this particular case is a one, one being the event that you're trying to you know forecast, if you have multi normal data, more than one output.

159
00:27:57.420 --> 00:28:04.500
Capstone Coach 1: But you have categorical data, then you would get the probability for each level of that categorical variable.

160
00:28:07.560 --> 00:28:20.100
Capstone Coach 1: Okay, then, of course, you have the metrics, so this is basically from a coding boot point, the only thing that's different here is the name of the function that you're calling in SK learn.

161
00:28:21.780 --> 00:28:32.130
Capstone Coach 1: Now, this is a case where you have nominal targets, we are going to be looking at nominal targets, most of what we're going to do tonight is normal targets.

162
00:28:32.460 --> 00:28:43.560
Capstone Coach 1: Why because we're going to be looking at images and those images then have many different classifications, and the example we're going to look at, for example, for instance.

163
00:28:44.070 --> 00:28:58.260
Capstone Coach 1: These are photographs of handwritten numbers zero through nine and we'll be taking a look at how deep learning structures, the network in order to predict or to identify what what numbers in the photograph.

164
00:28:59.280 --> 00:29:08.070
Capstone Coach 1: Now, this is a very simple example but it's a good one, to start with, because the network is not overly burdensome and you'll see why.

165
00:29:09.540 --> 00:29:13.620
Capstone Coach 1: Even at this level, though it's it's kind of can take some time.

166
00:29:15.720 --> 00:29:24.510
Capstone Coach 1: Right So here we have a this This, I think I would call this a one hidden layer nominal.

167
00:29:25.560 --> 00:29:43.980
Capstone Coach 1: Next network and particular one hidden layer nlp network nlp is identification that oh everything flows from the left to the right from the top, to the bottom, the the numbers don't back up they don't come back.

168
00:29:45.030 --> 00:29:57.960
Capstone Coach 1: And everything is connected to everything else in the in the model so all of the inputs are connected to all of the hidden precept ron's in the first hidden layer

169
00:29:58.920 --> 00:30:14.190
Capstone Coach 1: All of the precept Johnson, the first hidden layer are connected to all of the precept trans in the next layer now, in this case the next layer happens to be the output layer, but it could have been another hidden layer if we wanted it to be.

170
00:30:15.660 --> 00:30:27.840
Capstone Coach 1: that here we have three outputs and the configuration of these preset prawns and the output layer is a little different than it is for the hidden layer

171
00:30:28.440 --> 00:30:40.200
Capstone Coach 1: For the hidden layer, we have no particular requirements about scaling the output from each one of these precepts ron's if we have a preceptor on that's delivering.

172
00:30:41.280 --> 00:30:56.760
Capstone Coach 1: signals that is higher than it should well the weight on that precept RON will adjust it for as input to the next one downstream so as these weights are sort of a self adjusting adjusting a way of handling problems with.

173
00:30:57.180 --> 00:31:08.460
Capstone Coach 1: Over scaling or under scaling that kind of thing, but for the output layer when it gets read it gets ready to send its output downstream to the user.

174
00:31:09.090 --> 00:31:21.090
Capstone Coach 1: Well, they want to make you want to make sure that this output is scaled in some respect if you're talking about nominal targets, then these output should look like probabilities.

175
00:31:21.660 --> 00:31:33.750
Capstone Coach 1: Every one of them should be between zero and one you can't have a 10 you can have a minus one, and if that won't do they all have to be numbers between zero and one.

176
00:31:34.110 --> 00:31:47.370
Capstone Coach 1: These are floating point numbers and Sir probabilities also, if you take these three numbers and you add them together, they should always equal one, regardless of the data.

177
00:31:48.090 --> 00:31:57.660
Capstone Coach 1: So mathematically this thing is designed to achieve that, and it does that by using something called soft Max X activation.

178
00:31:58.110 --> 00:32:09.900
Capstone Coach 1: So if you have nominal targets, you are in almost always going to be using soft Max activation on the output side on the output side.

179
00:32:10.770 --> 00:32:20.070
Capstone Coach 1: Now for the others, the hidden layer you can use you have choices, a half a dozen or more different kinds of activation functions.

180
00:32:20.460 --> 00:32:30.060
Capstone Coach 1: That you can put in the hidden layer, but on the output side, no soft Max now if this was binary where you have only a single output.

181
00:32:30.480 --> 00:32:39.570
Capstone Coach 1: And that output, being the probability of a zero of probability of a one, then you don't use soft Max you use something called Sigma.

182
00:32:40.230 --> 00:32:54.600
Capstone Coach 1: sigmoid Apps activation also guarantees that the numbers between zero and one but there's no requirement for all of the numbers to add up to one in a binary case you only have one probability that's coming out.

183
00:32:59.790 --> 00:33:01.200
Capstone Coach 1: Okay, so.

184
00:33:04.260 --> 00:33:14.100
Capstone Coach 1: You know this is, I think I just showed this is the nominal fit we're going to go over that we're talking about now we're talking about convolution.

185
00:33:15.690 --> 00:33:22.170
Capstone Coach 1: convolution what is convolution well we're going to be talking about this for a while tonight.

186
00:33:23.280 --> 00:33:38.670
Capstone Coach 1: If you have a photograph or video you have too much data just too much even a small photograph if you count the number of pixels in that photograph that photograph has a lot of data right.

187
00:33:39.990 --> 00:33:50.670
Capstone Coach 1: So we need to have ways in which we tricks, if you want for how do you process these images when you have thousands or millions.

188
00:33:51.030 --> 00:34:04.530
Capstone Coach 1: Of features in an image right, so it wouldn't be unusual to have a photograph let's let's say it, that is 100 by 100 pixels as a matter of fact, the screen that you're looking at right now.

189
00:34:05.970 --> 00:34:07.710
Capstone Coach 1: has probably.

190
00:34:08.730 --> 00:34:20.490
Capstone Coach 1: Over 1000 pixels in both directions and so, if you multiply those the hyphens the whip you get a very large number of pixels on your screen right.

191
00:34:21.390 --> 00:34:27.330
Capstone Coach 1: that's a good thing, because it gives a certain resolution and we can see what's on the screen and things like that, but.

192
00:34:28.200 --> 00:34:37.890
Capstone Coach 1: If you're talking about analyzing what's on the screen oh boy, we need to reduce the information that's there down to some manageable size.

193
00:34:38.250 --> 00:34:46.980
Capstone Coach 1: And what is manageable of course depends upon the what you're trying to achieve if you're just looking for a large object or a single object.

194
00:34:47.460 --> 00:34:53.670
Capstone Coach 1: In the photograph then that's much easier than if you were looking for multiple objects smaller.

195
00:34:54.120 --> 00:35:04.410
Capstone Coach 1: And so you require more resolution and things like that so tonight we're going to be looking at a problem where the photographs are pretty nice to us there they have.

196
00:35:05.220 --> 00:35:18.840
Capstone Coach 1: they're only 28 by 28, thank goodness, so you have so bad, but 2828 is 784 pixels so you still have 784 features.

197
00:35:19.350 --> 00:35:29.520
Capstone Coach 1: That you have to deal with, and these are all new numbers so we'll see how that's done you're you're basically the trick to handling this sort of information.

198
00:35:30.090 --> 00:35:41.550
Capstone Coach 1: is to use convolution and that's what under CNN is convoluted does not forget the TV show the news network stands for convolution neural network.

199
00:35:41.970 --> 00:35:54.810
Capstone Coach 1: And the main difference between the LP and CNN is that see it in Einstein designed to pre process large scale photographs and this is so much fun.

200
00:35:56.310 --> 00:36:08.010
Capstone Coach 1: You know, we could go on and on about this and we probably will for a lot for the rest of the Semester, but the the techniques for processing photographs images are are amazing.

201
00:36:09.360 --> 00:36:20.850
Capstone Coach 1: First, the images are converted to 10 source and you've heard the word tense or many times tensorflow what's utensil or well it's a fancy word that.

202
00:36:22.230 --> 00:36:26.670
Capstone Coach 1: Amazon made up oh sorry Google for forgiving Google.

203
00:36:27.720 --> 00:36:43.830
Capstone Coach 1: Google made up the word tensile and it basically refers to a non P array of almost any number of dimensions in the context of photographs it's usually four dimensions.

204
00:36:44.760 --> 00:36:55.440
Capstone Coach 1: And the context of it can be three dimensions but it's usually four and then, in the context of video, you have a fifth dimension which is time right.

205
00:36:56.220 --> 00:37:17.580
Capstone Coach 1: So, think about that as far as processing but we're just going to look at the simple thing of photographs tonight 28 by 28 usually you have a supervised target you're trying to classify the photographs so ordinarily with photographs, the target is either binary or nominal.

206
00:37:18.780 --> 00:37:29.580
Capstone Coach 1: binary example that will look at is looking at X rays, to determine whether the X Ray is normal or abnormal so that would be a simple question.

207
00:37:30.120 --> 00:37:39.210
Capstone Coach 1: And that's a binary question so your target, there would be binary you would have a collection of X rays that were from normal patients.

208
00:37:39.570 --> 00:37:47.760
Capstone Coach 1: And then you would have others have probably the same amount same number that are from patients with an abnormality.

209
00:37:48.240 --> 00:37:58.290
Capstone Coach 1: And you use then you're going to train your deep learning network using those photographs to recognize which one X Ray is normal, and which one is not.

210
00:37:59.250 --> 00:38:10.050
Capstone Coach 1: Now nominal classification also occurs and would the example we look going to look at tonight is nominal and the number of nominal classes can be quite large.

211
00:38:10.770 --> 00:38:24.360
Capstone Coach 1: 10 is not unusual, but you could have actually several hundred suppose, for example, you're trying to do your training the computer to recognize all of the products that are sold in the grocery store.

212
00:38:25.410 --> 00:38:36.480
Capstone Coach 1: You know you have a camera and it's watching everything it's coming across the belt and then it's sort of looking to see Okay, is that can good is that frozen good is it candy is that an ice cream yet sort of thing.

213
00:38:37.470 --> 00:38:46.320
Capstone Coach 1: So you have literally hundreds of things that it might be trying to identify so you're a nominal classification problem, then, is very large.

214
00:38:47.760 --> 00:38:49.260
Capstone Coach 1: Unlike nlp.

215
00:38:50.520 --> 00:39:08.940
Capstone Coach 1: Networks we're going to have many more layers normally and an nlp network multi layer preceptor on network, the number of hidden layers is one, two or three it's unusual to see something more than that.

216
00:39:10.110 --> 00:39:15.090
Capstone Coach 1: Man why well there is a mathematical theorem that says.

217
00:39:16.350 --> 00:39:18.690
Capstone Coach 1: Regard for an ml P network.

218
00:39:19.800 --> 00:39:31.110
Capstone Coach 1: A you can always always find a one hidden layer solution that is as good as anything else Okay, if you put enough.

219
00:39:31.710 --> 00:39:43.140
Capstone Coach 1: preceptor on so that first hidden layer that it's going to it can do as well or outperform see configurations where you have maybe two or three hidden layers.

220
00:39:43.680 --> 00:39:52.620
Capstone Coach 1: Now, as a result of that what you see a lot of people doing, including myself, is basically cramming a whole bunch of perceptions in that first layer

221
00:39:52.980 --> 00:40:03.450
Capstone Coach 1: Running it's running it to see how well it does, and then, if I noticed that i'm having trouble fitting than that, though, that huge number of perceptions, whatever it is.

222
00:40:04.020 --> 00:40:18.480
Capstone Coach 1: And the solution so i'm getting her bouncing all over the place, I get a really good solution, and then the next run out garbage and then another one oh, it looks good garbage you know that's just random noise so it's affecting the optimizer.

223
00:40:19.770 --> 00:40:33.990
Capstone Coach 1: Then I know the optimizer is having problems fitting the network and that that point, I will try to go to multi layers two layers maybe three layers and I will stack the layers and so.

224
00:40:35.280 --> 00:40:44.790
Capstone Coach 1: Personally, what I do is I look at how many inputs do I have let's suppose I have 100 inputs and then I also look at how much data do I have well.

225
00:40:45.630 --> 00:40:58.620
Capstone Coach 1: If I have 100 inputs in 100 cases or observations i'm in serious trouble, so what I hope to see is 100 cases and 10,000 or 20,000 or more.

226
00:40:59.340 --> 00:41:09.390
Capstone Coach 1: Cases so let's suppose that that's a situation i've got 100 features and i've got thousands of observations here that i'm going to process to fit this network.

227
00:41:09.870 --> 00:41:17.670
Capstone Coach 1: In that situation i'll put 100% ron's the number precept ronson the first hidden layer will be the same as the number of inputs.

228
00:41:18.240 --> 00:41:27.840
Capstone Coach 1: i'm overloading them, so you know that should do a pretty good job of fitting the data unless there's some complexity about the data that.

229
00:41:28.530 --> 00:41:35.040
Capstone Coach 1: I understand, and I will try to do that and then from that I might shrink the first layer

230
00:41:35.910 --> 00:41:45.840
Capstone Coach 1: I might also then put a second layer behind the first, and when I put that second layer in there it's good to have fewer preceptor on So if I have 100 and the first.

231
00:41:46.170 --> 00:41:54.960
Capstone Coach 1: i'm going to go to about 50 about half in the second and then the third one i'll go to even smaller half of that so normally I go half half half.

232
00:41:55.350 --> 00:42:06.300
Capstone Coach 1: When I back off and come in with these other networks and the only reason to bring in those other networks is so that the solutions, become more stable and more.

233
00:42:07.320 --> 00:42:08.070
Capstone Coach 1: Reliable.

234
00:42:10.110 --> 00:42:25.980
Capstone Coach 1: The other thing that CNN has that nlp has is it's going to be dense all preset drones are connected to all previous on following layers so the interconnections in a CNN network are huge.

235
00:42:27.420 --> 00:42:40.710
Capstone Coach 1: What you're going to see also with the CNN because of this characteristics as you're going to say is that the number of weights in the CNN network it gets to be enormous and i'm talking over a million.

236
00:42:41.850 --> 00:42:46.860
Capstone Coach 1: And so the and you're thinking wait a minute i've only got 100 photographs.

237
00:42:47.340 --> 00:43:05.580
Capstone Coach 1: yeah but each photograph is cut like 12,000 data points you know 12,000 pixels on that photograph sort of process that photograph we're going to need a very complicated arrangement here to basically strip out the information that's in that photograph.

238
00:43:06.600 --> 00:43:15.240
Capstone Coach 1: So the main difference between nlp and CNN is that CNN is used to process large scale photographs and videos.

239
00:43:16.530 --> 00:43:28.770
Capstone Coach 1: First, these photographs are converted to send sores and then they basically in a convolution network, the sense of the 10th source rather are filtered.

240
00:43:29.670 --> 00:43:43.410
Capstone Coach 1: By convolution and to strip out to simplify the photograph so the photograph starts out as a 100 by 100 pixels and what happens is these.

241
00:43:44.070 --> 00:44:00.570
Capstone Coach 1: convolution filters will narrow will produce new types of images new types of images from the original that are much smaller and they get more and more focused on what is on the characteristics in the photograph.

242
00:44:03.990 --> 00:44:08.610
Capstone Coach 1: There are a class of filters, for example, so you may have seen some of these if you.

243
00:44:09.240 --> 00:44:20.580
Capstone Coach 1: If you work with the in photography and nowadays, you have things like photoshop that will allow you to do some pretty amazing things with photographs change the colors.

244
00:44:21.180 --> 00:44:33.000
Capstone Coach 1: And there are certain types of filters that are used there that are going to be here, you will see them used in a convolution that work, and so, for example, one of the filters is.

245
00:44:35.130 --> 00:44:42.330
Capstone Coach 1: process the photograph so that I can see only the horizontal lines and the picture.

246
00:44:43.590 --> 00:44:49.560
Capstone Coach 1: Then there's another photograph that says, do the same thing, but now give me the vertical lines.

247
00:44:49.920 --> 00:45:04.440
Capstone Coach 1: that are in the picture now a line would be a part of the photograph where the color changes dramatically from the or the other, like intensity changes dramatically from low to high and then back to low again.

248
00:45:05.340 --> 00:45:15.300
Capstone Coach 1: So these that's what the filters are doing they're looking for lines that are in the photograph and then the neural network will.

249
00:45:15.870 --> 00:45:22.410
Capstone Coach 1: process these images these lines to identify different types of objects.

250
00:45:23.010 --> 00:45:30.000
Capstone Coach 1: And, for example, if you're processing the face well, of course, you're you're you're you're looking for something you know, an image that's.

251
00:45:30.450 --> 00:45:39.660
Capstone Coach 1: Somewhat round and then you're looking for dark areas, the eyes the nose and the mouth and things like that that's all done that can all be done through convolution.

252
00:45:42.660 --> 00:45:54.360
Capstone Coach 1: The recurrent neural network rem is also available and charisse and, as I mentioned this, this is primarily used for data that have a time component.

253
00:45:54.840 --> 00:46:03.840
Capstone Coach 1: Of the you can order the photographs, or you can order, whatever the data is it doesn't have to be photographs, it could be just regular time series data.

254
00:46:04.230 --> 00:46:11.190
Capstone Coach 1: If you're processing ordinary time series data and it's very complicated and you have information.

255
00:46:11.700 --> 00:46:25.170
Capstone Coach 1: Common that information about the data that are that's part of the feature structure as a, not only do you have the output that's you know time oriented, but you also have features that help describe.

256
00:46:25.800 --> 00:46:39.690
Capstone Coach 1: The output itself, then a rnn neural network is is a useful machine for doing time series forecasting normally when you take a course in statistics, they teach you about arena.

257
00:46:41.100 --> 00:46:53.760
Capstone Coach 1: Are regressive moving average type of a model, and those are very good, they were invented sometime ago when we didn't have these kinds of computers.

258
00:46:54.150 --> 00:46:59.310
Capstone Coach 1: And you wanted a technique that you could use that gave you a pretty sophisticated answer.

259
00:46:59.730 --> 00:47:10.050
Capstone Coach 1: That describe the structure of the data and also gave you pretty good forecasts that could be developed quickly with few complications, and so the arena.

260
00:47:10.320 --> 00:47:19.560
Capstone Coach 1: models that you see a really good at that they're they're also very good at just describing the structure of the phenomenon that you're looking at.

261
00:47:20.490 --> 00:47:35.580
Capstone Coach 1: If you don't really care about all of that structure, you can use an rnn and rnn will grab a ton of data and produce a a very sophisticated time series forecast based upon all of that data.

262
00:47:37.290 --> 00:47:38.850
Capstone Coach 1: These are fairly new, by the way.

263
00:47:40.200 --> 00:47:58.170
Capstone Coach 1: Back about 15 years ago we knew they existed, I mean there was the mathematical theory was there, but the code to do this wasn't available and the computing power wasn't there as well, so these are just now starting to come into their own for forecasting.

264
00:47:59.490 --> 00:48:24.870
Capstone Coach 1: And soon we got here so for let's let's go talk about tensorflow in Paris just for a moment here i'm showing you the the usual imports, so you say importance of florists Ts tf tensorflow as tf and you always have this and, as you re import not be is normally there as well.

265
00:48:27.240 --> 00:48:40.260
Capstone Coach 1: not be intensive for tensorflow are they like live together, so, if you look at what what is a tense, or what you find is that a tense or is a number array.

266
00:48:40.710 --> 00:48:50.040
Capstone Coach 1: Of multi dimensions, basically, when we think of a race we normally think of a two way table like an excel table or something like that.

267
00:48:50.340 --> 00:49:03.000
Capstone Coach 1: or a two way matrix most linear algebra courses spend a ton of time on the idea of a two way matrix and not be certainly handles that.

268
00:49:03.600 --> 00:49:13.470
Capstone Coach 1: But not they will also handle three, four and five dimensions structures and arrays and a tense or is basically a structured array.

269
00:49:13.800 --> 00:49:26.880
Capstone Coach 1: Where you normally have more than just two, you can have only two dimensions in a tense or or even only one but and when it comes to photography they're usually three or four dimensions, at least.

270
00:49:28.020 --> 00:49:33.990
Capstone Coach 1: So what is a tense or well it's a it's a lumpy like array multiple dimensions.

271
00:49:34.380 --> 00:49:44.100
Capstone Coach 1: And it's something that Google started using their jerk on to refer to the fact that I guess they probably found people when you said array.

272
00:49:44.460 --> 00:49:59.580
Capstone Coach 1: They thought automatically two dimensions of our most mathematicians will say what why, if you have an array can be any number of dimensions well typically in your linear algebra classes it's just talked about in the two dimensional context.

273
00:50:01.020 --> 00:50:11.010
Capstone Coach 1: From tensorflow work flow charisse important model, so this is the import statement that lets you design a neural network model.

274
00:50:12.270 --> 00:50:21.510
Capstone Coach 1: Then you have layers, which is the structure within carousel lets you design the hidden layers and the output layer

275
00:50:22.290 --> 00:50:32.550
Capstone Coach 1: And then you have utilities like import to God categorical hey guess what this to categorical here will take a nominal.

276
00:50:32.880 --> 00:50:50.490
Capstone Coach 1: series, you know, like a y value that's all like 1234 or five that kind of thing and convert it to one hat I want hot So if you say to categorical then in parentheses you give it why it's going to return the one hot columns for that why.

277
00:50:51.540 --> 00:51:10.170
Capstone Coach 1: Well that's really cool man it's so easy to use this bulletproof one one statement actually so when you're running a neural network, you want to do that you want to change all of your categorical data to one hot columns even the target is going to be handled in this way.

278
00:51:12.360 --> 00:51:21.570
Capstone Coach 1: The data sets care assessment data sets for examples and the one we're going to be using most today is called in NIS T.

279
00:51:22.140 --> 00:51:32.070
Capstone Coach 1: And this is a survey says 70,000 images actually 70,000 black and white photographs each one of them is 28 by 28 pixels Thank goodness.

280
00:51:32.610 --> 00:51:49.860
Capstone Coach 1: Each one of those black and white, thank goodness, and so we don't have we have 784 pixels on each image and that's what we have to process, which means that we can do this on a relatively slow computer and not wait all night for the solution.

281
00:51:50.940 --> 00:52:00.570
Capstone Coach 1: Now from tensorflow charisse optimizes we have several that are available, this has become my latest favorite it's called alpha Delta.

282
00:52:00.900 --> 00:52:10.800
Capstone Coach 1: I have used all of the optimizer, most recently, and several different contexts this guy seems to work had head over heels over everything else.

283
00:52:11.220 --> 00:52:24.300
Capstone Coach 1: There is another one that's very popular it's called Adam Ada and it's been around for some time at a delta is basically a small but important change to add them.

284
00:52:24.690 --> 00:52:36.270
Capstone Coach 1: So at a delta changes the learning rate as you get closer and closer to the optimum and that seems to make it more stable and a little more aggressive in terms of finding a good solution.

285
00:52:37.440 --> 00:52:47.340
Capstone Coach 1: This is fairly recent i'd say the last three four years and it's only found right now and charisse and so you if you pick up a book.

286
00:52:48.540 --> 00:52:54.780
Capstone Coach 1: And look read through the book, you may see no reference to it you'll see references to add on.

287
00:52:55.140 --> 00:53:05.850
Capstone Coach 1: And then there's some other optimizes that are popular, but if you get a recent book let's say 2019 or or before or later you'll see more references to this.

288
00:53:06.300 --> 00:53:18.810
Capstone Coach 1: and give it a try, you can only see us the others good I like to try them all and see which one performs better now let's see did I get that right right.

289
00:53:19.830 --> 00:53:20.490
Capstone Coach 1: rnn.

290
00:53:21.630 --> 00:53:27.510
Capstone Coach 1: The data has a time component supervised targeted dance, and I think I did the server ready.

291
00:53:29.010 --> 00:53:29.520
Capstone Coach 1: Okay.

292
00:53:31.110 --> 00:53:33.930
Capstone Coach 1: Well, this is the imports for me LP.

293
00:53:36.480 --> 00:53:43.800
Capstone Coach 1: And we just talked about that what what's going on all right hand written business, this is the data we're going to be looking at tonight.

294
00:53:45.570 --> 00:53:53.250
Capstone Coach 1: So what they did was they took a whole bunch of people and ask these people to write down the numbers zero to nine.

295
00:53:54.090 --> 00:54:14.640
Capstone Coach 1: On pieces of paper, and then they photograph them and they they put them in black and white, and so there we have we have 70,000 of these guys 60,000 are categorized as the training sample and then 110,000 are categorized as the validation sample.

296
00:54:16.260 --> 00:54:23.970
Capstone Coach 1: We when you analyze these there are a couple ways to do it the first way we're going to do it is we're going to take the M LP approach.

297
00:54:24.930 --> 00:54:29.940
Capstone Coach 1: Multi layer perception approach and i'm surprised at how well those tasks, by the way.

298
00:54:30.330 --> 00:54:39.450
Capstone Coach 1: But it does pretty good so let's take a look and see what happens there and then we're going to talk about convolution and what that what is really going on with convolution.

299
00:54:39.960 --> 00:54:49.950
Capstone Coach 1: So if you're going to work with this kind of data, the first thing that you'll do is you'll read in the photographs the photographs will come in as an image.

300
00:54:50.460 --> 00:55:05.250
Capstone Coach 1: into Python and, basically, if you look at the structure that image it'll be a number of the array yeah black and white images come in as a basically a two dimensional array.

301
00:55:06.390 --> 00:55:14.010
Capstone Coach 1: And, but it's stored in not be it's stored in three dimensions so you'll have, if you look at the the well actually four dimensions.

302
00:55:14.430 --> 00:55:23.190
Capstone Coach 1: If you look at the first dimension you'll see the observation number Okay, so if I read in X T here which will do in just a minute.

303
00:55:23.580 --> 00:55:42.960
Capstone Coach 1: And then I look at the shape of X T you'll see there for numbers, the first number will be 60,000, which is the number of rows your have the next number will be 28 the width of the fixture the next number 28 the height of the picture the next number one.

304
00:55:44.610 --> 00:55:56.490
Capstone Coach 1: That last number is called the Channel and for black and white it's only there's only one channel if you're dealing with color photographs that last number will be three.

305
00:55:57.150 --> 00:56:08.310
Capstone Coach 1: Red blue green, so the the photo color photographs are basically three different pictures that are overlaid one, on the other, blue red and green.

306
00:56:08.940 --> 00:56:17.100
Capstone Coach 1: And the your eyes, does he do a great job of blending those colors together to come up with a rich colored photograph.

307
00:56:17.490 --> 00:56:28.500
Capstone Coach 1: But on the computer the computer can see oh that's green that's red that's blue, and they are different actually different data structures.

308
00:56:28.860 --> 00:56:35.640
Capstone Coach 1: So you'll be able to actually pull those out from the on the computer and process them separately.

309
00:56:36.390 --> 00:56:49.530
Capstone Coach 1: You could do things like take a full color photograph and make it black and white, for obvious reasons right it's do you don't want three times as much data if you can cut back you can cut a third of the data that's pretty good.

310
00:56:50.310 --> 00:56:51.900
Kilani, Shadi: pretty good job, I have a question.

311
00:56:54.720 --> 00:56:55.230
Capstone Coach 1: sure.

312
00:56:57.450 --> 00:56:57.750
Kilani, Shadi: This is.

313
00:56:59.160 --> 00:57:01.530
Capstone Coach 1: Wait a minute, let me find a let me find the.

314
00:57:03.360 --> 00:57:04.590
Capstone Coach 1: Okay, where is it.

315
00:57:06.540 --> 00:57:09.150
Capstone Coach 1: Oh there it is okay, I need to pull up the.

316
00:57:10.830 --> 00:57:12.000
Capstone Coach 1: i'd like to pull up the.

317
00:57:13.050 --> 00:57:14.400
Capstone Coach 1: chat and everything and.

318
00:57:16.980 --> 00:57:18.510
Capstone Coach 1: Okay yeah sure go ahead, no.

319
00:57:19.890 --> 00:57:31.890
Kilani, Shadi: Just a question about the data prep how much how Labor intensive, it is to convert those images and what exactly how do you do that or what's like a general idea of course i'm not you know we can force you know.

320
00:57:31.920 --> 00:57:38.850
Kilani, Shadi: But just yeah, how do you convert the image to this these arrays seems, at least from my point of view, what i'm seeing.

321
00:57:39.990 --> 00:57:44.010
Kilani, Shadi: It seems like it's very Labor intensive are very complicated, but maybe there's a way to do it.

322
00:57:44.100 --> 00:57:53.940
Capstone Coach 1: No actually well, you can take an image and read it read it, as you can take an ordinary jpeg image and read it as a no as a numeric file.

323
00:57:55.380 --> 00:57:56.070
Capstone Coach 1: In Python.

324
00:57:56.520 --> 00:57:58.020
Kilani, Shadi: it's as simple as that okay.

325
00:58:00.870 --> 00:58:01.740
Capstone Coach 1: The image.

326
00:58:02.670 --> 00:58:13.170
Capstone Coach 1: Every image is just a series of numbers belief in the facts, so, if you look at these these black and white images here white is the number 255.

327
00:58:13.800 --> 00:58:31.200
Capstone Coach 1: Black is zero on these images, if you were to pull out these actually look at the numbers and which you can which we will do here and look at these numbers you'll see Oh, most of the number serves and zero, which is black oh here's a here's a 255 white.

328
00:58:32.850 --> 00:58:40.680
Capstone Coach 1: So, and in fact that's what allows convolution to work so well so convolutions scan this image and see oh.

329
00:58:41.340 --> 00:58:46.830
Capstone Coach 1: A white area, right here, why area, right here by around it's all you know it's going to see.

330
00:58:47.610 --> 00:58:57.600
Capstone Coach 1: several pieces of information that allow you to identify these numbers in on the computer so actually you know you could you could set up.

331
00:58:58.050 --> 00:59:08.070
Capstone Coach 1: You can take a video, for example, off of a drone or something like that, and you can pull out off of video the individual pictures right.

332
00:59:08.610 --> 00:59:19.320
Capstone Coach 1: So if you're taking a photograph nowadays if you're taking videos on your camera it's probably going to be 4k resolution, which means the number of pixels is huge.

333
00:59:20.970 --> 00:59:39.630
Capstone Coach 1: And the number of frames per second is probably 3024 30 maybe 60 if you have a really high tech camera or something like that, so that means if you've got a video you basically have 30 photographs per second in that file.

334
00:59:41.040 --> 00:59:51.870
Capstone Coach 1: And now, if you get you have to have a healthy computer to bring all that video in and then you could pull out any frame you wanted to and look at it and process it.

335
00:59:53.040 --> 00:59:54.810
Capstone Coach 1: And it's just going to be numbers.

336
00:59:56.400 --> 01:00:03.150
Capstone Coach 1: So it seems come more more complicated COMP COMP complex than it actually is, but anyway.

337
01:00:05.490 --> 01:00:08.280
Capstone Coach 1: Like like to go over the good question, thank you.

338
01:00:09.300 --> 01:00:10.860
Capstone Coach 1: Okay it's seven o'clock.

339
01:00:10.890 --> 01:00:23.940
Capstone Coach 1: I think it's a water break time, so, if you would please let's take a 10 minute break here and download there's a file that I want you to download and start up spider.

340
01:00:25.590 --> 01:00:27.810
Capstone Coach 1: Let me see if I can pull up the campus here.

341
01:00:29.040 --> 01:00:33.450
Capstone Coach 1: There we go the one that says LP on it we're going to start there.

342
01:00:34.680 --> 01:00:45.900
Capstone Coach 1: So we're going to start we're going to look at the photographs and the numbers and the photographs and all that sort of thing and look at how we use an ordinary neural network to process images and then.

343
01:00:46.410 --> 01:00:59.280
Capstone Coach 1: we're going to go to the convolution type of approach, which is the more sophisticated deep learning approach that gives us additional information okay so let's let's get reconvene and 710.

344
01:01:00.660 --> 01:01:04.290
Capstone Coach 1: get some coffee or some water and see that.

345
01:01:05.670 --> 01:01:09.840
Capstone Coach 1: Shall we get started here, first of all, are there any questions.

346
01:01:11.190 --> 01:01:13.950
Capstone Coach 1: about what we've what we've been talking about or otherwise.

347
01:01:20.100 --> 01:01:21.570
Fireman, Karen: You know, it seems easy when you do it.

348
01:01:24.360 --> 01:01:24.840
Capstone Coach 1: You what.

349
01:01:26.460 --> 01:01:28.590
Fireman, Karen: It said it always seems easy when you do it.

350
01:01:31.980 --> 01:01:35.010
Jim Clark: jones's Jim I do have a question about.

351
01:01:36.300 --> 01:01:44.160
Jim Clark: Some of this and maybe this is something related to this there's so many different combinations of choices, you can make.

352
01:01:45.180 --> 01:01:46.590
Jim Clark: Two or how many.

353
01:01:47.910 --> 01:02:04.920
Jim Clark: You know I think perceptions you you build in and and you do this again and again and again to your specified this good enough, is there a way to kind of semi automates the process, so he will he will run through all those pages, for you or or zero in on the best method.

354
01:02:07.740 --> 01:02:08.280
Capstone Coach 1: well.

355
01:02:09.390 --> 01:02:10.080
Capstone Coach 1: You know.

356
01:02:11.220 --> 01:02:21.990
Capstone Coach 1: it's it's most things are like this are just experience you know the, the more you do it, the more you realize, there are there's there are things that you want to look for.

357
01:02:23.160 --> 01:02:26.310
Capstone Coach 1: But there's no formal process for it.

358
01:02:27.450 --> 01:02:34.200
Capstone Coach 1: Especially this material because it much of a changes so quickly the.

359
01:02:35.550 --> 01:02:44.310
Capstone Coach 1: The types of packages that we're using only been out for a few years and curious now they'll probably be different so.

360
01:02:45.720 --> 01:02:55.080
Capstone Coach 1: You have to have the ability to read the documentation and adjust what you're doing to the new new approaches what's going on.

361
01:02:56.040 --> 01:03:14.850
Capstone Coach 1: For example, right now, we're talking about tensorflow and charisse which came from Google right, and this is, this has been touted now is Oh, this is so great so good, you can do deep learning about all these different things that we weren't able to do before and that's all true.

362
01:03:16.530 --> 01:03:26.040
Capstone Coach 1: In fact, they pretty much have Cone coined the word deep learning, but really When did they come out well it came out just a few years ago.

363
01:03:26.820 --> 01:03:38.610
Capstone Coach 1: And before that there was no deep learning and, by the way, there's a new one out now it's called P wide torch may have heard of that that the torch is similar to tensorflow.

364
01:03:39.510 --> 01:03:52.020
Capstone Coach 1: Except that it came from Facebook so Facebook is jumping in in the arena and saying what we've got some pretty cool software over here to that we can.

365
01:03:52.440 --> 01:04:13.230
Capstone Coach 1: provide to the general public that will help you recognize images and things like that so P, why torches and other alternative that's growing popularity and probably continue to do so apple hasn't come out with anyone or no nothing really that we noticed but probably will soon.

366
01:04:16.590 --> 01:04:17.040
Capstone Coach 1: Okay.

367
01:04:17.370 --> 01:04:17.730
Jim Clark: Thank you.

368
01:04:21.900 --> 01:04:31.980
Capstone Coach 1: let's take a look at that code so i've opened it up here on in spider and if you could follow, along with me, he will start at the top.

369
01:04:33.240 --> 01:04:53.820
Capstone Coach 1: and take a look at the import statements, and these are similar what you saw when we were talking earlier about the inputs for SK SK learner here logistic regression and nlp classifier nlp regress over is not in there, because the target in this case is nominal.

370
01:04:55.080 --> 01:05:01.800
Capstone Coach 1: And then you have a bunch of imports for tensorflow The first one is always tensorflow as tf.

371
01:05:02.700 --> 01:05:11.910
Capstone Coach 1: And then charisse is a subsystem within tensorflow so it's always tensorflow dot charisse import something.

372
01:05:12.390 --> 01:05:31.530
Capstone Coach 1: So this something the models program comes from tensorflow but tensorflow is within i'm sorry comes from charisse but charisse is within them, I don't know you know they didn't get too confusing obviously this is a product that grew over time as so they did more and more things.

373
01:05:32.700 --> 01:05:47.340
Capstone Coach 1: The models is going to let us build the nlp models multi layer preset tron neural networks layers is the procedure that lets us build the hidden layers and the output layer in network.

374
01:05:48.030 --> 01:05:56.940
Capstone Coach 1: regularize or we haven't talked a lot about that, but there are two major forms of regular risers in deep learning.

375
01:05:57.780 --> 01:06:12.990
Capstone Coach 1: That are basically used to control the weights that go in the network, so you know we have l one l to regularization for regression that are designed to control the.

376
01:06:13.500 --> 01:06:23.580
Capstone Coach 1: Multi linearity effects among the coefficients while you have the same thing here you have l one l to available plus another one that you don't see in regression.

377
01:06:25.680 --> 01:06:32.310
Capstone Coach 1: And then we have some utilities, here we have categorical for building one hot categorical data.

378
01:06:33.540 --> 01:06:46.500
Capstone Coach 1: We have him in is where we're going to be pulling the data from and then these are the most popular optimized servers Adam is is been around for some time rms propagation.

379
01:06:47.310 --> 01:07:03.000
Capstone Coach 1: At a delta, which is the one that i'm going to mostly use and then sgt and other form of optimizer basically you use your favorite and then, when your favorite doesn't work for you, you try another look for a new favorites.

380
01:07:04.680 --> 01:07:20.310
Capstone Coach 1: There is a big function here called charisse accuracy plots and that's all it does it's not doing much of any kind of calculation for you it's just summarizing the results of the learning model that you just created.

381
01:07:21.090 --> 01:07:28.080
Capstone Coach 1: So this is a giveaway something i've had to put together so that we could see the results of our analysis.

382
01:07:28.620 --> 01:07:43.260
Capstone Coach 1: And it's in this program it's you're going to see the other programs as well i'm just going to scroll down to the bottom, it says, right here, it says in of charisse graph and metric function so that's the end of the function.

383
01:07:44.280 --> 01:07:46.830
Capstone Coach 1: And now we start with the real.

384
01:07:48.000 --> 01:08:03.180
Capstone Coach 1: Learning going on here so number one let's go to the top back i'm sorry I should have deafness let's run all of these imports so grab them, just like that select them right click and see run selection.

385
01:08:04.560 --> 01:08:10.560
Capstone Coach 1: And you'll see it come up over here in your I Python window if everything is.

386
01:08:11.760 --> 01:08:25.020
Capstone Coach 1: installed and it recognizes, all of this, then you get just know the prompt no errors, if you get an error there's something probably not installed on your machine.

387
01:08:26.940 --> 01:08:44.460
Capstone Coach 1: Now, hopefully, no one, no one saw that that scroll back down to the bottom of this long function, so now i'm going to line number 170 166 mixture, it says true in 166.

388
01:08:46.410 --> 01:09:00.690
Capstone Coach 1: There are several there are two sections or three sections of code here, and I use this habit of creating a flag, like in this case data reset equals true which controls which section, you can just going to run and so forth.

389
01:09:01.800 --> 01:09:16.980
Capstone Coach 1: So this section is the section that reads the Dana, so I would like, for you to highlight the statement 170 on your code here again right click one run selection.

390
01:09:18.540 --> 01:09:20.460
Capstone Coach 1: bang pretty fast right.

391
01:09:21.630 --> 01:09:25.830
Capstone Coach 1: What well, maybe it's fast because i've already read this so many times.

392
01:09:28.530 --> 01:09:30.300
Capstone Coach 1: So generally run faster you.

393
01:09:32.370 --> 01:09:36.630
Jim Clark: Well, well, Sir, I was just going to ask him, he said 166 I think that.

394
01:09:36.720 --> 01:09:39.090
Capstone Coach 1: 117 on my.

395
01:09:39.120 --> 01:09:44.730
Capstone Coach 1: machine says 170 917 day.

396
01:09:48.720 --> 01:09:57.960
Capstone Coach 1: It says X T whitey X ve y equals to me and I St dot load data.

397
01:09:59.610 --> 01:10:04.200
Capstone Coach 1: So this this command actually read send all those 70,000 photographs.

398
01:10:08.400 --> 01:10:12.930
Capstone Coach 1: That you was able to get that running champ Can somebody confirm.

399
01:10:13.650 --> 01:10:14.700
Fireman, Karen: Yes, Professor yes.

400
01:10:14.760 --> 01:10:15.990
Zuberi, Bilal: it's very fast for us.

401
01:10:16.080 --> 01:10:18.720
Capstone Coach 1: Okay, I don't want to move ahead until we.

402
01:10:18.780 --> 01:10:20.610
Capstone Coach 1: were like everybody's got the data right.

403
01:10:21.150 --> 01:10:38.850
Capstone Coach 1: So if you've got the data, then what i'd like you to do is go over to the I Python live window and type eye X ti dot shape, I think you know this command this command basically says tell me what your shaped like X ti X T is the training.

404
01:10:39.900 --> 01:10:51.810
Capstone Coach 1: matrix is coming across it's got 60,000 rows and then two columns 28th and 28 that's the width and the height of the picture.

405
01:10:52.830 --> 01:11:04.080
Capstone Coach 1: And so the total number of variables and of values for each pixel is denoted by a number is 28 times 28 784.

406
01:11:05.610 --> 01:11:17.550
Capstone Coach 1: Now, if you look at if you type another thing you should might be curious about is what is this anyway type in type and then inside parentheses X T like that.

407
01:11:19.050 --> 01:11:30.240
Capstone Coach 1: And it'll tell you it's a non P array so the data just came in, as an empty array but you notice it doesn't say photo photograph.

408
01:11:30.780 --> 01:11:55.650
Capstone Coach 1: No photographs come in as an up erase erase of numbers hmm this is curious let's take a look at the first line in X T so go X T and then in parentheses in brackets now zero like that and let's see what we get who wow, this is the first photograph.

409
01:11:57.150 --> 01:12:00.600
Capstone Coach 1: hmm remember what I said about zeros and.

410
01:12:02.460 --> 01:12:14.730
Capstone Coach 1: All of those heroes are black areas and the photograph and then the numbers to 55 that's the wide area numbers between them like here's an ad in a 156 those are great.

411
01:12:15.900 --> 01:12:24.660
Capstone Coach 1: The smaller than number the lighter Gray, the darker the closer, it is to 50 to 55 the darker tests right.

412
01:12:26.040 --> 01:12:44.850
Capstone Coach 1: So well that's cool So if you have the photograph one photograph it's basically a set of numbers, and this is a two by it's a tube, by the way, not be array it's got 28 rows and 28 columns notice the square brackets here.

413
01:12:45.870 --> 01:12:55.980
Capstone Coach 1: Each row is a list of 28 numbers right there i've highlighted the first row and here comes the second row.

414
01:12:56.820 --> 01:13:18.570
Capstone Coach 1: Each that's that's basically a line of pixels going from left to right on the photograph and then you have another row like that another line there are 228 lines because it's 200 i'm sorry 2028 lines 28 lines on this photograph right.

415
01:13:20.100 --> 01:13:35.850
Capstone Coach 1: So if I take a look at one line one line by itself let's say I go here i'm going to type in X T and then bracket zero, which means i'm looking at the first photograph and then i'm going to go comma.

416
01:13:36.930 --> 01:13:56.460
Capstone Coach 1: Zero comma zero, which is the first line of pixels on the first photograph and now I go comma and i'm just going to put a colon in there, like that, and this is sort of not be notation or at least it should be, this should show me the 28 pixels on the first line right there.

417
01:13:57.660 --> 01:14:04.020
Capstone Coach 1: notice the data type is integer eight, which is a good thing, there conserving memory.

418
01:14:04.530 --> 01:14:13.410
Capstone Coach 1: So, each one of these numbers is a small integer so we only need eight bits to represent it so we can put a lot of photographs in memory.

419
01:14:13.800 --> 01:14:27.030
Capstone Coach 1: If they're only using eight bits of data for each number in that photograph right so let's let's go down down and let's go to line number 10 and the photograph which is near the middle right.

420
01:14:28.050 --> 01:14:36.420
Capstone Coach 1: So the other 28 lines, the one in the middle, is actually well let's go to line 14 right, that is, the middle.

421
01:14:37.950 --> 01:14:47.700
Capstone Coach 1: and see what we got all right so hear us now see some zeros and now some larger numbers, so the zeros of the black.

422
01:14:48.360 --> 01:15:05.610
Capstone Coach 1: And the larger numbers like to 40 to 50 that's white, so we if we were to take a line and drop the middle of this photograph we see we cry we it looks like we're going to cross a white line right there in the middle, you can see how that works out right.

423
01:15:07.140 --> 01:15:07.830
Capstone Coach 1: So.

424
01:15:09.810 --> 01:15:18.720
Capstone Coach 1: what's the getting curious about what this first photograph actually is let's go into the code here and let's take a look at says.

425
01:15:20.760 --> 01:15:21.090
Capstone Coach 1: who's been.

426
01:15:24.060 --> 01:15:32.100
Capstone Coach 1: There I do have some code in here so here it is yeah if you really want to see what one of these looks like you use.

427
01:15:33.150 --> 01:15:35.640
Capstone Coach 1: guess what math plot line.

428
01:15:36.750 --> 01:15:52.530
Capstone Coach 1: You just say plot dot IMS show see this guy right here huh cool let's highlight that this is line 175 and 176 let's highlight that go right click run.

429
01:15:56.580 --> 01:16:10.020
Capstone Coach 1: Ah, so this is the number anybody want to guess is just like I wrote this okay it's a five, and if you were to draw a line right in the middle, you can see it's going to cross that number one time.

430
01:16:10.950 --> 01:16:31.050
Capstone Coach 1: there's no if you had an eight you probably cross twice or something like that, so this is the first photograph it's a five cool now let's change that zero there to know I don't know let's change it to a nine and rerun this and see what we get if we change it to a nine.

431
01:16:32.550 --> 01:16:48.750
Capstone Coach 1: boom oh it's a for cool So if I take the four and I run a straight white line of pixels through that I should get two areas of white is that right yeah so let's try this let's go X T.

432
01:16:50.760 --> 01:16:51.360
Capstone Coach 1: bracket.

433
01:16:52.740 --> 01:17:09.630
Capstone Coach 1: For Well, this is no i'm sorry this is nine so it's actually image number 10 and then we're going to go over halfway down, would it be about 14 and then a comma and a colon like that see what we did.

434
01:17:11.280 --> 01:17:17.970
Capstone Coach 1: Okay, so it's nine comma 14 comma colon and now i'm going to hit a return.

435
01:17:19.740 --> 01:17:26.940
Capstone Coach 1: There the numbers and now notice that we have two fields with white, we have a field, right here.

436
01:17:27.990 --> 01:17:34.320
Capstone Coach 1: On the left and then it goes down to see us going down to the dark area, and then it comes back up.

437
01:17:35.430 --> 01:17:53.460
Capstone Coach 1: To fields with white now, this is what is happening with the neural network it's analyzing these patterns that are in the data to see what to look for differences in the digits so there's another column, which is up here it's y te.

438
01:17:54.480 --> 01:17:59.250
Capstone Coach 1: And if I type in whitey and then in brackets nine.

439
01:18:00.780 --> 01:18:03.690
Capstone Coach 1: guess what huh for.

440
01:18:05.070 --> 01:18:07.050
Capstone Coach 1: whitey bracket zero.

441
01:18:08.700 --> 01:18:13.350
Capstone Coach 1: hmm five y T is the target.

442
01:18:14.370 --> 01:18:19.740
Capstone Coach 1: it's the actual solution to the question what number is it.

443
01:18:20.940 --> 01:18:37.890
Capstone Coach 1: So the neural network once it's going to build a network that tells you Oh, this is the number three number four number five or what have you and it's going to do so by comparing the patterns and the pixels to the target values that you have here right.

444
01:18:38.910 --> 01:18:54.210
Capstone Coach 1: And so the network will learn to differentiate between four and 504 is got multiple spots of white a five one so on a very basic level that's what's kind of what's going on, so.

445
01:18:54.330 --> 01:18:55.680
Kilani, Shadi: Dr Jones I have a question.

446
01:18:57.390 --> 01:19:12.210
Kilani, Shadi: So I got the part of initially when you said about loading the pictures and they come in, as you know, zeros and ones that don't make sense as numbers, but what about the Why did we have to program that because that's something we need to tell them right that this is a five is.

447
01:19:12.210 --> 01:19:15.360
Capstone Coach 1: That is correct, whoever created this data.

448
01:19:15.870 --> 01:19:30.180
Capstone Coach 1: They would have gotten the photographs off with a digital camera Thank God for digital cameras right, by the way, I was thinking about that question during the break and I was thinking oh wait a minute yeah if we were still using analog cameras, this would be a painting.

449
01:19:31.650 --> 01:19:42.330
Capstone Coach 1: Because you have to digitize those pixels but nowadays digital cameras do this automatically so they store the photograph as a set of numbers, basically.

450
01:19:43.080 --> 01:19:56.190
Capstone Coach 1: But now, if you're talking about why somebody has to actually go in and type in okay that's photograph five okay that's a photograph of for that's a photograph three yeah somebody that was not automatic.

451
01:19:57.630 --> 01:20:02.250
Capstone Coach 1: When they registered the photographs they reached out to them according to that y value.

452
01:20:04.350 --> 01:20:04.620
Kilani, Shadi: Thank you.

453
01:20:05.490 --> 01:20:07.350
Capstone Coach 1: yeah so it's not not entirely.

454
01:20:08.730 --> 01:20:11.520
Capstone Coach 1: yeah it's not a not at all easy.

455
01:20:12.780 --> 01:20:23.070
Capstone Coach 1: If you were doing if you wanted to to do this, instead of recognizing digits you want to recognize males and females let's say boys and girls something like that.

456
01:20:24.510 --> 01:20:34.320
Capstone Coach 1: that's probably going to be easier because it's binary you only have to, but for each photograph somebody's got to look at it and saying boy okay girl, you know.

457
01:20:35.040 --> 01:20:43.290
Capstone Coach 1: That kind of thing and then you can input that into the computer and then that is used to train the neural network, along with the photograph.

458
01:20:45.090 --> 01:20:56.820
Capstone Coach 1: Okay, so, if you look at the data, the way it came in it came in as a training data set 60,000 images and as a validation data set an additional 10,000.

459
01:20:58.230 --> 01:21:01.650
Capstone Coach 1: The number of pixels well, that would be 28 times 28.

460
01:21:03.300 --> 01:21:03.870
Capstone Coach 1: and

461
01:21:06.840 --> 01:21:07.530
Capstone Coach 1: Wait a minute.

462
01:21:09.240 --> 01:21:12.870
Capstone Coach 1: This looks like i've coated wrong, this is very wrong.

463
01:21:14.610 --> 01:21:15.240
Capstone Coach 1: yeah.

464
01:21:16.260 --> 01:21:18.660
Capstone Coach 1: Because no, no, no it's fine.

465
01:21:19.860 --> 01:21:26.460
Capstone Coach 1: Because X zero is the first photograph right So if I go X T zero.

466
01:21:28.980 --> 01:21:29.880
Capstone Coach 1: dot shape.

467
01:21:31.590 --> 01:21:43.050
Capstone Coach 1: You just you just see the default one photographic Sierra photograph and you can see it's 28 by 28 so the total pixels was 28 times 28 784.

468
01:21:43.980 --> 01:22:06.240
Capstone Coach 1: The number of classes, the number of different digits that are found in the data, you can get that by using the unique function and not be in P dot unique and then you give it the name of the series, so this case whitey and the link will that actually if you say in NP not be unique.

469
01:22:07.260 --> 01:22:14.580
Capstone Coach 1: And why T the training data well it prints out all of the unique values that it finds in why.

470
01:22:15.870 --> 01:22:29.640
Capstone Coach 1: And the same thing with why V could get the same information there, so the number of distinct values and why is the length of the array So if you type in length and then NP unique.

471
01:22:30.840 --> 01:22:31.650
Capstone Coach 1: y te.

472
01:22:32.730 --> 01:22:35.790
Capstone Coach 1: You get 10 yeah 10 dishes okay.

473
01:22:37.020 --> 01:22:48.660
Capstone Coach 1: So that's being called classes, right here, and then just for the heck of it we're printing out the first image, you could pronounce more if you wanted to but you'll see that we're just using.

474
01:22:50.310 --> 01:22:54.630
Capstone Coach 1: The math lab map plot line to do to print out that image.

475
01:22:55.710 --> 01:23:11.400
Capstone Coach 1: notice that it says Gray it tells the, this is a code to math plot line that this is not a clear photo, this is a great now the reason that's important is because the color photo is going to have three images.

476
01:23:12.180 --> 01:23:34.620
Capstone Coach 1: And there'll be a green blue and green and a red image they will all have set 28 by 28 pixels so math plot live by telling the math plotline, that is, this is a great image it knows oh just one light layer only 784 pixels fine.

477
01:23:36.480 --> 01:23:52.530
Capstone Coach 1: If you say no it's color if you don't put this in there, it seems that it's color it's got three levels to it and it's going to try to basically display them by overlaying them on your monitor right because your monitor has three layers as well red blue green usually.

478
01:23:54.210 --> 01:24:06.720
Capstone Coach 1: um so then we were counting up here, the number of train images, the number of validation images 60 and 10 the total number of pixels which would be here.

479
01:24:07.800 --> 01:24:09.240
Capstone Coach 1: The width by the height.

480
01:24:10.350 --> 01:24:18.510
Capstone Coach 1: The number of glasses we've got that okay now now we get to something that's kind of important it's called reshape.

481
01:24:19.650 --> 01:24:26.640
Capstone Coach 1: resize reshape it allows us is a operator and let's see where does that come from.

482
01:24:28.440 --> 01:24:29.850
Capstone Coach 1: that's a lumpy operator.

483
01:24:31.980 --> 01:24:32.370
Capstone Coach 1: yeah.

484
01:24:34.770 --> 01:24:45.180
Capstone Coach 1: reshape allows you to take a lumpy array and to reconfigure it so i'm going to i'm going to at this point we have.

485
01:24:46.260 --> 01:25:08.550
Capstone Coach 1: let's see we have X T in there right there so i'm going to run this so i'm going to right click and say run the selection well whoops pixels is not defined I cheated all right, I need to run everything difference there like that okay.

486
01:25:09.870 --> 01:25:17.340
Capstone Coach 1: define what takes us actually years how many pixels are there in the photo all right that's it one all that.

487
01:25:18.450 --> 01:25:32.490
Capstone Coach 1: There we go it says images contains 784 pixels and 10 classes okay yeah, we know that so here now what i'd like to do is to say, well X T, what is the shape of X T.

488
01:25:33.570 --> 01:25:38.640
Capstone Coach 1: it's 6028 and 28 now what is extreme.

489
01:25:39.690 --> 01:25:40.200
Capstone Coach 1: shape.

490
01:25:42.330 --> 01:25:44.490
Capstone Coach 1: Oh, what happened oh.

491
01:25:45.630 --> 01:25:59.070
Capstone Coach 1: Well, you see what happened in the X T the incoming data is actually a three dimensional lumpy array 60,000 rows and then 28 that the dimensions and 28 dimensions.

492
01:25:59.790 --> 01:26:10.230
Capstone Coach 1: The reshape what it did was it says, let me take each one of these photographs and get rid of all those brackets and just make it 784 pixels Yahoo.

493
01:26:11.460 --> 01:26:22.650
Capstone Coach 1: Because the neural network doesn't care about the orientation of the photograph what line you're on what height Iran it's looking for patterns among those pixels so.

494
01:26:23.220 --> 01:26:31.950
Capstone Coach 1: The neural network does need to have this neural network, the nlp network doesn't need to have information on what line and what column around.

495
01:26:32.310 --> 01:26:42.060
Capstone Coach 1: So this will this is called a tensor and what we just did was we took a three dimensional lumpy array and make it a two dimensional tense are.

496
01:26:43.050 --> 01:26:48.390
Capstone Coach 1: Basically, and so this allows this will run faster on the neural network.

497
01:26:49.320 --> 01:27:04.740
Capstone Coach 1: And we don't even know about it really let's do the same thing of the validation data so let's let's choose the next couple of lines and go run that and you'll see the same thing happens again is that it goes from three down to two dimensions.

498
01:27:06.570 --> 01:27:29.790
Capstone Coach 1: And we can then save it out as a csv file if you grab all this and and and basically say run again then it's going to take the data that we have and put it out into two csv files one for train caltrain csv and one called validate csv okay.

499
01:27:32.340 --> 01:27:41.190
Capstone Coach 1: Before we do that let's let's make it let's make life easy down here, where it says model, one true take make that a false.

500
01:27:42.660 --> 01:27:43.050
Capstone Coach 1: Okay.

501
01:27:44.970 --> 01:27:53.970
Capstone Coach 1: alrighty and then run the whole program because basically the what's the run will do is just run what we were just doing.

502
01:27:55.290 --> 01:27:58.680
Capstone Coach 1: it's going to redo it start from the front and from the top.

503
01:27:59.730 --> 01:28:03.480
Capstone Coach 1: From south shows you the little number four, which is the first image.

504
01:28:04.860 --> 01:28:11.340
Capstone Coach 1: yeah or I don't know if we change the number of number nine I think in the code, I may have changed that number two.

505
01:28:14.220 --> 01:28:18.000
Capstone Coach 1: yeah which I change it to a number nine so that's the ninth image actually.

506
01:28:19.170 --> 01:28:27.570
Capstone Coach 1: So right now it's reading the data and reshaping the data and setting things up that we're ready to go.

507
01:28:28.710 --> 01:28:33.150
Capstone Coach 1: And it just a moment it's it'll give us another i'm done cursor.

508
01:28:38.820 --> 01:28:52.080
Capstone Coach 1: The part of what's happening here is the the y data are are getting encoded they are becoming one hot column So you see why train is equal to two categorical why key.

509
01:28:52.500 --> 01:29:01.830
Capstone Coach 1: This is creating TIM one hot columns from the training data, and then we do the same thing down here 10 one hot columns for the validation data.

510
01:29:02.760 --> 01:29:20.550
Capstone Coach 1: And then i'm getting kind of cool here i'm putting all of this into a data frame didn't really need to do this and then and then pushing it from the data frame out to a csv file, this is not very cool, this is not very convenient what he called conservative.

511
01:29:22.980 --> 01:29:32.220
Capstone Coach 1: slower, a lot more unnecessary work didn't really need to do that, because what we're going to be operating on in 15 or network or X train why train.

512
01:29:32.760 --> 01:29:52.050
Capstone Coach 1: And then the X and the y validate that's what will be using, but I wanted to save it off, and so I did all this stuff here which really just confuses the they should probably not a good thing let's scroll down to the start of the real code which is down here, where it says false.

513
01:29:53.280 --> 01:30:07.650
Capstone Coach 1: And now we're going to work through this area, piece by piece, so you sort of see what's going on at this point, the data set up if I say if I say X train like that Shay.

514
01:30:08.850 --> 01:30:11.430
Capstone Coach 1: yeah 60,007 84.

515
01:30:12.840 --> 01:30:24.570
Capstone Coach 1: X underscore validate shape same sort of thing it'll be 716 thousand 794 Okay, so now we're ready to start doing some deep learning.

516
01:30:25.740 --> 01:30:34.950
Capstone Coach 1: I like that word so first of all that we're going to start the clock, but we're not going to start the clock because because we're going to goof around so much the time does mean too much.

517
01:30:35.250 --> 01:30:50.790
Capstone Coach 1: But it will timeout if you run this if you change this to true and run it it'll run the whole code, but I want to want to this piece by piece or explained kind of what's going on here, first of all here is how you set the random seed and tensor.

518
01:30:52.200 --> 01:31:02.130
Capstone Coach 1: tf tensorflow dot random that set seed and then some number like that, if you don't do this before you build a model.

519
01:31:03.510 --> 01:31:21.030
Capstone Coach 1: Certainly well what's going to happen is your results will vary from one run to the next run I don't like that I like to be able to make changes and see what happens when I make those changes and not worry about randomness of the data itself.

520
01:31:22.080 --> 01:31:30.180
Capstone Coach 1: So I like to set the seed, before I start tensorflow and then we're going to build a model, so the number of fire 253.

521
01:31:31.020 --> 01:31:38.970
Capstone Coach 1: down to number 262 you see all of that this has to do with defining what the neural network looks like.

522
01:31:39.270 --> 01:31:48.120
Capstone Coach 1: This is an m LP multi layer preceptor on network, so I know it's going to be dense everything's connected to everything else.

523
01:31:48.450 --> 01:31:55.230
Capstone Coach 1: And the the first layer comes in, then the second layer, then the third and they're all one after another connected.

524
01:31:56.010 --> 01:32:11.280
Capstone Coach 1: In this case, i'm just going to have two layers i'm going to have one hidden layer you see it here called layer one and then we'll have one output layer called layer out right how many neurons are we going to have in that first layer

525
01:32:12.990 --> 01:32:14.340
Capstone Coach 1: 794.

526
01:32:15.480 --> 01:32:17.910
Capstone Coach 1: The number of pixels so we have ooh.

527
01:32:19.230 --> 01:32:21.480
Capstone Coach 1: Okay 784.

528
01:32:22.590 --> 01:32:29.160
Capstone Coach 1: The input dimensions, will be the number of features which is 784.

529
01:32:30.390 --> 01:32:38.700
Capstone Coach 1: Number of neurons does not have to equal, the number of inputs so that's why they're asking for both numbers here's the number of inputs.

530
01:32:39.150 --> 01:32:49.320
Capstone Coach 1: And then put dimension and then this is the number perceptions which could be less than or even more than I saw an example where they were using like.

531
01:32:51.960 --> 01:32:57.270
Capstone Coach 1: To me, usually more preset fronts and data it's a little overkill.

532
01:32:58.290 --> 01:32:59.370
Capstone Coach 1: I don't like to get.

533
01:33:00.420 --> 01:33:19.650
Capstone Coach 1: This least as a start and then, then it says, let me move this out of the way Colonel regularization equals none hmm This is where you set l one l to regularization if you're going to use it and we will play with this and just a minute.

534
01:33:20.820 --> 01:33:32.790
Capstone Coach 1: Activity regularization another place where you can institute regularization and your model activation our email you Oh yes, thank goodness.

535
01:33:34.530 --> 01:33:45.750
Capstone Coach 1: we're going to talk about activation, but this is, this is the one that you use in deep learning, if you have a very big network use our e l you.

536
01:33:46.200 --> 01:33:59.670
Capstone Coach 1: And the reason is the computation of this activation is very fast very efficient it's going to run a lot faster if you use something like an H, or even sigmoid or something like that slower.

537
01:34:00.570 --> 01:34:06.690
Capstone Coach 1: than you want to make sure you name the layer so that when you look at the output, you know what you're looking at it in one.

538
01:34:08.250 --> 01:34:18.600
Capstone Coach 1: And then, here comes the output layer make sure the activation is appropriate for the target here, it says soft Max because we have a nominal target.

539
01:34:19.290 --> 01:34:30.000
Capstone Coach 1: If I had a binary target, it would be sigmoid and, if I have a an interval target, it would be none, and we look at this, I think, in our last week.

540
01:34:30.720 --> 01:34:32.910
Capstone Coach 1: And then the name i'm just always going to call it out, but.

541
01:34:33.750 --> 01:34:45.420
Capstone Coach 1: And then you add layer one lead layer out and then summary this model summary will show you your work so let's let's grab the statements, all the way from tf random.

542
01:34:45.840 --> 01:34:52.470
Capstone Coach 1: down to model summary and let's right click on that and run and see what we get there's what we get.

543
01:34:53.460 --> 01:35:08.790
Capstone Coach 1: It shows you the we have two layers a hidden layer and an output layer in the hidden layer we have 615,000 440 weights and that's much.

544
01:35:09.720 --> 01:35:29.820
Capstone Coach 1: So that's because we have 784 inputs going into 794% so 784 times 784 is more than 615,000 that's where that big number comes from.

545
01:35:30.750 --> 01:35:45.030
Capstone Coach 1: And then the next number what it's doing it says Okay, I have 600 784 neurons going into 10 so that means I have 7840 weights wait a minute.

546
01:35:45.660 --> 01:35:59.190
Capstone Coach 1: This is 700 and 8050 or the extra 10 come from they came from this, these are the bias terms that are associated with every preceptor on so not only do you have weights on the.

547
01:35:59.880 --> 01:36:23.550
Capstone Coach 1: channels of information that are connecting one layer to another, you also have a way to special way the intercept or the bias wait that's on or associated with each one of these precepts total 623,290 weights with that number of weights, we should get a pretty good fit right.

548
01:36:24.660 --> 01:36:32.730
Capstone Coach 1: In a rare regression model you'd probably have 784 coefficients maybe not much more.

549
01:36:34.110 --> 01:36:48.240
Capstone Coach 1: Okay, so now we're ready to set up the conditions that we're going to use for running the model, this is done by the compile statements so here opt that little guy.

550
01:36:48.660 --> 01:36:59.670
Capstone Coach 1: defines what the optimizer is going to do, or how its configured, this is the delta optimizer and we're setting a learning rate there at 6.5.

551
01:37:01.260 --> 01:37:16.650
Capstone Coach 1: that's what we're going to use I basically I experimented with this learning rate trying low values and high values, to see what works, the best what gave me the best fit and 6.5 was close to the best.

552
01:37:17.670 --> 01:37:37.770
Capstone Coach 1: And then we're going to go model compile which describes the type of network, we have the loss function is categorical cross entry entropy, which is a definition that hey we have a nominal target the optimizer is op T, which is added Delta.

553
01:37:38.910 --> 01:38:01.830
Capstone Coach 1: And then the metrics that we want to return as it's running the fifth is accuracy and so accuracy will be part of the data stream that comes from the estimation process in the neural network so let's let's grab the OPS line and the compiler line right click run.

554
01:38:04.770 --> 01:38:12.900
Capstone Coach 1: nothing happens right little bit all you're doing they're setting up the description of the further description of what the neural network looks like.

555
01:38:13.650 --> 01:38:23.370
Capstone Coach 1: Now we get down to the number crunching part which is the fit part right here, you see history equal to model that fit this is going to kicked off.

556
01:38:23.910 --> 01:38:39.270
Capstone Coach 1: A routine that's going to estimate those weights and so let's take everything from pixels all the way down to print runtime let's take that whole thing right there and let's go right click and run on that.

557
01:38:41.790 --> 01:38:42.450
Capstone Coach 1: Okay.

558
01:38:43.620 --> 01:38:58.080
Capstone Coach 1: Now it's just it's going to kick off here in just a second there it goes so you'll see you'll watch rather the the verbose the verbose option was set to to.

559
01:38:58.830 --> 01:39:12.570
Capstone Coach 1: If you set this to zero you wouldn't see any output at all, but because we said to you're getting this condensed version of the output and you can see, is showing you the epoch number seven out of 20 here.

560
01:39:13.770 --> 01:39:19.530
Capstone Coach 1: The length of time that it took to run that he bought four seconds, it seems, on most of them.

561
01:39:20.670 --> 01:39:27.420
Capstone Coach 1: And then the last function for the training data, the accuracy for the training data.

562
01:39:28.560 --> 01:39:34.950
Capstone Coach 1: The data loss function for the validation data, and then the accuracy for the validation.

563
01:39:36.240 --> 01:39:45.300
Capstone Coach 1: One word here the loss is not the the misclassification rate the losses, the Cross entropy.

564
01:39:46.020 --> 01:39:57.330
Capstone Coach 1: categorical cross entropy calculation, which is a function of the estimated probabilities not the number of miss classifications, and so.

565
01:39:58.050 --> 01:40:07.080
Capstone Coach 1: You you sometimes it may not see that obvious now when I when the last goes to zero and notice the accuracy goes to one.

566
01:40:07.800 --> 01:40:22.230
Capstone Coach 1: that's an indication that Oh, there is no, there are no miss classifications in this area there's plot let's take scroll back and see the graphs so the blue line is the validation number and the red line is the.

567
01:40:23.400 --> 01:40:24.210
Capstone Coach 1: Training data.

568
01:40:25.650 --> 01:40:32.250
Capstone Coach 1: The train data behave so last goes going to zero here and the accuracy going to one on the bottom graph.

569
01:40:33.390 --> 01:40:43.590
Capstone Coach 1: divination that's being stubborn it doesn't go to one on the almost zero on loss and it doesn't go to one on the accuracy, so there.

570
01:40:44.220 --> 01:40:51.180
Capstone Coach 1: may be claims here that we have some overfitting let's take a look at the confusion matrix here.

571
01:40:51.960 --> 01:40:58.830
Capstone Coach 1: And the confusion matrix for the training data no miss classifications it's all zeros except for the diagonal.

572
01:40:59.760 --> 01:41:19.500
Capstone Coach 1: Now validation data well it's not bad really the misclassification rate here is 1.4 or 5% 145 out of 10,000 that's what it now, this is what i'm getting the you may get something different, because you're on a different computer probably different type of computer and.

573
01:41:21.060 --> 01:41:33.540
Capstone Coach 1: You know, when I set that random seed on my computer, it may be set to a slightly differently than it is on your machine, and this is something i've actually kind of curious about did anybody get a.

574
01:41:33.540 --> 01:41:38.670
Capstone Coach 1: 140 5.49 you got what no.

575
01:41:39.090 --> 01:41:42.240
Swanson J, Charles: 1.49 I got one point.

576
01:41:42.480 --> 01:41:43.620
Swanson J, Charles: or one or eight.

577
01:41:43.980 --> 01:41:44.490
Okay.

578
01:41:46.170 --> 01:41:57.180
Capstone Coach 1: that's what I thought it close but not exact so now, if you were to run this again on your computer, you should get the same number 1.48 1.49 whatever it is.

579
01:41:57.720 --> 01:42:08.340
Capstone Coach 1: You should get the same number now the reason we're not getting between machines, the same number is because you have a different processor your cpu is different and you're on windows.

580
01:42:08.760 --> 01:42:25.560
Capstone Coach 1: So are not I don't know that necessarily are, but your operating system, and then the actual computer hardware you're using will change the red and blue sequence that you're getting and so your numbers will be close but not exactly the same right.

581
01:42:26.160 --> 01:42:27.810
Fireman, Karen: You know Jones yeah.

582
01:42:28.770 --> 01:42:31.020
Fireman, Karen: different numbers, when I ran it twice.

583
01:42:31.680 --> 01:42:32.970
Capstone Coach 1: Oh, really.

584
01:42:33.360 --> 01:42:35.490
Fireman, Karen: yeah 148 141.

585
01:42:36.960 --> 01:42:53.040
Capstone Coach 1: Okay well that's curious, if you would please make sure well, let me see here yeah you see the statement number 252 yes, possibly, that was not run the with the issue, running from scratch.

586
01:42:54.360 --> 01:42:54.810
Fireman, Karen: I mean.

587
01:42:55.410 --> 01:42:56.310
Fireman, Karen: Originally, I did.

588
01:42:57.000 --> 01:43:00.960
Capstone Coach 1: yeah this know this, should this should make things.

589
01:43:02.820 --> 01:43:14.520
Capstone Coach 1: compatible, you know I mean repeatable So if you say let's let's change let's change this thing that says model, one faults up there let's make that true like this.

590
01:43:15.420 --> 01:43:25.680
Capstone Coach 1: And, and then we will clear the Colonel so we're going up here to console restart Colonel like that, yes, please clear it.

591
01:43:26.430 --> 01:43:37.230
Capstone Coach 1: And then we're going to run it and see what number, we should get a number, which is the same if we were to restart and so forth, so i'm going to click click go.

592
01:43:38.160 --> 01:43:48.840
Capstone Coach 1: And now it's starting with reading the data again so and pre processing the data this takes a minute or 30 seconds, or something like that it's fairly quick.

593
01:43:52.890 --> 01:44:07.050
Capstone Coach 1: But Karen yeah if you do this if you if you make sure that that random set seed is executed and you I was fair my current like I don't think you should get the same number every time.

594
01:44:08.280 --> 01:44:17.940
Capstone Coach 1: If you're not getting the same number number time that's curious it's not a disaster or anything like that it doesn't mean you did anything wrong, it just means your computer is a little different.

595
01:44:20.100 --> 01:44:20.370
Swanson J, Charles: But.

596
01:44:20.670 --> 01:44:21.780
Fireman, Karen: It probably is.

597
01:44:22.710 --> 01:44:31.200
Capstone Coach 1: And I I don't I would hate to see what is that is that a an analytics computer or your home computer.

598
01:44:31.860 --> 01:44:41.400
Fireman, Karen: It is, but I think actually we avoided that section that you're making a good point so we didn't set the scene, maybe when we ran it together just now so maybe that's why it came different.

599
01:44:41.430 --> 01:44:42.630
Capstone Coach 1: Oh, it could be.

600
01:44:43.590 --> 01:44:44.790
Capstone Coach 1: yeah it'd be.

601
01:44:46.200 --> 01:44:49.620
Capstone Coach 1: Definitely could be that if you don't set the seed you'll get different.

602
01:44:49.620 --> 01:45:06.270
Capstone Coach 1: Results every time and that's Okay, but anyway so it's cranking away four seconds times 20 is 180 seconds that's about two and a half, but it's I guess like that.

603
01:45:07.050 --> 01:45:09.420
Fireman, Karen: Okay, I did get the 148 that I got original.

604
01:45:09.810 --> 01:45:16.620
Capstone Coach 1: Okay, now, while this is running i'd like you to open up a if you go back to.

605
01:45:19.440 --> 01:45:30.540
Capstone Coach 1: This guy the week 11 and a if download for me the F of excel file, it says nlp comparison so i'd like to show you some numbers that I collected.

606
01:45:31.200 --> 01:45:42.930
Capstone Coach 1: I ran several different configurations on this code and you'll sheet your see some interesting things I think they're so we'll open that up while we're running this off.

607
01:45:44.640 --> 01:45:45.330
Capstone Coach 1: and

608
01:45:48.570 --> 01:45:51.960
Capstone Coach 1: Okay, here we go we're at number 17.

609
01:45:54.330 --> 01:45:54.870
Capstone Coach 1: Okay.

610
01:45:59.610 --> 01:46:01.740
Capstone Coach 1: You see, the accuracy turned to one at about.

611
01:46:03.600 --> 01:46:04.740
Capstone Coach 1: 13 or 14.

612
01:46:06.000 --> 01:46:06.570
Capstone Coach 1: pretty good.

613
01:46:07.890 --> 01:46:11.550
Capstone Coach 1: pretty good action on the train data could be better on validation so.

614
01:46:12.960 --> 01:46:13.590
Capstone Coach 1: What.

615
01:46:14.760 --> 01:46:22.470
Capstone Coach 1: There you go alright so yeah I got 145 1.4 or five again, so I got the same results.

616
01:46:24.540 --> 01:46:27.690
Capstone Coach 1: And kind of boring, as usual, all right.

617
01:46:28.740 --> 01:46:37.770
Capstone Coach 1: But so let's take a look at that excel spreadsheet i'd like to describe what's the talk about what's in there for a minute, here we go turned it on.

618
01:46:39.120 --> 01:46:39.660
Capstone Coach 1: and

619
01:46:43.590 --> 01:46:45.210
Capstone Coach 1: whoops not so fast.

620
01:46:50.880 --> 01:47:02.970
Capstone Coach 1: Okay, so this is, I did a series of runs with this code and what I did was I changed the code little bit right now we just have.

621
01:47:03.450 --> 01:47:19.470
Capstone Coach 1: A two layers we have the first layer layer number one set to 784% trans and then we have layer number two of the output layer which here is to noted, as layer five set to 10% trots.

622
01:47:20.550 --> 01:47:29.130
Capstone Coach 1: Then I did some experimenting with different other layers in between the two, you can see what happens to the number of weights.

623
01:47:29.820 --> 01:47:41.730
Capstone Coach 1: That they're the it's a large number of weights, but not overly impossible to work with and then this is the misclassification number for training and validation.

624
01:47:42.570 --> 01:47:55.980
Capstone Coach 1: This is the training accuracy, the validation accuracy and then the difference between the two of them, so you can see the validation accuracy is about one and a half 1.4%.

625
01:47:57.090 --> 01:48:17.190
Capstone Coach 1: off below the training accuracy, the accuracy for the training data which in most cases is 100% and then the time that it took to run that configuration and the spirits, all the way from a little less than a minute to about five minutes in one case.

626
01:48:18.540 --> 01:48:28.320
Capstone Coach 1: All right, let's talk about these three layers that well the configuration that we are just running right now that was 20 epochs wait a minute.

627
01:48:30.030 --> 01:48:32.190
Capstone Coach 1: where's the there, it is the that size.

628
01:48:33.720 --> 01:48:54.570
Capstone Coach 1: Is the, why should move it around don't do that, so this is the number of the box on the left and then the second column is the batch size now in the code, we have 28 bucks in a batch size of 128 and let's see if I can find that here.

629
01:48:58.650 --> 01:48:59.670
Capstone Coach 1: looks like.

630
01:49:03.570 --> 01:49:11.010
Capstone Coach 1: should be that one, yes it is see the, the number of validation and this classifications is 145.

631
01:49:12.210 --> 01:49:15.870
Capstone Coach 1: So it's line number 10 and spreadsheet is is what i'm.

632
01:49:16.980 --> 01:49:19.080
Capstone Coach 1: What we're seeing right now on the screen.

633
01:49:20.430 --> 01:49:27.930
Capstone Coach 1: Now what and what i'm doing here is i'm experimenting with changing the number of epochs changing the BAT size.

634
01:49:29.250 --> 01:49:30.120
Capstone Coach 1: And also can.

635
01:49:31.590 --> 01:49:37.800
Capstone Coach 1: expect experimenting with regularization there are two forms of regularization that are available.

636
01:49:38.580 --> 01:49:53.970
Capstone Coach 1: One is called the traditional regularization l one and two, and you can see, I prefer to use last so, which is the one you could use to you, could you could configure your network to do that you wanted to.

637
01:49:54.990 --> 01:50:09.240
Capstone Coach 1: And then I also experimented with adding a another hidden layer in between the the first one, which is very large 794 nodes with a layer in between.

638
01:50:10.440 --> 01:50:23.910
Capstone Coach 1: That and the the final layer over here the D is a different form of regularization it's called drop out and you put this into your network, what happens is.

639
01:50:24.660 --> 01:50:32.340
Capstone Coach 1: The algorithm the optimization algorithm will go through and look for weights that are very low, and it will get rid of them.

640
01:50:33.330 --> 01:50:57.030
Capstone Coach 1: So it drops, the requirement to have a dense array and goes to looking for connections that don't seem to matter and killing them and reducing them now what percentage is reduced you control that in this case I said point four or 540 5% kill off 45% of the weights from one.

641
01:50:58.080 --> 01:51:09.360
Capstone Coach 1: You know Level one layer to the next kill off 45% of the weights, this is another form of regularization and in the least in tensorflow and charisse.

642
01:51:09.780 --> 01:51:19.440
Capstone Coach 1: Is what they're doing is they're saying the weights can be extreme very large very small, and we want to identify those extremes and get rid of them.

643
01:51:20.070 --> 01:51:30.690
Capstone Coach 1: regularization this at all this l one will do that somewhat automatically and the algorithm but dropouts does it manually head dummy by force.

644
01:51:31.800 --> 01:51:43.800
Capstone Coach 1: It you're either allowed in, or you're thrown out and 45% are going to be thrown out I could have chosen something smaller maybe 10% or 20%.

645
01:51:44.460 --> 01:51:53.520
Capstone Coach 1: But this is this seemed like a starting good starting place so let's take a look at what what happened here, first of all.

646
01:51:54.150 --> 01:52:14.190
Capstone Coach 1: If you run up the number of the box you'll see, most of them were done at 20 but then I have one in 52 and 50 and 100 and so forth, did I get results that are any better or smaller well let's take a look at that let's compare the blue line here, where I have a box of 50.

647
01:52:15.360 --> 01:52:20.820
Capstone Coach 1: And you'll see in the blue line i'm the on using.

648
01:52:21.870 --> 01:52:32.370
Capstone Coach 1: l one regularization and the in the line just above it I have 28 box, also with the bed size of everything is the same.

649
01:52:32.910 --> 01:52:41.880
Capstone Coach 1: And the same number precept ron's this same regularization the same number of weights, the same everything is same.

650
01:52:42.420 --> 01:52:52.440
Capstone Coach 1: The only thing that's different is the number of the boxes increased and look what happened to the misclassification for the validation data it decreased.

651
01:52:53.070 --> 01:53:07.590
Capstone Coach 1: goes from 141 to 125 is that a meaningful number i'm not so sure it's in terms of accuracy we're talking about going from 98.6 to 98.8.

652
01:53:08.460 --> 01:53:30.450
Capstone Coach 1: it's still in both cases says you, you may have an overfitting problem here, because the training data accuracy is 100% and you're not able to achieve that with the validation data you're about 1.3% 1.4% less accuracy without than in the validation data so.

653
01:53:33.630 --> 01:53:42.450
Capstone Coach 1: that's what happens when you increase the number of the box so ordinarily what I do is I experiment around using those small number of the box like 20.

654
01:53:43.050 --> 01:53:53.400
Capstone Coach 1: or 10 even and then I try I try to get things to sort of optimum with respect to the number of preceptor ons and also regularization.

655
01:53:54.000 --> 01:54:08.160
Capstone Coach 1: And then i'll jack up the number of the box good when I do that let's see what how long it took to run these two okay 2.9 minutes for 50 bucks and, as you might expect, one point for about half.

656
01:54:09.180 --> 01:54:17.460
Capstone Coach 1: For 20 so that's why you don't just jump out at the sale that's 1008 bucks nope nope no start with something less.

657
01:54:18.150 --> 01:54:26.730
Capstone Coach 1: get it to work well and then ratchet up the number of the box, otherwise you will be spending forever trying to get this thing done.

658
01:54:27.690 --> 01:54:41.460
Capstone Coach 1: notice, I did run make a run here with 100 a box and, as a matter of fact that we're online here, and the one 100 is exactly like the number with the let's see what happened.

659
01:54:43.710 --> 01:54:54.720
Capstone Coach 1: Look at this by going to 100 ad packs the number of misclassification skills gets worse for the validation data goes from 125 to 135.

660
01:54:55.500 --> 01:55:12.180
Capstone Coach 1: Of course, the accuracy decreases and, of course, the runtime as almost five minutes and 100 epochs so when you increase the number of the box it doesn't necessarily get better and if it does get better it may only just slightly get better.

661
01:55:14.160 --> 01:55:25.950
Capstone Coach 1: Okay let's look at what happened when we jacked up the number of perceptions in that first hidden layer going from 784 to.

662
01:55:26.670 --> 01:55:43.260
Capstone Coach 1: 1024 was there any benefit to that will take a look at these run number four and number five in the spreadsheet these are identical, except for the size of that first layer did we look at the number of graduates, first of all.

663
01:55:44.640 --> 01:55:55.350
Capstone Coach 1: 1.3 million versus 1 million, and then, if we go over to the right, you can see, there is a big somewhat difference in the runtime.

664
01:55:57.090 --> 01:56:03.900
Capstone Coach 1: The misclassification is to 71 versus 280.

665
01:56:05.220 --> 01:56:15.510
Capstone Coach 1: And for the training data and then 182 verses 178 for validation data, so the training accuracy is about the same.

666
01:56:16.500 --> 01:56:34.380
Capstone Coach 1: With or without the regularization and the validation accuracy is almost the same with or without regularization so that being the battle i'm sorry, by increasing the number of perceptions there didn't really give us much of a benefit just took more time.

667
01:56:35.610 --> 01:56:46.920
Capstone Coach 1: So that's why, and this is what I expected to see if you increase the number of perceptions and that, personally, or to go above the number of inputs, you probably going to be.

668
01:56:47.370 --> 01:57:00.690
Capstone Coach 1: Running longer, but not getting any better results it's not going to get worse, but my not much better at all so that's why I just stuck with 187 84 all the way through.

669
01:57:01.770 --> 01:57:05.850
Capstone Coach 1: I should have tried going a little somewhat lower than this, I didn't do that.

670
01:57:07.470 --> 01:57:13.950
Capstone Coach 1: it's better to go lower if you expect lots of multi linearity in the data now with respect to photographs.

671
01:57:17.790 --> 01:57:35.040
Capstone Coach 1: It depends on the photograph right for the what we're looking at digits on numbers black and white, you probably don't expect to see a lot of multicolor in the area and let's the the you know between rose between the the between the roast in the photograph right so.

672
01:57:36.150 --> 01:57:43.290
Capstone Coach 1: I wouldn't worry so much about that here, but there would be other cases and facial recognition, where you might have some problems with that.

673
01:57:44.550 --> 01:57:51.480
Capstone Coach 1: Now let's take a look at this regularization thing and i'll show you how this is done in the code, if you go to the code.

674
01:57:53.100 --> 01:58:05.610
Capstone Coach 1: over here which we've been working on now we're going back here to line number 249 and you can see the layer i'm going to put a layer in between this and.

675
01:58:06.840 --> 01:58:12.120
Capstone Coach 1: Well, let me make sure that I have the right import statements.

676
01:58:17.610 --> 01:58:18.330
Capstone Coach 1: Okay.

677
01:58:19.560 --> 01:58:21.570
Capstone Coach 1: i'm gonna cheat and go down to down down.

678
01:58:22.950 --> 01:58:24.930
Capstone Coach 1: Okay model to.

679
01:58:26.070 --> 01:58:34.680
Capstone Coach 1: Up better yet let's just go down the model to and take a look at that now so up here, where it says model to false.

680
01:58:35.730 --> 01:58:48.450
Capstone Coach 1: You can leave that as false, we can run this one, at a time, so to speak, but you'll see there are some lines that are commented out line number three oh to remove the comment.

681
01:58:50.160 --> 01:58:54.030
Capstone Coach 1: And you see it says layer one a equal to.

682
01:58:55.050 --> 01:58:55.830
Capstone Coach 1: drop out.

683
01:58:57.840 --> 01:59:10.830
Capstone Coach 1: So what this says, is it, I want you to inspect the weights between layer one and layer two and remove any that are inconsequential.

684
01:59:11.460 --> 01:59:27.420
Capstone Coach 1: they're not adding to the forecast so that's the dropout regularization that how you implement the dropout rate there's another one down here that is going to go between the second layer and the output layer see that.

685
01:59:28.950 --> 01:59:35.670
Capstone Coach 1: So that's one way to do regularization, this is the dropout way of doing regularization.

686
01:59:37.410 --> 01:59:42.450
Capstone Coach 1: And then you over here you'll see it says, Colonel regular riser.

687
01:59:44.040 --> 01:59:53.910
Capstone Coach 1: Regular risers dot l one that that whole thing is setting up l one regularization for the first layer

688
01:59:55.140 --> 02:00:03.150
Capstone Coach 1: And then you can do the same thing for any other layer I don't do it with the output layer and I don't do that no.

689
02:00:03.780 --> 02:00:17.640
Capstone Coach 1: But with respect to the other layers where you're just estimating the weights of these preceptor ons you can do the regularization by adding regular risers dot l one or two.

690
02:00:18.690 --> 02:00:30.690
Capstone Coach 1: And then the learning rate here are the penalty one E minus six now, the larger the penalty, the more boring things get because this.

691
02:00:31.530 --> 02:00:37.350
Capstone Coach 1: If you put a large penalty in here then it's gonna it's going to throw off quite a few weights.

692
02:00:38.250 --> 02:00:57.540
Capstone Coach 1: You say 45%, but it may decide to throw off more than that, if you use this penalty is actually changes i'm sorry the dropout rate is shown up here dropout equal 2.45, and so the the dropout is going to remove 45% of the weights between.

693
02:00:58.620 --> 02:01:00.000
Capstone Coach 1: Between these layers.

694
02:01:01.080 --> 02:01:11.490
Capstone Coach 1: going from one layer to the next 45% of the connections will be dropped now if you use regularization l one up here, there are no weights are dropped.

695
02:01:12.300 --> 02:01:26.010
Capstone Coach 1: The weights themselves are modified, they are suppress their they are kept from becoming too large, because the penalty then becomes increasingly increases as the the size of the weight.

696
02:01:26.400 --> 02:01:35.490
Capstone Coach 1: increases, this is an absolute value, by the way, so if the penalty function here is large large being like point one.

697
02:01:37.350 --> 02:01:49.740
Capstone Coach 1: Then it's going to dampen or severely affect the freedom to set those weights to high values and low values so ordinarily.

698
02:01:50.280 --> 02:02:06.390
Capstone Coach 1: If i'm doing one here I would set this to have value, where I see no effect and then go and change it ratchet it down a little bit so let's turn off the drop well we'll leave that we've already got the dropout turned on.

699
02:02:09.180 --> 02:02:10.920
Capstone Coach 1: let's turn it off again let's.

700
02:02:12.000 --> 02:02:21.120
Capstone Coach 1: just turn off layer one and turn off layer two like that, so we have no dropout going on now, we just have l one regularization.

701
02:02:22.260 --> 02:02:42.900
Capstone Coach 1: let's see Okay, so this is this network has 784% drones on the first layer hidden layer 256 in the second hidden layer and then of course it has the output layer with the same kind of thing soft Max X activation and things like that now.

702
02:02:44.010 --> 02:02:57.240
Capstone Coach 1: We are not going to add layer number two, we are only going to add layer number one and the output layers so that means that the second layer will be ignored it's just going to fit.

703
02:02:57.780 --> 02:03:07.230
Capstone Coach 1: 784% tron So the first layer and then the output layer layer here, I could remove these comments and activate this part right.

704
02:03:08.280 --> 02:03:11.250
Capstone Coach 1: And so, then everything else.

705
02:03:12.930 --> 02:03:14.070
Capstone Coach 1: looks correct.

706
02:03:15.900 --> 02:03:23.670
Capstone Coach 1: And oh here's what I want to change the box and size let's make this so that we can we don't we're not here all night long.

707
02:03:24.510 --> 02:03:45.780
Capstone Coach 1: let's change this 10 to 2028 box and let's go ahead and leave 256 because that's, the higher the number, the faster it's going to run right so we'll leave that and now let's select everything from start time to stop time which is down here somewhere.

708
02:03:48.090 --> 02:03:56.520
Capstone Coach 1: Oh, there we go stop time right there so we're going to go into say select that guy and then click run selection.

709
02:03:59.460 --> 02:04:00.990
Capstone Coach 1: i'm here we go.

710
02:04:02.310 --> 02:04:18.000
Capstone Coach 1: First, you can see the network it's set up with 620 3000 weights, we have one hidden layer at 784 and then we have 10 you know, in the output layer you see it's running fairly quick here.

711
02:04:19.230 --> 02:04:30.690
Capstone Coach 1: This is running with l one regularization you can't see any difference in the output, the only difference you're going to see is in the final.

712
02:04:32.850 --> 02:04:46.290
Capstone Coach 1: Miss misclassification rate will be a little different if you're using regularization it may go up and may go down, depending upon how what that number is i've set it to.

713
02:04:47.790 --> 02:04:52.200
Capstone Coach 1: 10 to the minus 618 minus six.

714
02:04:53.370 --> 02:04:55.590
Capstone Coach 1: So that's five zeros on one.

715
02:04:57.150 --> 02:05:01.800
Capstone Coach 1: And here we go we're converging quickly on epoch 20.

716
02:05:04.200 --> 02:05:05.790
Capstone Coach 1: and see what happens here.

717
02:05:25.980 --> 02:05:26.190
Capstone Coach 1: Coming up.

718
02:05:33.570 --> 02:05:44.130
Capstone Coach 1: Here we go so perfect accuracy on the training data and 1.44% on the.

719
02:05:45.840 --> 02:05:51.660
Capstone Coach 1: You know, on the validation data 144 misclassification is not bad.

720
02:05:52.710 --> 02:06:01.500
Capstone Coach 1: not bad at all now let's go in and change the one let's instead of going minus six let's go minus 12.

721
02:06:02.940 --> 02:06:03.420
Capstone Coach 1: Double it.

722
02:06:05.220 --> 02:06:15.480
Capstone Coach 1: Now, what would you think's going to happen now that we increase the we actually decreasing the learning rate the penalty here, what do you think's going to happen.

723
02:06:16.920 --> 02:06:17.490
Capstone Coach 1: Anybody.

724
02:06:20.070 --> 02:06:22.200
Swanson J, Charles: we're gonna overfit the data even more.

725
02:06:22.740 --> 02:06:40.830
Capstone Coach 1: Yes, most I don't know but The thing is, is that what you get is no change from not doing regularization if you make this smaller really small, then the number should get out are going to be the same as if you did know regularization at all.

726
02:06:42.210 --> 02:07:02.970
Capstone Coach 1: And so, if I were to run this i'm going to get I make it the number of it's a little different and what i'm going to do, then, is, I will set those base conditions on the base, maybe, maybe it's 100 and I don't know what but in a run oops started from the very top forget it.

727
02:07:04.050 --> 02:07:17.250
Capstone Coach 1: So the point, the point that i'm trying to make here is that, how do you set the regularization start with a really small number and then ratchet it down until you start to see some changes in the output.

728
02:07:18.210 --> 02:07:23.670
Capstone Coach 1: And that's where you probably want to leave it the other way to go, is to start with some.

729
02:07:24.240 --> 02:07:41.880
Capstone Coach 1: Big numbers, the question is what's big that depends on the data depends on how many weights, you have all sorts of things so it's way easier to start with a small number and then make it entertaining gradually increase it see what what happens versus going to going the other way around.

730
02:07:43.980 --> 02:07:48.030
Capstone Coach 1: Now, instead of using that one you could just use the drop out let's.

731
02:07:49.170 --> 02:08:05.490
Capstone Coach 1: Well i'm not going to do that now because i've messed up the MIC machine, but you can experiment with this yourself and and get a feel for for these days, anyway, what what's going on with my data, so I changed the regularization okay.

732
02:08:06.570 --> 02:08:07.710
Capstone Coach 1: Any questions.

733
02:08:10.200 --> 02:08:11.040
Capstone Coach 1: alrighty.

734
02:08:12.150 --> 02:08:17.790
Capstone Coach 1: As usual, please feel free to ask questions I don't see the chat.

735
02:08:19.470 --> 02:08:20.010
Capstone Coach 1: Very well.

736
02:08:21.570 --> 02:08:35.940
Capstone Coach 1: And you're willing to interrupt my conversation here now let's go to the other set of code which you can download, this is the sea and encode, and so this is labeled curious digits CNN dot P y.

737
02:08:37.980 --> 02:08:49.290
Capstone Coach 1: So this is the one that's going to be new really new to us, we haven't seen this before let's open this up in in spider.

738
02:08:51.150 --> 02:08:55.770
Capstone Coach 1: And i'm going to clear off the right hand side by clear in the kernel.

739
02:08:59.820 --> 02:09:04.650
Capstone Coach 1: we're going to be using the same data but different network all together.

740
02:09:06.330 --> 02:09:07.170
Capstone Coach 1: So we'll.

741
02:09:09.960 --> 02:09:27.420
Capstone Coach 1: See make this a little bit larger to make it more visible cash should do it hope you can see that fine let's take a look at the inputs you're going to see some new ones in here and some old ones for SK learn there's no change and.

742
02:09:28.530 --> 02:09:38.730
Capstone Coach 1: The others up here, no change but i've added a few down here, look at this charisse layers co envy 2d.

743
02:09:39.270 --> 02:09:49.200
Capstone Coach 1: Max pooling today and flatten there are a few others that we could put in here, but these are the ones that are most commonly used with images.

744
02:09:50.160 --> 02:10:05.190
Capstone Coach 1: This is the convolution filter the Max pooling filter and flattened which basically takes a three dimensional nubby array and puts it into one dimension, so the calculations go faster.

745
02:10:06.390 --> 02:10:21.180
Capstone Coach 1: And, of course, to categorical we have that there are the others we've seen before, when we did the last one, there is a big function in the just like we had any other one and then so let's scroll down to.

746
02:10:24.570 --> 02:10:27.750
Capstone Coach 1: right here line number 167.

747
02:10:29.880 --> 02:10:43.020
Capstone Coach 1: Now, you should see right here line 60 so we that's familiar right we had that's how we read the data and then we get the number of classes, by using nothing function unique.

748
02:10:43.800 --> 02:11:01.980
Capstone Coach 1: And then we get the image size, which is looking at the the X ti value there so let's copy or rather select those first three lines 167 6869 right click and run.

749
02:11:03.810 --> 02:11:04.170
Capstone Coach 1: Oh.

750
02:11:05.190 --> 02:11:07.830
Capstone Coach 1: him in is T not defined.

751
02:11:09.540 --> 02:11:16.770
Capstone Coach 1: Oh, I need to run all the I did this last time Tutoring that i'm kidding see now okay so.

752
02:11:17.970 --> 02:11:22.380
Capstone Coach 1: run these please before we go too far okay.

753
02:11:23.940 --> 02:11:24.900
Capstone Coach 1: And yeah.

754
02:11:27.390 --> 02:11:33.000
Capstone Coach 1: Here we go now let's go down and take a look at our data again.

755
02:11:36.270 --> 02:11:47.010
Capstone Coach 1: we're going to run our data, right here our classes and then our image size and so let's run that there we go very quick.

756
02:11:48.030 --> 02:11:52.650
Capstone Coach 1: So what is the image size hmm let me see image size.

757
02:11:55.740 --> 02:12:10.470
Capstone Coach 1: 28 OK, so the size here is just the width of the photograph or the height fortunately here the width and the height of the same, otherwise you would be carrying around two numbers, not just image size but you'd have width and height.

758
02:12:12.780 --> 02:12:22.620
Capstone Coach 1: And then we go and we're going through the reshaped thing again because, if I look at X ti dot shape sure enough it's.

759
02:12:25.860 --> 02:12:30.900
Capstone Coach 1: And when I reshape it which we're going to do right now.

760
02:12:33.330 --> 02:12:40.470
Capstone Coach 1: copy the line 70 and 71 and just run those and now I go X T whoops.

761
02:12:42.930 --> 02:12:43.470
Capstone Coach 1: shape.

762
02:12:48.390 --> 02:12:53.850
Capstone Coach 1: You see it's four dimensions Oh, what happened there wow what of it.

763
02:12:54.990 --> 02:13:02.760
Capstone Coach 1: So what was wrong with 6028 and 28 now we got 60,028 28 and one.

764
02:13:04.710 --> 02:13:10.350
Capstone Coach 1: that's because the compilation filters are designed to work with both black and white and color.

765
02:13:11.340 --> 02:13:23.700
Capstone Coach 1: And normally if you're if you're working with color this last digit would be three three channels three layer each color photograph is really three pictures three layers of photos.

766
02:13:24.180 --> 02:13:36.240
Capstone Coach 1: Green red blue and by with black and white it's so one thing if this is a three things slow down, basically, but the the code is the same, by the way.

767
02:13:36.900 --> 02:13:46.680
Capstone Coach 1: So if these somehow we got cut off photographs in here, so the black and white, this would have come out three and the code was still be the same, the reshape and everything.

768
02:13:48.060 --> 02:13:54.510
Capstone Coach 1: So, then you you go and look at what's going on here and 172 and 173.

769
02:13:56.430 --> 02:14:01.290
Capstone Coach 1: i'm basically dividing every number in the photograph by 255.

770
02:14:02.430 --> 02:14:05.970
Capstone Coach 1: So that normalizes all of those numbers it changes them.

771
02:14:07.110 --> 02:14:26.730
Capstone Coach 1: Instead of being eight bit editors it's now flooding point because it's so number between zero and one, so we really scaled the pixel information, instead of and independent digits I mean numeric integers we have a floating Point number why.

772
02:14:27.840 --> 02:14:41.760
Capstone Coach 1: Well, because the neural network likes it to have scaled numbers like this floating point numbers because everything that's done in the tensorflow calculation is floating point arithmetic.

773
02:14:42.330 --> 02:14:59.160
Capstone Coach 1: And so, if we send them a bunch of integers as the data, then tensorflow is going to have to convert it to floating point every time it gets oh more managers are floating point, and so there it slows things down this is going to speed things up.

774
02:15:00.180 --> 02:15:09.900
Capstone Coach 1: With tensorflow by converting them the 255 that's helpful but what's really helpful is just convert them all the floating point.

775
02:15:11.160 --> 02:15:15.960
Capstone Coach 1: And then, this is courses generating the next two lines generate the.

776
02:15:17.010 --> 02:15:25.470
Capstone Coach 1: One hot encoding so i'm going to select those and run those as well that was quick and now we get into running our.

777
02:15:26.100 --> 02:15:48.120
Capstone Coach 1: Our network let's take a look at what we have, first of all it's going to look familiar sort of check this out all right there is this setting the random number seat oh yeah I remember that model equal the model of start sequential mm hmm oh what's this what happened, the layers.

778
02:15:49.170 --> 02:16:05.430
Capstone Coach 1: Okay well we're going to add a convolution to the layer it says, and then we're going to add a Max pooling to D layer and then another convolution layer, my goodness, and then we're going to let add layer called flatten.

779
02:16:06.480 --> 02:16:09.240
Capstone Coach 1: And then we're going to do some regularization drop out.

780
02:16:10.260 --> 02:16:16.710
Capstone Coach 1: Okay, and then here's the output layer now the output layer it looks like what we did before right.

781
02:16:17.310 --> 02:16:31.860
Capstone Coach 1: It says oh I got 10% runs in here the regularization is none yeah don't regularize the output and activation soft Max Of course this is nominal target, we have to do soft box.

782
02:16:32.280 --> 02:16:42.270
Capstone Coach 1: And this is called, so this is exactly the same thing that we had in the previous network, everything else is different, everything is different.

783
02:16:42.720 --> 02:16:57.390
Capstone Coach 1: So I wonder how this what's happening here well the conclusion to to the filter is a mathematical procedure or for smoothing an image and getting rid of the edge.

784
02:16:58.290 --> 02:17:11.310
Capstone Coach 1: And how much of the edge you get rid of, and how much smoothing this done depends upon the, the number of filters this guy right here, Colonel size.

785
02:17:11.730 --> 02:17:23.670
Capstone Coach 1: Colonel size, so what is Colonel SUP it's three see up here, it says, I set the size, two, three, I could have said it, the four or five or even heart higher.

786
02:17:24.570 --> 02:17:33.420
Capstone Coach 1: But what this defines is a matrix in this case, it would be a three by three matrix it's like a.

787
02:17:33.930 --> 02:17:43.770
Capstone Coach 1: You can think of it like a almost like a magnifying glass you put this down over the photograph and it picks up nine pixels three by three pixels right.

788
02:17:44.400 --> 02:17:51.330
Capstone Coach 1: And then it's going to smooth out the numbers that are in those pixels to get rid of the noise it's in the photograph.

789
02:17:52.020 --> 02:17:59.940
Capstone Coach 1: And it but it'll it won't process the edge so we're going to we're going to lose two pixels all the way around the edge.

790
02:18:00.780 --> 02:18:13.500
Capstone Coach 1: So if the image comes in, as 28 by 28th it's going to leave as 26 by 26 so you've done some small reduction there.

791
02:18:13.980 --> 02:18:30.420
Capstone Coach 1: But, more importantly, the new version of that photograph is going to be smooth it's going to be different and you'll see if you were to look at this, which we can then you would see that the their lines that are in the photograph or more distinct or more sharp.

792
02:18:31.590 --> 02:18:43.020
Capstone Coach 1: And then we go to Max pooling Oh, this is the one I really like because notice this two dimensional and there's something there called pooled size how big is this.

793
02:18:43.530 --> 02:18:56.010
Capstone Coach 1: Well, the pool sizes, too, so this is another filter it's a two by two filter and what it does, is it looks at the four filters and the four pixels that are defined by that two by two array.

794
02:18:57.090 --> 02:19:18.870
Capstone Coach 1: And it uses the Max value and that becomes the new pixel and drops the other three so this guy is going to reduce the number of pixels in the photograph we're working on by two so we're going to go from 28 to 26 with the convolution and then the pooling thing we're going to go from.

795
02:19:20.280 --> 02:19:20.880
Capstone Coach 1: To.

796
02:19:22.980 --> 02:19:25.260
Capstone Coach 1: chopping out and half when we do the pooling.

797
02:19:26.730 --> 02:19:36.420
Capstone Coach 1: And then we're going to redo the convolution on this new photograph that we just created and when we get through it, that we're going to flatten it back down to an empty array.

798
02:19:37.560 --> 02:19:46.200
Capstone Coach 1: and send it send it into these 10 10% prawns and the output side and it's going to give us our information so.

799
02:19:47.550 --> 02:19:48.990
Capstone Coach 1: So let's do this.

800
02:19:50.010 --> 02:19:59.400
Capstone Coach 1: we're going to run this see let's say we have 10 in a box and 128 size that's Okay, this is slower, by the way.

801
02:20:00.210 --> 02:20:13.080
Capstone Coach 1: This business of convolution takes a little more time than just ordinary the ordinary because it's extra gotta go and check multiple pixels and do calculations and get you know get things in order.

802
02:20:15.390 --> 02:20:24.030
Capstone Coach 1: So let's take everything from the runtime up to right here it's a start time we're going to take all of that.

803
02:20:25.380 --> 02:20:31.890
Capstone Coach 1: And I think we've run everything above that, so we should be good here let's just right click run.

804
02:20:33.120 --> 02:20:36.780
Capstone Coach 1: And there it goes okay X train is not defined.

805
02:20:39.060 --> 02:20:44.490
Capstone Coach 1: I guess we didn't run I didn't run that part up there, so anyway.

806
02:20:45.510 --> 02:20:54.660
Capstone Coach 1: let's Let me run that part of their first the sky, right here just to make sure right click run phone okay now.

807
02:20:56.430 --> 02:20:57.360
Capstone Coach 1: All of this.

808
02:20:58.410 --> 02:21:00.750
Capstone Coach 1: Right click and run.

809
02:21:03.990 --> 02:21:13.440
Capstone Coach 1: Okay, now I guess all right, first of all take a look at the table, what we see here it's more complex than what we're doing with the nlp network.

810
02:21:14.370 --> 02:21:26.460
Capstone Coach 1: We have one convolution layer and what's coming into the completion layer 20 oh no i'm sorry what's coming out of the layer is 26 by 26 remember I said you're going to drop.

811
02:21:27.420 --> 02:21:38.040
Capstone Coach 1: The images or 28 by 28 and it's going to drop two pixels from that image, these are the your edge the edge, because this three by three.

812
02:21:38.760 --> 02:21:57.090
Capstone Coach 1: filter that's being used it can't be placed over the edge the top two pistols because part of the filter be laying will be off the photograph so it starts with the you know the the second row down and says set of the first room.

813
02:21:59.220 --> 02:22:01.470
Capstone Coach 1: Actually zero in the first.

814
02:22:01.770 --> 02:22:03.090
started throwing number two.

815
02:22:04.170 --> 02:22:04.560
Capstone Coach 1: Then.

816
02:22:05.940 --> 02:22:13.500
Capstone Coach 1: So this says this requires 640 parameters or weights these weights or the weights that are being used by the.

817
02:22:14.790 --> 02:22:27.870
Capstone Coach 1: filter itself the convolution filter and so these are being estimated to optimize the the identification, in this case the nominal identification, then.

818
02:22:29.130 --> 02:22:41.640
Capstone Coach 1: It goes to Max pooling and the photograph know drops down to 13 by 13 so we're going from 2828 to 2626 to 13 by 13.

819
02:22:43.470 --> 02:23:02.730
Capstone Coach 1: And then these 13 this this little picture is only 13 by 13 right, so it goes into the next convolution and we're going to lose some more pixels around that so we lose two pixels around 13 we end up with a little picture 11 by 11.

820
02:23:04.140 --> 02:23:20.820
Capstone Coach 1: And then we flat my thing out Oh, and we flatten it out, we have 707,744, and that is then becomes our tense or, if you want and.

821
02:23:21.330 --> 02:23:37.500
Capstone Coach 1: We drop some of those weights here drop out and then that go they all go into the 10 outputs you notice that the number of weights in the outputs is 77450.

822
02:23:38.040 --> 02:23:58.980
Capstone Coach 1: So that's basically 10 times 774 plus another 10 for the bias terms that go in these in these weights here now, this is kind of typical of what you'd see in the photograph you have a lot of pre processing that's going on using convolution.

823
02:24:00.150 --> 02:24:10.830
Capstone Coach 1: And also pooling and then finally flatten and then into the last now oftentimes after this flattened part right here.

824
02:24:11.340 --> 02:24:21.180
Capstone Coach 1: you'll have another hidden layer multiple hidden layers in this area that help you then improve the forecasts that you're going to get here.

825
02:24:21.810 --> 02:24:31.320
Capstone Coach 1: So it's going to be interesting to see, we have no hidden layers we're basically taking the photographs as input we are filtering them through convolution.

826
02:24:32.220 --> 02:24:40.140
Capstone Coach 1: And then the smaller version of those photographs, which is on which are only 11 by 11 by the time they get down through here.

827
02:24:40.620 --> 02:24:57.450
Capstone Coach 1: That goes into the output layer precept ron's and their weights are us, then, to make the forecast of which digit this belongs to, here we are we've got we're epoch number seven.

828
02:24:58.740 --> 02:25:15.480
Capstone Coach 1: And that's why I made sure that we didn't have it set to something like 100 which is okay if you're going to get coffee or something like that, but you can see each epoch here is taking about 3637 seconds.

829
02:25:16.590 --> 02:25:28.770
Capstone Coach 1: So 1010 a box is going to be about 380 400 seconds divided by 60 that's a few minutes.

830
02:25:32.190 --> 02:25:54.090
Capstone Coach 1: Alright how's it doing well let's see the losses point of to good good oh look at this the validation loss and the target loss or about the same that's good and then the accuracy on the training data and the accuracy on the validation data are very close.

831
02:25:55.350 --> 02:26:00.840
Capstone Coach 1: So this is from that respect it looks better than the nlp solution.

832
02:26:03.120 --> 02:26:21.060
Capstone Coach 1: will see in the final numbers in just a minute how this works out now if we were to run more epochs and I probably would have probably jack it up at least a 20 in the final solution, then you would see maybe these numbers, which would be better or worse, but most likely they'll get better.

833
02:26:33.750 --> 02:26:37.890
Capstone Coach 1: Now I was, I was thinking about signing some.

834
02:26:39.120 --> 02:26:50.190
Capstone Coach 1: Some homework for this situation, and what I was going to just have you do is run some scenarios and report your what you ran from this from running this code is.

835
02:26:50.910 --> 02:27:03.900
Capstone Coach 1: If you change the if you added regularization, for example, does the fifth improve the misclassification rate, this improve if you right here we're using.

836
02:27:04.740 --> 02:27:20.640
Capstone Coach 1: Two levels of convolution if I were to drop out that second level and just use a single level of convolution How does that affect the the the fit to get better get worse that kind of thing.

837
02:27:23.250 --> 02:27:28.110
Capstone Coach 1: So I wanted I wanted somebody's opinion about that about whether we should have some.

838
02:27:29.250 --> 02:27:35.700
Capstone Coach 1: Some homework along those lines, we can rack up a few more points or not.

839
02:27:36.990 --> 02:27:37.440
Capstone Coach 1: Okay.

840
02:27:38.640 --> 02:27:48.510
Capstone Coach 1: Alright, so I have a problem in the code charisse accuracy Oh yes, I forgot to run that part of the code, yes, of course.

841
02:27:49.770 --> 02:27:50.430
Capstone Coach 1: So.

842
02:27:51.450 --> 02:27:54.960
Capstone Coach 1: What we'll do here what I can do is go to the top, where it says.

843
02:27:56.550 --> 02:28:02.430
Capstone Coach 1: function Kara cx accuracy plots let's basically whoops.

844
02:28:04.920 --> 02:28:09.060
Capstone Coach 1: copy not copy but select all of that and run it.

845
02:28:10.950 --> 02:28:11.490
Capstone Coach 1: There we go.

846
02:28:13.200 --> 02:28:25.920
Capstone Coach 1: That run, nothing will happen, but you're basically running that function to defining what the function is now let's go back down here, where it was calling that function so.

847
02:28:26.970 --> 02:28:37.290
Capstone Coach 1: That would be right here, it says charisse accuracy plots this should work now so i'm going to copy that and right click and run that.

848
02:28:38.370 --> 02:28:44.820
Capstone Coach 1: And there we go, so this shows this what how successful or unsuccessful, we are.

849
02:28:45.870 --> 02:28:59.040
Capstone Coach 1: Very different picture than when we were getting within LP notice that the validation accuracy is better than the training, so no i'm sorry yeah it is no no.

850
02:29:00.240 --> 02:29:11.310
Capstone Coach 1: No it's 95 out of 10,000 is point nine five is the misclassification rate and here is point 367 so the training is still better it's still a better fit up here.

851
02:29:11.670 --> 02:29:21.180
Capstone Coach 1: But there, these are not as different from one another as they were with em LP now the training accuracy of there was zero.

852
02:29:21.600 --> 02:29:33.240
Capstone Coach 1: And, and here it was it was something worse than this, it was like 145 or 150 over 10,000 like 1.5 or 1.4%.

853
02:29:33.990 --> 02:29:45.270
Capstone Coach 1: misclassification here we've got it below 1% so I actually feel like this is a better fit because the last the decrease in terms of the number of.

854
02:29:45.990 --> 02:30:06.360
Capstone Coach 1: Of overfit them, you know overfitting the difference between these two as less and also the validation numbers come down a little bit, so I think that's a good thing, but we might be able to do better by changing the design of the network, the one thing.

855
02:30:08.190 --> 02:30:22.980
Capstone Coach 1: And that would mainly be a question of whether another layer of convolution would help or whether with we removed one layer of convolution and maybe replaced it with a hidden layer for subtracts maybe that would even do better.

856
02:30:24.240 --> 02:30:43.530
Capstone Coach 1: So there are many, many options here that you could explore to try to improve this fit the filter size, the kid the Colonel sighs I probably would leave that alone, because these are small images 28 by 28 and when you took off two rows of pixels that's the edge.

857
02:30:44.610 --> 02:30:54.960
Capstone Coach 1: Well that's different than if we have that 200 by 200 photograph your with 200 by 200 you don't miss those pixels for 2028 well.

858
02:30:56.070 --> 02:30:56.640
Capstone Coach 1: I don't know.

859
02:30:58.530 --> 02:30:58.980
Capstone Coach 1: So.

860
02:31:00.270 --> 02:31:01.110
Capstone Coach 1: Okay, so.

861
02:31:01.230 --> 02:31:03.030
Fireman, Karen: question the question, yes Karen.

862
02:31:03.120 --> 02:31:06.270
Fireman, Karen: i'm in your first couple additional layer on.

863
02:31:09.000 --> 02:31:17.970
Fireman, Karen: You have the variable input shape it wasn't good shape, whereas on the second later layer you don't mention that 193.

864
02:31:18.030 --> 02:31:28.560
Capstone Coach 1: Right, you know that's that's a good observation, you see, on this one right here you have input shape is equal to input shape, which is things 28.

865
02:31:30.810 --> 02:31:39.360
Capstone Coach 1: Well, you know i'm sorry input shape is 28 is a tubal three numbers 2828 one.

866
02:31:40.020 --> 02:31:48.600
Capstone Coach 1: So for the photograph that's what they mean by shape, I mean to go with the height the width the hype, and the number of channels that last number one.

867
02:31:49.290 --> 02:31:58.050
Capstone Coach 1: So yes, it says what is the input shape here, you always it's good practice to do that on the first layer

868
02:31:59.040 --> 02:32:05.490
Capstone Coach 1: Is this is actually a neural network layer the ceiling V to be here it's actually a series of.

869
02:32:06.300 --> 02:32:13.320
Capstone Coach 1: Book it's considered a layer it's treated as a layer, and so they want you to tell what to say what the shape of the incoming data is.

870
02:32:13.680 --> 02:32:25.260
Capstone Coach 1: Just as a double check to make sure that there's no mismatch for the other layers that you have following they know what the shape is because they know the what the preceding later look like.

871
02:32:26.340 --> 02:32:42.120
Capstone Coach 1: i'm so it's only it's only necessary to have the input size on that, on the first layer if you looked at the the previous code you'll see here, it says input dimensions equals in features.

872
02:32:43.830 --> 02:32:46.980
Capstone Coach 1: That only is only necessary.

873
02:32:48.150 --> 02:32:54.870
Capstone Coach 1: On the first layer if it's put on the second layer as it is here that's a boo boo not needed.

874
02:32:56.670 --> 02:33:03.210
Capstone Coach 1: Take it out just realize it was not a good at the, I guess, they just ignored it but.

875
02:33:04.350 --> 02:33:07.410
Capstone Coach 1: You know practice you supposed to just put that on the first video.

876
02:33:09.780 --> 02:33:11.220
Capstone Coach 1: Okay, good question.

877
02:33:12.990 --> 02:33:15.450
Capstone Coach 1: let's go back here so that's.

878
02:33:16.680 --> 02:33:23.070
Capstone Coach 1: I have two versions, I have you can see one here, it says model, one and one here model tune.

879
02:33:24.120 --> 02:33:32.820
Capstone Coach 1: What I normally do is I don't touch model to this is sort of like a backup in case something's not working, I can reef.

880
02:33:33.090 --> 02:33:35.670
Capstone Coach 1: restored whatever I did up here on the top.

881
02:33:36.330 --> 02:33:36.780
Right.

882
02:33:38.070 --> 02:33:51.300
Capstone Coach 1: But what i'd like you to do now is to make some changes here and let's run it together and that'll, be it for tonight, but what i'd like you to do is let's take out the second convolution.

883
02:33:51.990 --> 02:34:03.930
Capstone Coach 1: and see what happens so that would be, you can see, the first convolution up here, and the second one So what did it take second one, I think this guy out right here.

884
02:34:06.930 --> 02:34:08.670
Capstone Coach 1: Well what's going to take that.

885
02:34:09.720 --> 02:34:11.280
Capstone Coach 1: would happen, you know that guy right.

886
02:34:12.810 --> 02:34:18.870
Capstone Coach 1: And let's see do we need to do anything else well huh drop out I don't want.

887
02:34:20.220 --> 02:34:21.330
Capstone Coach 1: To get rid of the drop out.

888
02:34:22.890 --> 02:34:29.340
Capstone Coach 1: like that and flatten hmm I don't know that we need flatten.

889
02:34:30.990 --> 02:34:31.440
Capstone Coach 1: This is.

890
02:34:32.580 --> 02:34:36.090
Capstone Coach 1: let's let's see what happens if we comment.

891
02:34:38.700 --> 02:34:43.020
Capstone Coach 1: because all that seems to be doing is changing the shape of the non P array.

892
02:34:44.280 --> 02:34:50.130
Capstone Coach 1: And I wonder if the last hidden layer, the last layer the upper layers going to.

893
02:34:51.570 --> 02:35:02.580
Capstone Coach 1: accommodate that or not see what happens so i'm going to go up here and restart the Colonel make sure this is a fair test restart yes.

894
02:35:05.520 --> 02:35:06.150
Capstone Coach 1: and

895
02:35:08.220 --> 02:35:13.710
Capstone Coach 1: Then what i'm going to do is i'm going to go up here, make sure it says true for model, one.

896
02:35:14.970 --> 02:35:17.400
Capstone Coach 1: Make sure it says false for model to.

897
02:35:19.140 --> 02:35:21.210
Capstone Coach 1: don't want to run that one not tonight.

898
02:35:22.350 --> 02:35:26.460
Capstone Coach 1: And everything else well we'll just leave it in, so it is so we're good.

899
02:35:27.510 --> 02:35:35.430
Capstone Coach 1: let's go ahead and click run and just start from the top it'll read the data, you know set up the columns and.

900
02:35:36.630 --> 02:35:38.640
Capstone Coach 1: All that sort of business, but what.

901
02:35:39.690 --> 02:35:40.350
Capstone Coach 1: Wait a minute.

902
02:35:46.200 --> 02:35:48.000
Capstone Coach 1: This I haven't seen before I.

903
02:35:48.540 --> 02:35:49.890
Fireman, Karen: Think it's incompatible.

904
02:35:50.790 --> 02:35:51.360
Capstone Coach 1: what's that.

905
02:35:51.960 --> 02:35:55.170
Fireman, Karen: line is said value error shapes are incompatible.

906
02:35:55.710 --> 02:36:04.230
Capstone Coach 1: yeah let's see what says let's see what Oh, I see what's happening, you know that flattened we took out it's not too happy about that.

907
02:36:06.030 --> 02:36:09.030
Capstone Coach 1: Okay, it was a test it was a test.

908
02:36:10.770 --> 02:36:22.050
Capstone Coach 1: And so let's put it back in maybe maybe life will get better and so we'll take the console here restart the Colonel just pretend that you didn't see that.

909
02:36:24.030 --> 02:36:25.560
Capstone Coach 1: yeah restart the car okay.

910
02:36:26.580 --> 02:36:35.880
Capstone Coach 1: Just pretend we didn't do that all right hey it was a mistake, I hope that was not the only mistake we met all right now click OK run.

911
02:36:36.420 --> 02:36:39.270
Jim Clark: So that's the guns with what lies that you take out.

912
02:36:40.350 --> 02:36:47.790
Capstone Coach 1: Well, I had I had commented out this line number 193, which is the flat line.

913
02:36:48.210 --> 02:36:52.590
Jim Clark: Yes, so is that really changed at all from where we started.

914
02:36:53.520 --> 02:37:13.380
Capstone Coach 1: The problem, the problem, apparently, is that if you don't flatten all of the photographs coming out of the convolution and pulling process, then the network, then neural network layer, which is, in this case it's the last output layer doesn't know what to do with the numbers.

915
02:37:15.450 --> 02:37:21.330
Capstone Coach 1: So if that's why we got these weird messages about this is not compatible with that different sizes.

916
02:37:23.040 --> 02:37:33.480
Capstone Coach 1: Because we are required if we're going from convolution to regular network analysis, I guess, we have to flat, and we must flat.

917
02:37:34.500 --> 02:37:35.850
Capstone Coach 1: To make that transition.

918
02:37:37.170 --> 02:37:38.280
Capstone Coach 1: Get the data and order.

919
02:37:39.450 --> 02:37:48.690
Capstone Coach 1: Now what that what you do see here is let's I don't know if this is a lot more of weights or not hmm.

920
02:37:50.190 --> 02:37:55.260
Capstone Coach 1: 108,000 that's not bad, but in terms of the total number of weights.

921
02:37:56.610 --> 02:38:00.690
Capstone Coach 1: In the other network nlp we were working with a million.

922
02:38:01.770 --> 02:38:05.490
Capstone Coach 1: So wow okay well let's see what happens.

923
02:38:06.660 --> 02:38:28.140
Capstone Coach 1: So far, what I like is the accuracy for the training data and the validation data are almost the same and the last is very similar as well, so of course that was slowly, we saw in the previous run things look pretty good in that respect as well, so let's see what happens.

924
02:38:29.220 --> 02:38:35.220
Capstone Coach 1: it's running faster yeah yeah it's running faster, because we have fewer weights fewer layers.

925
02:38:36.960 --> 02:38:44.190
Fireman, Karen: You know the first ad hoc is worse on the training than it is in the validation and it's true for mine as well.

926
02:38:46.050 --> 02:38:46.320
Fireman, Karen: Just the.

927
02:38:46.350 --> 02:38:46.800
Capstone Coach 1: word about.

928
02:38:47.070 --> 02:38:49.350
Fireman, Karen: What about the very first epoch yeah.

929
02:38:49.440 --> 02:38:55.650
Fireman, Karen: Your accuracy point 91 versus your nation accuracy is 96 and I had the same thing.

930
02:38:56.220 --> 02:38:57.600
Capstone Coach 1: That has happened, you know.

931
02:38:57.690 --> 02:38:58.620
Capstone Coach 1: This is just.

932
02:38:59.220 --> 02:39:01.020
Capstone Coach 1: draw the luck, of the draw.

933
02:39:02.790 --> 02:39:06.510
Capstone Coach 1: There you do see some weird things going on, oh look at this one that's in this is kind of.

934
02:39:08.040 --> 02:39:09.030
Capstone Coach 1: yeah well, no.

935
02:39:09.210 --> 02:39:12.720
Swanson J, Charles: I figured out dropping dropping accuracy here.

936
02:39:13.950 --> 02:39:16.860
Capstone Coach 1: We did you mean with this this new version.

937
02:39:17.340 --> 02:39:18.600
Capstone Coach 1: Correct yeah.

938
02:39:18.960 --> 02:39:26.940
Capstone Coach 1: So our little experiment here wasn't too successful it ran faster, but the results, not so great.

939
02:39:29.130 --> 02:39:33.000
Capstone Coach 1: You know so let's let's let's put that layer back.

940
02:39:36.180 --> 02:39:37.890
Capstone Coach 1: Okay let's see.

941
02:39:39.240 --> 02:39:42.300
Capstone Coach 1: Well, if I go down below.

942
02:39:43.470 --> 02:39:45.390
Capstone Coach 1: And that's in the second model.

943
02:39:46.710 --> 02:40:01.560
Capstone Coach 1: You should see there, it is the whole thing, so I wanted, I want to do this part right here, and I wanted to well you know what let's just copy the whole thing.

944
02:40:02.820 --> 02:40:03.420
Capstone Coach 1: like that.

945
02:40:04.530 --> 02:40:07.080
Capstone Coach 1: The recent reboot restart to where we were.

946
02:40:10.860 --> 02:40:14.670
Capstone Coach 1: hilarious so yeah What about.

947
02:40:15.960 --> 02:40:16.200
Capstone Coach 1: well.

948
02:40:17.280 --> 02:40:18.630
Swanson J, Charles: I think you went too far.

949
02:40:18.960 --> 02:40:19.470
Oh.

950
02:40:21.450 --> 02:40:22.410
Capstone Coach 1: I went too far.

951
02:40:22.560 --> 02:40:31.710
Capstone Coach 1: yeah yeah yeah I see that oh Oh, thank you very much, just seeing that Oh, my goodness, oh wow.

952
02:40:32.880 --> 02:40:36.900
Capstone Coach 1: So that was a real that could have been a real disaster.

953
02:40:38.040 --> 02:40:43.410
Capstone Coach 1: Alright, so I think this is okay, now we got 123 combinations.

954
02:40:44.940 --> 02:40:58.710
Capstone Coach 1: The first one is followed by bullying the second one by pooling and the last one by flatten and we're words got the dropouts back, and we have the the the output layer there so, then the.

955
02:40:59.070 --> 02:41:03.360
Fireman, Karen: Second, originally didn't have three convolutions I think he just had to.

956
02:41:05.130 --> 02:41:10.560
Fireman, Karen: Do you got an extra convolution now get the third convolution on line 195 that you didn't.

957
02:41:11.160 --> 02:41:11.760
Capstone Coach 1: Oh, really.

958
02:41:12.300 --> 02:41:12.720
Fireman, Karen: yeah.

959
02:41:13.020 --> 02:41:15.000
Capstone Coach 1: Well that's what I wanted to do anyway, I wanted.

960
02:41:15.150 --> 02:41:16.080
Fireman, Karen: To add a third one.

961
02:41:17.550 --> 02:41:19.530
Capstone Coach 1: Oh yeah exactly.

962
02:41:20.910 --> 02:41:22.380
Fireman, Karen: So you put that after flat.

963
02:41:22.890 --> 02:41:25.830
Capstone Coach 1: No, no, before you see it says.

964
02:41:26.100 --> 02:41:29.520
Fireman, Karen: I don't know what exactly your Max pool okay yeah so you got.

965
02:41:29.700 --> 02:41:37.110
Capstone Coach 1: convert gone to the Max pool God TV Mexico can to be flapping.

966
02:41:38.670 --> 02:41:43.050
Capstone Coach 1: So yeah we have three three convolutions now.

967
02:41:45.390 --> 02:41:48.150
Capstone Coach 1: And to Max pools, and one final.

968
02:41:51.210 --> 02:41:56.250
Capstone Coach 1: thing oh everything's there's good so.

969
02:41:57.270 --> 02:41:59.520
Fireman, Karen: We still do drop out Johnson okay.

970
02:42:00.390 --> 02:42:08.790
Capstone Coach 1: Well, why not well I don't like trump I don't like regularization here let's let's let's let's take it out.

971
02:42:11.280 --> 02:42:12.060
Capstone Coach 1: For this from.

972
02:42:13.800 --> 02:42:21.420
Capstone Coach 1: This commented uh yeah you see the dropout is 20 the dropout rate is 20% here.

973
02:42:23.820 --> 02:42:29.400
Capstone Coach 1: Personally I think these particular photographs the regularization is not working very well.

974
02:42:30.270 --> 02:42:34.140
Capstone Coach 1: But heck we'll try it all right let's run it and see.

975
02:42:35.190 --> 02:42:41.640
Capstone Coach 1: see if we can get this to finish before it gets hit the stream that the pumpkin turns so.

976
02:42:41.760 --> 02:42:44.130
Swanson J, Charles: Far we turn into pumpkins yeah.

977
02:42:44.490 --> 02:42:44.760
Like.

978
02:42:47.280 --> 02:42:51.330
Capstone Coach 1: Okay, the total number of parameters is less 80,000.

979
02:42:53.100 --> 02:43:09.180
Capstone Coach 1: yeah 80,000 verses was 103 in the last go hmm in that you know I always I always think that the more parameters, you have the better the fit should be but it looks like, in this case we're seeing some of the just maybe just the opposite.

980
02:43:10.380 --> 02:43:17.850
Capstone Coach 1: But I think there was going on here is that Max pooling and the second, looking at the now we're looking at a three by three photograph.

981
02:43:19.620 --> 02:43:35.850
Capstone Coach 1: And look at when we flattened we got 576 pixels So you can see what happened we go from 26 to 1311 to five to three, that is, this three by three photograph going to have sufficient information on it to.

982
02:43:37.320 --> 02:43:52.800
Capstone Coach 1: To allow this distinguish between digits I think it could, because if you look at these numbers, there are characteristics reach digit which makes them quite different from one another right like the number one one vertical line.

983
02:43:54.150 --> 02:43:59.220
Capstone Coach 1: 02 vertical line so it's going to look like two vertical lines right.

984
02:44:02.790 --> 02:44:12.030
Capstone Coach 1: And eight distinguish them in an eight from the zero that might be a problem or six and eight from a six and that could be a problem yeah.

985
02:44:13.290 --> 02:44:24.060
Capstone Coach 1: So here we're going going down the road and let's see what we have a level five and all 398 and 98 OK OK now.

986
02:44:25.620 --> 02:44:25.980
Capstone Coach 1: It says.

987
02:44:30.180 --> 02:44:36.750
Capstone Coach 1: what's what's sort of interesting here is, you have fewer parameters, but maybe it's running slower, or is it just me.

988
02:44:37.830 --> 02:44:38.400
Capstone Coach 1: I don't know.

989
02:44:40.950 --> 02:44:41.640
Capstone Coach 1: So.

990
02:44:41.820 --> 02:44:43.890
Swanson J, Charles: Now it seems to be running slower.

991
02:44:44.100 --> 02:44:50.070
Capstone Coach 1: It does on my machine and it says 35 seconds 30, so I think it's line.

992
02:44:51.240 --> 02:44:53.460
Capstone Coach 1: I don't think that's right because.

993
02:44:55.860 --> 02:44:59.910
Capstone Coach 1: Maybe it's because the clock is pointing upwards at a.

994
02:45:03.240 --> 02:45:03.810
Capstone Coach 1: well.

995
02:45:05.010 --> 02:45:14.970
Capstone Coach 1: So I when I was in Austin over the weekend, it was just the culture they're so different right, so you have you're driving around, and you have all these tents underneath the freeway.

996
02:45:16.560 --> 02:45:22.530
Capstone Coach 1: They got a whole herd, and also have people living in tense now on city streets.

997
02:45:23.610 --> 02:45:27.120
Capstone Coach 1: And I hadn't seen that before and other turnips awesome.

998
02:45:28.800 --> 02:45:30.030
Capstone Coach 1: And then you.

999
02:45:31.020 --> 02:45:39.810
Jim Clark: know not to not good cry that often but there's a lot of people living against below the underpasses in relation to.

1000
02:45:40.260 --> 02:45:43.350
Capstone Coach 1: Really, I say I haven't been in Houston for a while.

1001
02:45:44.460 --> 02:45:45.960
Capstone Coach 1: So it's the same thing there too.

1002
02:45:47.490 --> 02:45:49.380
Jim Clark: Yes, actually around background buying.

1003
02:45:49.950 --> 02:45:56.790
Capstone Coach 1: All that I could believe if I was if I was homeless I go to the by you hanging out down there that's Nice.

1004
02:45:58.440 --> 02:45:59.370
Capstone Coach 1: People drop food.

1005
02:46:00.960 --> 02:46:07.050
Capstone Coach 1: I don't know I actually had a friend who was the guy he was almost killed in the bike riding the bicycle.

1006
02:46:08.520 --> 02:46:15.240
Capstone Coach 1: At night, it was near dusk it wasn't really night yep and he was he was a very good bicyclist.

1007
02:46:16.830 --> 02:46:23.610
Capstone Coach 1: He was must have been doing 20 at least 20 miles an hour and somebody threw a water balloon at him.

1008
02:46:25.320 --> 02:46:32.010
Capstone Coach 1: Well, it turns out that a water balloon if you hit an option let's move in 20 miles an hour, the water balloon it's like a brick.

1009
02:46:33.030 --> 02:46:37.440
Capstone Coach 1: So he almost lost his eyes ended up in the hospital for several weeks.

1010
02:46:40.050 --> 02:46:43.320
Capstone Coach 1: didn't think about that about the they actually dropped a.

1011
02:46:43.470 --> 02:46:45.960
Capstone Coach 1: balloon from top of the bridge somewhere something like.

1012
02:46:47.790 --> 02:46:48.870
Capstone Coach 1: That it was harmless.

1013
02:46:51.750 --> 02:46:53.970
Capstone Coach 1: So, here it goes heat TIM.

1014
02:46:57.540 --> 02:47:06.240
Jim Clark: My machine here running about 17 seconds where we're every step or two but but twice the speed twice the time.

1015
02:47:07.440 --> 02:47:10.710
Capstone Coach 1: Really, how it what machine, are you using.

1016
02:47:11.550 --> 02:47:14.490
Jim Clark: The computer laptop here, I see.

1017
02:47:14.520 --> 02:47:16.950
Capstone Coach 1: yeah Well, this is, this is my.

1018
02:47:19.110 --> 02:47:29.190
Capstone Coach 1: MAC pro which has been a fine machine for me, usually I have to get rid of these scenes after about four years this guy's been around for a while okay.

1019
02:47:30.450 --> 02:47:31.560
Capstone Coach 1: epoch number nine.

1020
02:47:32.700 --> 02:47:34.260
Capstone Coach 1: Of course, your numbers may be different.

1021
02:47:35.430 --> 02:47:37.020
Capstone Coach 1: A lot different.

1022
02:47:37.680 --> 02:47:43.980
Fireman, Karen: It was much slower five minutes versus 3.7.

1023
02:47:47.310 --> 02:47:48.390
Capstone Coach 1: we'll see here.

1024
02:47:50.070 --> 02:47:50.910
Fireman, Karen: 50% more.

1025
02:47:59.970 --> 02:48:05.370
Capstone Coach 1: So what do you think about should we have a homework problem this week and basically to do more, what we're doing right now.

1026
02:48:09.420 --> 02:48:10.890
Capstone Coach 1: Or whether we're calling it.

1027
02:48:11.040 --> 02:48:19.140
Coleman, Clay: optional extra credit yeah some there's worse about halfway through I mean I stopped to present and I know.

1028
02:48:19.260 --> 02:48:20.130
Capstone Coach 1: I know I know.

1029
02:48:20.700 --> 02:48:25.530
Capstone Coach 1: that's why i'm that's why i'm raising the question some of your done, I know.

1030
02:48:26.640 --> 02:48:31.620
Capstone Coach 1: Some of you done an outstanding, I know, but some of us to work on your problems so.

1031
02:48:34.260 --> 02:48:40.080
Capstone Coach 1: i'll make it i'll give it an optional optional optional homework thing you have time fine.

1032
02:48:42.870 --> 02:48:44.070
Swanson J, Charles: it's really Thank you.

1033
02:48:47.130 --> 02:48:48.060
Coleman, Clay: Now that appreciate it.

1034
02:48:48.750 --> 02:49:04.290
Capstone Coach 1: But in this particular case, all you're going to be doing sort of what we're doing here, change the code, a little bit and run it to you know evaluate what's what conditions So here we get whoo whoo I like.

1035
02:49:04.500 --> 02:49:05.250
Capstone Coach 1: amazing.

1036
02:49:05.970 --> 02:49:13.050
Swanson J, Charles: what's up that's amazing that this classification reduction in the validation set yeah.

1037
02:49:13.680 --> 02:49:29.310
Capstone Coach 1: yeah well in both the other, the other one is almost it's close to zero, but yeah so, and this, this would say you have less funding overfitting, I think, but I think, with some more tweaking you might get this down to a perfect fit.

1038
02:49:31.530 --> 02:49:46.500
Capstone Coach 1: With a little bit more tweaking and maybe some extra runtime might be able to get this down to 100% accuracy i've got it i've got another train, you said that data set that we're going to look at next time, which is the X Ray data which is.

1039
02:49:47.640 --> 02:49:49.650
Capstone Coach 1: Basically, looking to buy pneumonia.

1040
02:49:49.890 --> 02:49:50.820
Capstone Coach 1: Not pneumonia.

1041
02:49:51.120 --> 02:49:51.960
Swanson J, Charles: Oh that's cool.

1042
02:49:52.350 --> 02:49:54.330
Capstone Coach 1: Now, this is a huge data set.

1043
02:49:54.690 --> 02:49:56.940
Capstone Coach 1: much bigger than 28 by 28.

1044
02:49:58.020 --> 02:50:09.990
Capstone Coach 1: But the the configurations are the same thing you you basically set up the convolution and then you have a neural network underneath that that fits the information you get from the convolution.

1045
02:50:10.410 --> 02:50:21.870
Capstone Coach 1: The basic idea here is to take the original image which has a huge amount of information and boil that down into the essence of what you need to make this choice and then.

1046
02:50:22.380 --> 02:50:36.840
Capstone Coach 1: put that into a neural network, but then trains train the neural network to use that information to make the choice and it I got that one to work perfect 00 this classification.

1047
02:50:38.280 --> 02:50:47.070
Capstone Coach 1: But it was some tweaking doing on on the combinations so we'll do that and for now that's it for tonight.

1048
02:50:48.510 --> 02:51:03.180
Capstone Coach 1: Thanks for staying with me and good luck with your capstones as of you that are on the way to do that, and I think I have a few that i'm going to be visiting here this week and looking forward to seeing the progress so we'll see.

1049
02:51:04.200 --> 02:51:11.640
Capstone Coach 1: Anyway, y'all be safe, get the vaccine, if you haven't already got it and you know.

1050
02:51:13.590 --> 02:51:15.870
Capstone Coach 1: enjoy life as much as you can right.

1051
02:51:16.620 --> 02:51:24.360
Amanda Pham: Dr Jones I had a quick question um did you want us to use the CNN template or the other one that we did the nlp.

1052
02:51:24.870 --> 02:51:27.660
Capstone Coach 1: Well, I think you know what I would probably do is is.

1053
02:51:29.340 --> 02:51:33.030
Capstone Coach 1: I will, I will mess, with the CNN one, what do you think.

1054
02:51:34.650 --> 02:51:37.890
Capstone Coach 1: You think you will be helpful to play with the other template as well.

1055
02:51:38.190 --> 02:51:38.490
Swanson J, Charles: All right.

1056
02:51:40.140 --> 02:51:42.600
Zuberi, Bilal: Because we can do CNN because I think.

1057
02:51:42.630 --> 02:51:46.500
Zuberi, Bilal: The last summer we did nlp so it will be good to get different experience.

1058
02:51:47.190 --> 02:51:53.430
Swanson J, Charles: yeah and then also the CNN seems to have a lower misclassification rate.

1059
02:51:54.240 --> 02:52:04.080
Capstone Coach 1: Oh yes, yes and that's that's to be expected convolution is sort of like with you're going to do photographic recognition you're going to use combination.

1060
02:52:05.760 --> 02:52:25.440
Capstone Coach 1: it's just it's a filtering process that gets rid of all the slick glossy stuff that's in the picture and just shows you the main lines and structure that are in that, and then you use that to in a network to do your notification so yeah well, let me think about that and i'll post the.

1061
02:52:26.460 --> 02:52:31.830
Capstone Coach 1: Written the homework optional homework tomorrow i'm anjana.

1062
02:52:33.210 --> 02:52:33.720
Capstone Coach 1: Okay.

1063
02:52:34.470 --> 02:52:35.130
Swanson J, Charles: Thank you.

1064
02:52:35.190 --> 02:52:43.920
Capstone Coach 1: Thank you so much i'm almost sad that this class is coming to a close, here we have, I think, four more weeks the.

1065
02:52:44.820 --> 02:53:00.780
Capstone Coach 1: Two of US weeks will be used basically in presenting your you know your project your class projects so, by the way, this could be a class project tweaking this to get a better solution and describing the solution and I will be fine.

1066
02:53:02.070 --> 02:53:03.360
Capstone Coach 1: Class project as well.

1067
02:53:04.770 --> 02:53:08.280
Capstone Coach 1: Okay we'll see you next week, then see you on Tuesday.

1068
02:53:09.270 --> 02:53:09.510
or.

1069
02:53:11.700 --> 02:53:14.040
Capstone Coach 1: Monday tomorrow we'll have a Q amp a tomorrow night i'm sorry.

1070
02:53:14.040 --> 02:53:14.460
Fireman, Karen: Yes.

1071
02:53:14.550 --> 02:53:16.560
Capstone Coach 1: yeah I forgot q&a tomorrow night.

1072
02:53:17.700 --> 02:53:18.090
Capstone Coach 1: bye now.

1073
02:53:18.690 --> 02:53:19.350
Zuberi, Bilal: Thank you present.

