WEBVTT

1
00:00:20.160 --> 00:00:20.550
Capstone Coach 1: hmm.

2
00:00:22.590 --> 00:00:24.150
Capstone Coach 1: They are maybe.

3
00:00:26.670 --> 00:00:37.170
Capstone Coach 1: Okay where's my little thing there Oh, I know I have to do, I have to get tickled with zoom right I keep forgetting that okay.

4
00:00:38.370 --> 00:00:38.940
Capstone Coach 1: and

5
00:00:40.500 --> 00:00:41.610
Capstone Coach 1: yeah that screen.

6
00:00:42.750 --> 00:00:43.440
Capstone Coach 1: I don't do it.

7
00:00:45.360 --> 00:00:45.960
Capstone Coach 1: Should.

8
00:00:47.340 --> 00:00:51.240
Capstone Coach 1: So you should you should be able to see my screen now I, I think.

9
00:00:54.570 --> 00:01:03.030
Capstone Coach 1: They bring back up here's the calendar that's what I was trying to show Okay, the calendar it's April, the 13th and it's not Friday that's good.

10
00:01:05.250 --> 00:01:09.360
Capstone Coach 1: Next week is the 20th the 20 and it's actually our last lecture.

11
00:01:10.980 --> 00:01:23.760
Capstone Coach 1: On the 27th then many of you have already volunteered or will be volunteering to give a presentation on the 27th of your class project and i'm not expecting.

12
00:01:24.720 --> 00:01:34.470
Capstone Coach 1: you're going to have something like your capstone i'm expecting that you're just going to demonstrate some knowledge of what we've been doing let's not hit the wrong button.

13
00:01:36.450 --> 00:01:42.870
Capstone Coach 1: yeah there it is some knowledge of what we've been using in class and maybe you you've done a little.

14
00:01:43.680 --> 00:01:57.450
Capstone Coach 1: invented something new and maybe maybe it's just an extension of one of the homework problems or something like that it's all good understand that you've been working on your capstones and preparing for finals and things like that so.

15
00:01:58.500 --> 00:02:16.560
Capstone Coach 1: i'm just really excited that i'd like to hear and see what you've done and, last week we went through and looked at how your projects lined and who's going to be doing what and so forth, if you have any questions or concerns about that, let me know and we'll get back on that.

16
00:02:18.780 --> 00:02:30.210
Capstone Coach 1: Anyway, so we have the lecture tonight and the focus tonight really is on time series analysis using rnn which is recurrent neural networks it's a.

17
00:02:30.570 --> 00:02:43.830
Capstone Coach 1: it's a feature that's very, very strong and charisse and since we're doing charisse it wouldn't be right if we just skipped over it, and said oh forget that read it we're not going to do that we're actually going to talk about.

18
00:02:44.970 --> 00:02:52.140
Capstone Coach 1: What recurrent neural networks are and and look at how you actually implement those so.

19
00:02:54.960 --> 00:03:05.130
Capstone Coach 1: This is tonight the Semester project reviews, there is a some downloads here, if you look on the week 13 folder you'll see a little.

20
00:03:05.910 --> 00:03:10.140
Capstone Coach 1: PowerPoint slides which i'm showing you now and, basically, this is just a.

21
00:03:10.860 --> 00:03:22.380
Capstone Coach 1: more of an outline of what we're going to be talking about a lot, a lot of detail here so we're going to start off by just mentioning that there was a optional assignment for week 1112 rather.

22
00:03:22.740 --> 00:03:35.190
Capstone Coach 1: And the idea, there was that you would take the template from week 12 per time series now that was a template that is based upon the traditional neural network approach to doing time series.

23
00:03:35.700 --> 00:03:50.220
Capstone Coach 1: And that we call that the LP approach, which is using terraces nlp neural network approach now what we do there and we talked about this last time is you create the lags for your data.

24
00:03:50.700 --> 00:04:06.390
Capstone Coach 1: The first slide the second line, the third leg, as many as you want, and then you use those as inputs to the neural network and use the neural network, then, to create a forecast based upon the lags this is similar to.

25
00:04:07.890 --> 00:04:19.710
Capstone Coach 1: Basically fitting and auto a Rima not auriemma excuse me auto regressive moving average model time series models called an ar moving average model.

26
00:04:20.340 --> 00:04:33.780
Capstone Coach 1: On your data in a nonlinear fashion, if you use something like psychic learn to fit a otter aggressive model ar ar to model or something like that.

27
00:04:34.230 --> 00:04:45.930
Capstone Coach 1: that's something like a regression approach, where you are Mina your coefficients that you get for the eight or one, and they are two terms the lag terms are based upon minimizing.

28
00:04:46.500 --> 00:04:52.770
Capstone Coach 1: The some of squares or maximizing the lack likelihood equation for that particular model.

29
00:04:53.220 --> 00:05:01.350
Capstone Coach 1: This approach that we're talking about with charisse basically uses a neural network, which is a more of a non parametric approach.

30
00:05:01.860 --> 00:05:08.070
Capstone Coach 1: There are no necessary assumptions about how much and any or normality or that sort of thing.

31
00:05:08.580 --> 00:05:21.600
Capstone Coach 1: we're treating the time series so basically a random series, and we have a black box, called the charisse neural network black box and we are going to train that box to make forecasts.

32
00:05:21.960 --> 00:05:35.520
Capstone Coach 1: Assuming that the behavior of the series going forward is going to be similar to what happened in the past that's always sort of underlying assumption for any time series model you're assuming that the.

33
00:05:36.450 --> 00:05:44.880
Capstone Coach 1: Situation that's driving the neural the time series hasn't changed dramatically, this is often wrong.

34
00:05:45.690 --> 00:05:58.530
Capstone Coach 1: And, which is why some time series of time series models are not always accurate, is that you may be dealing with the situation, for example, this pandemic that we've had and must upset a lot of.

35
00:05:59.010 --> 00:06:11.550
Capstone Coach 1: forecasting people because you're they're experiencing injections into their time series that they've never seen before, at least in the last you know few decades.

36
00:06:12.930 --> 00:06:23.220
Capstone Coach 1: Alright, so in this, you have I list of the teams your team names and identifications are in here in case you, you want to look that up.

37
00:06:23.550 --> 00:06:35.610
Capstone Coach 1: Also, you have some general comments on the right hand side, this is just more or less notes for myself and maybe for you the telling reminded me of what you're planning to work on.

38
00:06:36.210 --> 00:06:42.690
Capstone Coach 1: If you change your mind about what you want to work on, please let me know via email before.

39
00:06:43.380 --> 00:06:53.640
Capstone Coach 1: The 20 so before that, well before the 27th I hope of the next week or so so basically we're on 13th we have one more class and then we're going to start with our.

40
00:06:54.150 --> 00:07:04.500
Capstone Coach 1: wind down we're going to start with the presentations and so you should this week and next week really be talking to your partners about how's it going.

41
00:07:05.190 --> 00:07:17.040
Capstone Coach 1: hours, the project your presentation going what I would try to do is to finish up the presentation this week and then next week, use the time that is available to you to.

42
00:07:18.090 --> 00:07:26.580
Capstone Coach 1: You know, go over that who's going to say what How is this going to be presented, you know that sort of thing I don't have any particular rules about this.

43
00:07:26.970 --> 00:07:36.630
Capstone Coach 1: I like to see everybody engaged or involved in the presentation, but if you have if you'd rather just have one person present the project that's fine.

44
00:07:37.020 --> 00:07:52.710
Capstone Coach 1: However, regardless of who presents that I would like to have some information in the presentation that says sort of a little bit about who did what what we what everybody's contributions were so that I can better assign points of scores, and things like that.

45
00:07:54.180 --> 00:08:01.920
Capstone Coach 1: Okay well let's move on ahead, are there any questions on the projects before we move on.

46
00:08:03.930 --> 00:08:07.050
Kilani, Shadi: I have a quick question Dr Jones this is killer speaking.

47
00:08:08.700 --> 00:08:09.330
Capstone Coach 1: shadi.

48
00:08:09.450 --> 00:08:09.870
Kilani, Shadi: Yes, sir.

49
00:08:10.170 --> 00:08:14.760
Kilani, Shadi: Okay it's a good question because I didn't see any updates on the notes on the side.

50
00:08:15.750 --> 00:08:17.580
Capstone Coach 1: You know there's no update ship.

51
00:08:17.940 --> 00:08:20.550
Kilani, Shadi: Okay, because I sent your stuff a week.

52
00:08:20.640 --> 00:08:23.760
Capstone Coach 1: Oh, I do it well, I thought I replied right, I have it.

53
00:08:24.150 --> 00:08:25.590
Kilani, Shadi: No, no, Sir, so.

54
00:08:26.310 --> 00:08:29.190
Kilani, Shadi: that's okay whenever you get to it Thank you so much what.

55
00:08:30.270 --> 00:08:33.210
Capstone Coach 1: Did I did I forget that i'm not hang on.

56
00:08:36.180 --> 00:08:41.220
Capstone Coach 1: Oh, I thought I remember seeing that and I did I did not reply.

57
00:08:41.310 --> 00:08:43.410
Kilani, Shadi: So you acknowledge that you got it, but then I.

58
00:08:43.470 --> 00:08:44.760
Capstone Coach 1: Go okay okay.

59
00:08:44.850 --> 00:08:45.180
Kilani, Shadi: Thank you.

60
00:08:46.230 --> 00:08:48.300
Capstone Coach 1: I did I did acknowledge yeah.

61
00:08:50.400 --> 00:08:59.100
Capstone Coach 1: Okay, and I did look at your screenshots and they look great they look fantastic i'll get back with you some more detailed information on that Thank you.

62
00:08:59.640 --> 00:09:00.000
Kilani, Shadi: Thank you.

63
00:09:00.120 --> 00:09:01.080
Okay okay.

64
00:09:02.130 --> 00:09:03.150
Capstone Coach 1: anyone else.

65
00:09:04.860 --> 00:09:06.420
Karis Jochen: Professor this is charisse.

66
00:09:06.450 --> 00:09:07.770
Capstone Coach 1: I have a nurse okay.

67
00:09:08.700 --> 00:09:13.080
Karis Jochen: Do you how long does each team have for the presentation.

68
00:09:13.920 --> 00:09:24.270
Capstone Coach 1: How long let's see a good question we have in total let's say J K L and ABC TV.

69
00:09:27.000 --> 00:09:47.070
Capstone Coach 1: And we have 19 or 20 presentations and we have basically two hours two and a half hours next time and then two and a half hours on May, the fourth, which would be our last was a total of five hours right, so if we take five hours and.

70
00:09:48.270 --> 00:09:50.610
Capstone Coach 1: Let me do is, let me do some quick math here.

71
00:09:52.350 --> 00:10:05.100
Capstone Coach 1: Times five okay so that's 300 minutes and and we divide that by about 20 we get 15 minutes, so I like 1515 minutes does that work for you cares.

72
00:10:07.380 --> 00:10:08.460
Karis Jochen: For me, thank you.

73
00:10:09.690 --> 00:10:11.190
Capstone Coach 1: Did I call you charisse.

74
00:10:12.480 --> 00:10:16.680
Capstone Coach 1: Okay that's kind of embarrassing all right.

75
00:10:20.700 --> 00:10:21.660
Capstone Coach 1: Okay, let me.

76
00:10:22.950 --> 00:10:28.620
Capstone Coach 1: there's something going on here Oh, there you are okay all right okay got it.

77
00:10:30.690 --> 00:10:35.130
Capstone Coach 1: Okay, Jim you looks you are you look like you're in the office what's going on.

78
00:10:38.250 --> 00:10:39.000
Capstone Coach 1: Are you okay.

79
00:10:41.130 --> 00:10:42.330
Capstone Coach 1: I don't see your guitar.

80
00:10:45.180 --> 00:10:46.320
Capstone Coach 1: Oh, you don't you don't.

81
00:10:46.980 --> 00:10:47.400
Know Sir.

82
00:10:48.990 --> 00:10:49.950
Capstone Coach 1: Oh, you don't have a MIC.

83
00:10:53.580 --> 00:10:55.350
Capstone Coach 1: Okay it's okay I.

84
00:10:55.710 --> 00:10:57.240
Jordan Gross: was hanging out in the actual.

85
00:10:57.690 --> 00:10:58.740
Capstone Coach 1: bill here in the class.

86
00:10:58.770 --> 00:11:03.420
Capstone Coach 1: Are you in the classroom as you're you're in the oh, my goodness, I should be there.

87
00:11:04.860 --> 00:11:20.190
Capstone Coach 1: Okay well when we do the presentations which is coming up on the 27th I will be there Okay, I will definitely be there, and if you're interested, we can go out and get a beer afterwards, or something.

88
00:11:21.210 --> 00:11:30.990
Capstone Coach 1: Okay Okay, because I know I know you got your masks ready, but I also know that some of you may be a lot of you have already have the vaccine.

89
00:11:31.530 --> 00:11:41.820
Capstone Coach 1: So I think that's a passport to go down to 52 seasons or yeah there you go go that will go down to one of the places out there and city Center and have a beer so.

90
00:11:42.840 --> 00:11:54.870
Capstone Coach 1: You know you're all in the everyone's invited everyone, regardless of whether you have the vaccine or not, I think you have to bring your your masks, of course, a gym is is is heavy are there tonight.

91
00:11:56.460 --> 00:11:59.700
Capstone Coach 1: yeah hobby is like the mask.

92
00:11:59.730 --> 00:12:00.600
Police.

93
00:12:03.210 --> 00:12:03.900
Capstone Coach 1: He gets me.

94
00:12:04.650 --> 00:12:06.540
Capstone Coach 1: Voices bigger the time.

95
00:12:06.720 --> 00:12:09.360
Capstone Coach 1: If he sees me without my mask I noticed that.

96
00:12:09.750 --> 00:12:21.420
Capstone Coach 1: And that's okay if somebody is somebody asked to be the man, the beliefs right anyway he's good he's good i'm so happy to see people in Houston I I didn't know.

97
00:12:22.830 --> 00:12:23.490
Capstone Coach 1: Okay.

98
00:12:23.760 --> 00:12:26.430
Jim Clark: Okay, well, I get my microphone on now, so I think i'm here.

99
00:12:26.940 --> 00:12:28.860
Capstone Coach 1: you're good you're good yeah.

100
00:12:29.940 --> 00:12:30.360
Capstone Coach 1: yeah.

101
00:12:31.740 --> 00:12:38.400
Capstone Coach 1: Okay, so you have to wear a mask, even though the social distance is like 15 feet.

102
00:12:40.800 --> 00:12:41.490
Jim Clark: We have a.

103
00:12:41.850 --> 00:12:43.890
Jim Clark: We do over TELCO we're told what to do.

104
00:12:44.310 --> 00:12:52.050
Capstone Coach 1: It said it otherwise hobby is going to be coming around and shaking you know he's not gonna be happy we got to keep our vr happy.

105
00:12:54.510 --> 00:12:57.030
Capstone Coach 1: Okay, Charles are you doing tonight she's here.

106
00:12:57.420 --> 00:12:59.580
Swanson J, Charles: I don't find sure okay good.

107
00:13:00.000 --> 00:13:02.970
Capstone Coach 1: let's see who else we got okay so yo.

108
00:13:04.020 --> 00:13:04.320
Capstone Coach 1: hey.

109
00:13:06.450 --> 00:13:11.250
Capstone Coach 1: you're at home, yes i'm at home and i'm doing good okay and there's David.

110
00:13:12.540 --> 00:13:18.270
Capstone Coach 1: David you look like you're in in some you're kind of US foggy or something I don't know.

111
00:13:20.010 --> 00:13:21.360
Purkiss, David: This is bad light.

112
00:13:21.780 --> 00:13:22.260
Capstone Coach 1: yeah.

113
00:13:22.830 --> 00:13:24.180
Purkiss, David: Okay behind me.

114
00:13:26.400 --> 00:13:31.710
Capstone Coach 1: Okay, I guess, I gotta get serious now and let's open up.

115
00:13:32.760 --> 00:13:37.920
Capstone Coach 1: let's see what I have some instructions here to remind me where we are.

116
00:13:38.970 --> 00:13:39.720
Capstone Coach 1: So.

117
00:13:41.190 --> 00:13:52.110
Capstone Coach 1: yeah I wish I wish you would download like you to download this particular file it's called charisse digits nlp rnn P, why.

118
00:13:53.160 --> 00:13:59.970
Capstone Coach 1: and basically what we're going to do tonight is similar to what we did last time we're going to go over some code.

119
00:14:00.330 --> 00:14:06.960
Capstone Coach 1: i'm going to talk about what is an Aryan and or recurrent neural network and how is that different from the.

120
00:14:07.590 --> 00:14:26.160
Capstone Coach 1: You know the nlp, which is the Multi layer preceptor on network and and CNN, which is the convolution neural network so we're this particular code actually has examples of all four of these in the code Karen hi.

121
00:14:27.870 --> 00:14:36.330
Capstone Coach 1: I see you're here you're back from Seattle, are you in Seattle okay okay okay you look like you had a great trip good.

122
00:14:37.140 --> 00:14:51.330
Capstone Coach 1: You happy all right good, so this particular code summarizes the four types of models that that I we've used, we will use some new ones tonight in the analysis of images.

123
00:14:52.050 --> 00:15:03.840
Capstone Coach 1: And so the first one we're going to look at is traditional multi layer preceptor on model for image analysis and here we're looking at image classification analysis.

124
00:15:04.290 --> 00:15:13.740
Capstone Coach 1: we're going to use those pictures of the 10 digits you know the 01234 or five pictures, you know really I hate to pester you with this, but.

125
00:15:14.250 --> 00:15:22.770
Capstone Coach 1: The reason i'm doing it is because that data is online you don't have to download it, you know it's accessible directly from charisse.

126
00:15:23.160 --> 00:15:32.430
Capstone Coach 1: So if you're if you're talking to guess you can pull this data down and number two it's it's a fairly long as everybody knows about this data it's used all over the planet.

127
00:15:32.790 --> 00:15:45.150
Capstone Coach 1: For examples of how to analyze images and so we'll do we're going to use it again now it's not the only example going to look at tonight, but it is the only image example we're going to look at tonight.

128
00:15:45.600 --> 00:15:56.040
Capstone Coach 1: And so we'll look at how we use nlp so multi layer per soprano to do the image analysis you we've seen that before and then CNN.

129
00:15:56.640 --> 00:16:10.560
Capstone Coach 1: Which is that's not the TV thing this is convolution convolution neural network this neural network good approach is traditionally what drives most image recognition programs.

130
00:16:11.100 --> 00:16:16.380
Capstone Coach 1: So a CNN is a little bit different creature than an nlp and we'll look at that.

131
00:16:16.860 --> 00:16:30.840
Capstone Coach 1: Then we're going to also talk about rnn for the first time recurrent neural network and how that differs from CNN and nlp and how it might be used to analyze images and then how you bring these together.

132
00:16:31.290 --> 00:16:45.660
Capstone Coach 1: ordinarily the convolution neural network is brought together, either with an nlp model or an rnn model to actually improve the classification or the or what whatever it is that you're trying to predict.

133
00:16:46.410 --> 00:16:58.650
Capstone Coach 1: So we'll start off with the nlp model, or what no i'm sorry first thing we're going to do is start off with these lines that you see at the top lines 171 through 190 that are in the code so.

134
00:16:59.730 --> 00:17:02.370
Capstone Coach 1: Let me bring this up hey there we go.

135
00:17:19.230 --> 00:17:30.930
Capstone Coach 1: This is it, yes, let me make a little larger here, so you know but easier to see, but this is lines, these are the lines 171 through 190.

136
00:17:31.620 --> 00:17:42.150
Capstone Coach 1: And you'll see it looks a little odd doesn't it, you have X tm LP XTC mm X the rnn and.

137
00:17:42.810 --> 00:17:53.400
Capstone Coach 1: The reason for this is because, even though these are all different variations of a careless neural network they require different input.

138
00:17:54.180 --> 00:18:03.780
Capstone Coach 1: And so the the Multi layer precept tron, for example, wants the inputs to be coming in, on a vector.

139
00:18:04.560 --> 00:18:19.590
Capstone Coach 1: And so that requires that if you look at the shape that it's basically the flat shape, so you have one, it would have war, the first index would be the road number, and then you have one column in there and so.

140
00:18:20.070 --> 00:18:38.460
Capstone Coach 1: it's it looks like a vector as the human this this is this solution, by the way, and then we looked at it last time reduces this flattened approach, where you you take the image and you, you convoluted use use the CNN.

141
00:18:39.690 --> 00:18:51.180
Capstone Coach 1: filters to reorient the or to simplify the image basically smooth it out simplified get rid of the edges, you know that kind of thing that's what you're doing in.

142
00:18:51.420 --> 00:19:00.990
Capstone Coach 1: convolution neural network and then, once that's done, you have to actually you're working with a picture, so you have data that's two dimensional.

143
00:19:01.470 --> 00:19:11.280
Capstone Coach 1: Or maybe three dimensional if you have color and so you have to flatten that out as a single vector and that's what's required for an nlp solution.

144
00:19:12.300 --> 00:19:16.830
Capstone Coach 1: Now the CNN data that comes down is actually three dimensional.

145
00:19:17.340 --> 00:19:28.980
Capstone Coach 1: And you'll see here, it has 123 in addition to the first column is the is the index of the room number, and then you have three columns after that, for the for each image.

146
00:19:29.460 --> 00:19:39.780
Capstone Coach 1: You have the width of the image, you have the height of the image and then there's a third column, and now, and this data it's it's all one.

147
00:19:40.260 --> 00:19:51.450
Capstone Coach 1: But you normally if you add color pictures, there would be three, this would be three dimensions and corresponding to read the blue and the Green layers in the image.

148
00:19:52.200 --> 00:19:59.310
Capstone Coach 1: In a black and white image and these pictures are black and white, you only have one column there so that's why you see the.

149
00:19:59.700 --> 00:20:05.820
Capstone Coach 1: what's happening here is we're going to have to take the incoming data, which is the the photograph right.

150
00:20:06.360 --> 00:20:18.450
Capstone Coach 1: And we have to reshape it so we use the number be reshape command to take that incoming picture and reshape it so that it has this poor column format.

151
00:20:18.960 --> 00:20:31.290
Capstone Coach 1: The first column being the row number, and then the second column would be the width of the picture than the height, the picture and then on the number one or the number three depending upon whether it's black and white or color.

152
00:20:33.060 --> 00:20:37.560
Capstone Coach 1: let's see so they in our images they the width and the height of the same.

153
00:20:38.280 --> 00:20:49.200
Capstone Coach 1: We call her we're using the term image size to refer to that and it's actually 28 pixels So these are kind of small images, which is another reason why these are you being used.

154
00:20:49.470 --> 00:21:02.370
Capstone Coach 1: If we use real photographs would be like 2000 by 2000 and or more and and downloading that and running that takes a lot more time so that's why part of the reason why we're using this example data.

155
00:21:03.060 --> 00:21:18.240
Capstone Coach 1: Now, and they are in in world instead of using four columns to or to orient ourselves in the images rnn wants this us to actually three excuse me, the first column would be the row number.

156
00:21:18.870 --> 00:21:32.940
Capstone Coach 1: The second column, it says him a mid size here not really so we'll be talking about this in a moment, this actually the second column here is actually the sequence number.

157
00:21:33.300 --> 00:21:53.250
Capstone Coach 1: So we're arguments are are specifically oriented towards analyzing data that are collected over time and this second column, then, is your time in Tibet index or step index, and then the third column is the size of the basically of the data.

158
00:21:54.420 --> 00:22:10.530
Capstone Coach 1: Now ordinarily this if you had a traditional time series this this third column would be the number one because you just have a sequence of numbers, like stock prices, but you can use our lens to analyze multi variable.

159
00:22:11.820 --> 00:22:23.520
Capstone Coach 1: Time series, this is fantastic so you have something like stock price on IBM, but then you have a bunch of other data, maybe it's some sales information that you have on on the on the company.

160
00:22:23.820 --> 00:22:36.780
Capstone Coach 1: Or maybe its sales information in that sector, so this is business equipment business supplies that they sell so maybe you have some other data that relate to forecasting, the price of IBM.

161
00:22:37.050 --> 00:22:57.540
Capstone Coach 1: You can bring that into a neural network, a specifically and rnn and you can use that to assist you and in forecasting, the main variable that the target variable there, so are in in as a three column format CNN has a four column format.

162
00:22:58.800 --> 00:23:04.170
Capstone Coach 1: nlp and basically has a two column format, the road number and then.

163
00:23:05.400 --> 00:23:07.200
Capstone Coach 1: The actual variable itself.

164
00:23:08.580 --> 00:23:09.150
Capstone Coach 1: and

165
00:23:10.230 --> 00:23:20.580
Capstone Coach 1: If you're doing doing these analysis if you're using these neural networks on fortunately you have to make sure that the data coming in.

166
00:23:21.630 --> 00:23:34.380
Capstone Coach 1: meets certain conditions, first of all, they all require the data to be nothing so they're all requiring they wouldn't care sees input number coming in it's not the number two it's floating point.

167
00:23:34.980 --> 00:23:47.820
Capstone Coach 1: that's why you see this these lines, right here that say X T mo P is equal to X dlp period as type float 32.

168
00:23:48.600 --> 00:23:58.230
Capstone Coach 1: And then i'm actually normalizing this, these are, since these are black and white photographs and color would be the same all of these are really basically integers that come in.

169
00:23:58.560 --> 00:24:11.970
Capstone Coach 1: And those indicators are skilled between zero and 255 so we're dividing them by 255, which means we have to use floating point we can no longer store them as m m integers.

170
00:24:12.450 --> 00:24:22.260
Capstone Coach 1: And they're going to be floating point numbers that go between zero to one that's plus zero plus one yeah.

171
00:24:23.250 --> 00:24:27.870
Capstone Coach 1: Now some people would say why don't you just keep them in senators well I didn't write charisse.

172
00:24:28.170 --> 00:24:36.090
Capstone Coach 1: That I would have made that I think that should be an option but it's not an option here and cares so they want it to come in as floating point that's fine.

173
00:24:37.050 --> 00:24:49.620
Capstone Coach 1: And you notice i'm doing that for all three types and making sure that they are properly, you know formatted for for charisse if you do not scale them, the way that we're doing this here.

174
00:24:50.460 --> 00:25:02.220
Capstone Coach 1: you'll find that the algorithms the algorithms that are used to optimize the neural network, the, these are the ones that are used to estimate the weights in the network, they don't work as well.

175
00:25:02.940 --> 00:25:12.870
Capstone Coach 1: So this little statements these statements that we see here actually a very beneficial and terms of fitting getting a good fit and the neural network.

176
00:25:14.010 --> 00:25:26.400
Capstone Coach 1: And so, if your ego things get tricky things get kind of tricky let's suppose you know we do we're going to see this, we have an image right so i'm going to want to use the convolution.

177
00:25:27.060 --> 00:25:39.540
Capstone Coach 1: phil approach for images, because what that does is it'll take images can be awfully huge I mean thousands of pixels right, so you want to smooth that out.

178
00:25:39.870 --> 00:25:52.410
Capstone Coach 1: And you want to shrink the image down to a smaller, more manageable size in terms of its representation, so the convolution neural network is what allows us to do that.

179
00:25:52.860 --> 00:26:07.320
Capstone Coach 1: And we saw this and you'll see it again tonight on how this happens is, if you go from 28 by 28 picture down to a maybe a five by five or three by three something like that something that where the we don't have.

180
00:26:08.490 --> 00:26:22.290
Capstone Coach 1: Too many weights, now we did, and we are able and terrace to work with millions of weights that can happen, but at some point, your computer's going to get tired and then it'll get sleepy in slow down right.

181
00:26:23.100 --> 00:26:40.470
Capstone Coach 1: Well here's we have tonight we have rnn rnn is on is our new thing for tonight and the basic idea with rnn, and this is a new thing, by the way, and before charisse you really couldn't do this.

182
00:26:41.190 --> 00:26:53.370
Capstone Coach 1: The software was not available to do this and basically what's happening here is, you can with our, then you can take virtually any neural network.

183
00:26:54.180 --> 00:27:12.300
Capstone Coach 1: And you can put it, make a loop inside of a network that feeds back to the hidden layers feeds back to the hidden layers to the estimators so the weights and things like that information about what just came through the door.

184
00:27:13.200 --> 00:27:20.610
Capstone Coach 1: So, in a time series you really want to know how well did we do forecasting that last observation.

185
00:27:21.300 --> 00:27:29.610
Capstone Coach 1: And if we didn't do too well, maybe we're too low or too high, then I want to adjust the weights, the next time around, to compensate for that.

186
00:27:30.270 --> 00:27:43.530
Capstone Coach 1: This is what our an end does so, if you're if you're trying to if you're fitting the you typical time series you use an ar auto regressive model, which means I only look at what's.

187
00:27:44.070 --> 00:28:00.030
Capstone Coach 1: coming in the door, what what happened yesterday and the day before, to make my predictions for tomorrow, I do not look, I do not take into account how well we've been forecasting the data that have been coming in.

188
00:28:01.080 --> 00:28:11.160
Capstone Coach 1: Now, in the arena case this is called the if I am a integrated moving average terms in a in a time series model.

189
00:28:11.940 --> 00:28:16.740
Capstone Coach 1: Often you'll you just use a straight ar model otter aggressive.

190
00:28:17.430 --> 00:28:28.260
Capstone Coach 1: But, in some cases it's not unusual, you will have to bring in the moving average terms and that's called the IMA or the integrated moving average terms.

191
00:28:28.620 --> 00:28:39.090
Capstone Coach 1: And you saw that probably in your time series course, how you do that with a remote are in in essentially implements that particular type of model.

192
00:28:39.720 --> 00:28:46.560
Capstone Coach 1: So, with an nlp approach to time series we've seen that and the memo P approach.

193
00:28:46.950 --> 00:28:56.820
Capstone Coach 1: We can model any ar model we can we can fit any any order aggressive model you'd like you'd like an ar model that goes back 100 terms, no problem.

194
00:28:57.150 --> 00:29:10.650
Capstone Coach 1: we'll just take a lag of 100 to the data and CERT and put that in this an input to the neural network no problem and the neural network should do a pretty fair job of forecasting any a our model.

195
00:29:11.190 --> 00:29:23.520
Capstone Coach 1: Which is the most common type of model or time series anyway, if you, especially if you start allowing us to take differences and smooth out the series take trends out of the series essentially using differences.

196
00:29:24.390 --> 00:29:29.910
Capstone Coach 1: However, sometimes that's still not enough and and the typical example of that would be a case where.

197
00:29:30.390 --> 00:29:42.630
Capstone Coach 1: You have a time series that's coming in and looks like a zigzag zigzag it looks like a sawtooth pattern in a song or something like that up down up down up down so if the last one is up, you know this one's going to be down.

198
00:29:43.020 --> 00:29:56.970
Capstone Coach 1: At the last one was down, you know the next one is going to be up that's a typical of a moving average or integrated moving average time series where the actual cyclical pattern changes dramatically over a short period of time.

199
00:29:57.600 --> 00:30:06.420
Capstone Coach 1: That kind of fact that kind of a model and you could see some of this here, if you look, this is the IBM time series.

200
00:30:06.870 --> 00:30:17.550
Capstone Coach 1: And the blue line here is the actual series, and you can see how it goes up and down up and down the sort of yeah there's a there's an overall trend here.

201
00:30:17.880 --> 00:30:23.460
Capstone Coach 1: But then, if there is something fighting that trend it's not very smooth going up and down up and down.

202
00:30:23.790 --> 00:30:36.780
Capstone Coach 1: That kind of up and down stuff usually is indicative of an integrated moving average and I am a term in the in the time series, and so, if you fit this using the auto regressive approach what you get is.

203
00:30:37.320 --> 00:30:50.820
Capstone Coach 1: Basically, a smoothing of this blue line and the the predictions, then, will be more or less what you might get if you were to take a pen and draw a smooth curve through this pattern that you see here.

204
00:30:51.660 --> 00:31:06.120
Capstone Coach 1: If you incorporate in addition to the ar terms you incorporate the moving average terms, then you can get to better predictions, which will be better at job at forecasting these ups and downs that you see in here.

205
00:31:07.080 --> 00:31:23.160
Capstone Coach 1: Still there's going to be some error in this, and anyway let's take let's go ahead and take a look at this so as I mentioned this in this little business up front is very important if you're planning to do a time series analysis for.

206
00:31:23.880 --> 00:31:32.220
Capstone Coach 1: For your data you're going to be your legal need to be careful about this, especially if you are let's suppose that you already have.

207
00:31:32.520 --> 00:31:38.340
Capstone Coach 1: A and nlp version of the time series which is the movie the you know the.

208
00:31:38.700 --> 00:31:48.480
Capstone Coach 1: Traditional neural network or porch and you've got lags and you've got the otter aggressive lives and you're fitting that kind of a model, and now you want to go use.

209
00:31:48.900 --> 00:31:55.230
Capstone Coach 1: An orientation model well you're going to have to change the data you'll have to reshape the data before you do that.

210
00:31:55.740 --> 00:32:05.970
Capstone Coach 1: But don't worry we're going to look at how we do that, how we reshape the data, first of all up on the next one starting here in line number 192 is the.

211
00:32:06.570 --> 00:32:21.690
Capstone Coach 1: Multi layer preceptor on model for basic now we're looking at the images, in this case now what happens with images is we're going to notice that we have three dimensions for an image with hype color.

212
00:32:22.380 --> 00:32:34.320
Capstone Coach 1: Basically, three dimensional data, so the sensors that are coming in, will have four dimensions to them, the first dimension will be the the actual image number.

213
00:32:35.070 --> 00:32:47.430
Capstone Coach 1: The second would be the the width the third would be the height and the fourth one would be the color the color layer now in black and white there's only one color layer, however, the tensor.

214
00:32:48.090 --> 00:33:02.550
Capstone Coach 1: indexes are still there, so if you want to address a particular pixel you actually have to give it a four level index and the code if you're going to do that we're not going to have to do that, but.

215
00:33:03.030 --> 00:33:15.900
Capstone Coach 1: i'm just pointing out that the natural characteristic of an image is basically fourth dimensional now the ml P or nlp model multiplayer preceptor on.

216
00:33:16.740 --> 00:33:30.660
Capstone Coach 1: can handle that kind of thing fairly well, but if you look at the me scoot up here to the top you'll see that it's just reshaped so instead of having four columns I now only have two columns.

217
00:33:31.080 --> 00:33:37.710
Capstone Coach 1: The first column would be as you see here it's it's then reshaping if you put a minus one there, it says all of it.

218
00:33:38.190 --> 00:33:49.200
Capstone Coach 1: All of it take all of it, this is the first column the index number for the role that song, you know for each of the images as a wrong number and image number.

219
00:33:50.100 --> 00:34:05.640
Capstone Coach 1: So the first the first index is going to be the image index, and then the second one, will be a number I pixel number and notice that the pixels is 28 times 24 784 pixels in that in that image.

220
00:34:06.120 --> 00:34:22.560
Capstone Coach 1: and basically what we're doing in an nlp approaches, we are flattening the image so it's just one strong one long list or a vector of numbers that represent that image it's not no longer two dimensional it's now one dimensional.

221
00:34:23.610 --> 00:34:33.420
Capstone Coach 1: And the way this works is the first 28 numbers will be the first row in the image, the next 28 the second row and so forth, and so on, just strings strung together.

222
00:34:34.080 --> 00:34:43.020
Capstone Coach 1: So this reshaping business basically takes that two dimensional photograph and strings without as one long vector now if you had.

223
00:34:43.590 --> 00:34:54.630
Capstone Coach 1: If you had a three layer image, then you would have to be a little bit more careful here, and you might want to actually create a different.

224
00:34:55.050 --> 00:35:05.400
Capstone Coach 1: variable for each layer red blue and green or you just might string them all out read first green second you know yellows the third or something like that.

225
00:35:07.590 --> 00:35:08.250
Capstone Coach 1: Okay.

226
00:35:09.870 --> 00:35:15.900
Capstone Coach 1: So, if you look at the curious neural network, then, and this is going to be in lines.

227
00:35:16.950 --> 00:35:30.750
Capstone Coach 1: What 197 to 209, these are the lines that define what the neural network is going to look like, and this is a traditional nlp neural network, and so we have two hidden layers.

228
00:35:31.380 --> 00:35:42.690
Capstone Coach 1: At no excuse me, we only have one hidden layer in this case, and you see it says layers dense 64 I have 64 neurons and one hidden layer

229
00:35:43.410 --> 00:35:57.240
Capstone Coach 1: And the word dense says that this particular hidden layer everything is connected to everything i've got 64 neurons every single one of them is connected to every single input.

230
00:35:57.930 --> 00:36:09.720
Capstone Coach 1: And every single one of them every single neuron all 64 are connected to the neurons in the next layer now the next layer happens to be the output layer

231
00:36:10.050 --> 00:36:19.920
Capstone Coach 1: And there are 10 neurons there because was just a classification problem each of the 10 neurons represents a probability the first one.

232
00:36:20.310 --> 00:36:30.900
Capstone Coach 1: would be the probability that you get zero that you have a secret i'm the second one would be the probability that you have the number one, and then the third one number two and so forth, and so on.

233
00:36:31.980 --> 00:36:48.990
Capstone Coach 1: The summation of the numbers assigned to each of these 10 if you added them together will always be one exactly one, and the reason, this is achieved is by this magic function called soft soft Max it's required.

234
00:36:49.410 --> 00:37:01.290
Capstone Coach 1: activation must be soft Max when you have a nominal output, that would be you know anything where you have more than three or two levels of categories that you're trying to predict.

235
00:37:01.770 --> 00:37:13.170
Capstone Coach 1: If you're predicting binary then instead of soft Max you would have sigmoid si G mo ID which is basically says i'm going to give you a number between zero and one.

236
00:37:14.340 --> 00:37:17.190
Capstone Coach 1: And and we're going to forecast the event.

237
00:37:18.060 --> 00:37:30.090
Capstone Coach 1: we're going to assign a probability to the event and the probability for the none of it is going to be one minus that number right, so we don't have to have two neurons we only put one for binary.

238
00:37:30.540 --> 00:37:39.210
Capstone Coach 1: way, however, if we have a categorical situation like we do here, then we're going to have one output neuron for every every particular guy every category.

239
00:37:40.350 --> 00:37:48.570
Capstone Coach 1: i'm using regularization here, it turns out that this is not a bad idea, you see, where it says, Colonel regularization.

240
00:37:49.290 --> 00:38:01.710
Capstone Coach 1: equal to regular risers thought it all one, so this is the l one regularization I like the old one, because basically What it does is it penalizes.

241
00:38:02.130 --> 00:38:08.550
Capstone Coach 1: The various coefficients that are the weights, in this case that have little or no impact on the output.

242
00:38:09.090 --> 00:38:17.340
Capstone Coach 1: So they may there may be a large number associated with the weight because of randomness or something like that, but if it's not really being.

243
00:38:17.880 --> 00:38:29.130
Capstone Coach 1: influence in the output, then this regularization will push the weight down towards zero and it'll make it easier than to get some better.

244
00:38:29.640 --> 00:38:38.460
Capstone Coach 1: model fits and convergence in the network, so I like using this and you can only if you make this number one E minus the six.

245
00:38:38.910 --> 00:38:43.710
Capstone Coach 1: If you make that number small enough basically you're removing regularization.

246
00:38:44.430 --> 00:38:58.590
Capstone Coach 1: If you make the number of large, then all of your weights, are going to close to zero, and you, you basically going to get only one forecasts and that's the events right so everything goes, all the way to go to zero and.

247
00:38:59.670 --> 00:39:09.420
Capstone Coach 1: Actually, the forecast, I think, is the non attendance is that everything goes to that, so I start usually the one thing about either the minus six.

248
00:39:10.110 --> 00:39:19.650
Capstone Coach 1: Actually I made me actually started smaller like either the minus eight or something like that, because I know at that point regularization is not really kicking in.

249
00:39:20.070 --> 00:39:26.790
Capstone Coach 1: And then, what I will do is, I will make it smaller and smaller and smaller until I can start to see the network getting.

250
00:39:27.600 --> 00:39:38.130
Capstone Coach 1: Strange or doing unusual different things right, so if you make that one even one is the eighth and what happens is it'll you'll get a solution.

251
00:39:38.970 --> 00:39:46.740
Capstone Coach 1: Now you change it to one minus seven, the solution doesn't change which says that the regularization is not really kicking in.

252
00:39:47.160 --> 00:40:00.660
Capstone Coach 1: And so you keep making it smaller smaller and smaller until you could see the network fit is changing now hopefully it's changed it to the better if it's not back off remove the regularization.

253
00:40:01.260 --> 00:40:10.320
Capstone Coach 1: Typically, though, I find that if you add regularization then this way, and you sort of slowly ratchet it down the larger larger values.

254
00:40:11.190 --> 00:40:20.370
Capstone Coach 1: What happens is very quickly at some point you'll find a sweet spot So this has given me much better predictions right here, stop.

255
00:40:20.820 --> 00:40:31.350
Capstone Coach 1: or you'll find that, as you start to put the pressure on with regularization you'll find that you get bad results, in which case you see you see your want to.

256
00:40:31.860 --> 00:40:39.450
Capstone Coach 1: Basically, just a comment I just put a comment right in front of Colonel here, you know I just type like I come in sign one.

257
00:40:40.440 --> 00:40:51.690
Capstone Coach 1: pound sign in front of it no regularization just so you can leave it there, knowing the in your code noting that we I tried it, but it didn't work too well, so i'm going to keep leave it there for right now.

258
00:40:53.160 --> 00:41:14.040
Capstone Coach 1: Now there is something, of course, the activation if it's the output it's going to be soft Max for multivariate multi normal normal calcification for binary it's going to be sigmoid and then for regular forecasts of a number you're going to just have none here I usually make this none.

259
00:41:15.630 --> 00:41:21.600
Capstone Coach 1: No parenthesis just none like it is right right and they're next door, of course, the name is output.

260
00:41:22.980 --> 00:41:29.460
Capstone Coach 1: Active as they have active ization for the hidden layer banana up at layer you can use.

261
00:41:30.210 --> 00:41:39.540
Capstone Coach 1: That here as well, notice that i'm seeing the Colonel regular is it regularize her is none, which means that i'm not using any regularization here.

262
00:41:40.350 --> 00:41:58.140
Capstone Coach 1: activation is already tell you this seems to be really fast and robust, you can change this to other things like hyperbolic tiana tan HR or you can also just use sigmoid if you want they're there, you can have a whole variety of different activations you can use and sometimes so often.

263
00:41:59.280 --> 00:42:09.720
Capstone Coach 1: The regularization the solution, you get is influenced by that activation that you're using and so you might you sometimes play without a bit as well.

264
00:42:10.290 --> 00:42:12.120
Karen Fireman: Dr Jones it's Karen can I see what.

265
00:42:12.120 --> 00:42:13.080
Capstone Coach 1: Sure sure.

266
00:42:13.410 --> 00:42:28.440
Karen Fireman: So um so I see that you use the regular Iser in the output layer and you didn't sit in the layer one is it usual that you use it in the output later I wasn't clear on that, in general, like a Mummy.

267
00:42:28.440 --> 00:42:31.530
Capstone Coach 1: Oh actually it's usually the other way around, but.

268
00:42:31.770 --> 00:42:33.330
Karen Fireman: Did I make a mistake of what you did.

269
00:42:33.960 --> 00:42:47.910
Capstone Coach 1: No, no, no, no, no, you can do you can you can do whatever you'd like to do, and which is better well trial and error, you know you just try it here I didn't work there, I try it here doesn't work, you know.

270
00:42:48.420 --> 00:42:49.200
Capstone Coach 1: travel there.

271
00:42:50.220 --> 00:42:50.340
Karen Fireman: Oh.

272
00:42:51.270 --> 00:42:53.940
Karen Fireman: yeah we're often you and put it in the first layer me.

273
00:42:53.970 --> 00:42:57.660
Capstone Coach 1: I would usually I try the first and stay away from the up but yeah.

274
00:42:57.840 --> 00:42:59.850
Karen Fireman: You do have it right now in the output layer right.

275
00:42:59.880 --> 00:43:00.990
Capstone Coach 1: I do, I do.

276
00:43:02.940 --> 00:43:03.330
Karen Fireman: Thank you.

277
00:43:03.600 --> 00:43:05.070
Capstone Coach 1: I hope I had a reason for that.

278
00:43:06.420 --> 00:43:07.020
Karen Fireman: I don't know.

279
00:43:08.730 --> 00:43:16.710
Capstone Coach 1: we're gonna we're gonna play with this little bit and see what happens, just for the heck of it now, there are a couple of the things that you'll notice in here.

280
00:43:17.310 --> 00:43:34.260
Capstone Coach 1: Coming going down you I put all the optimizer sin like Adam at a delta sgt, these are all the top auto optimizer that you have available in this kind of unnatural network, the optimizer super super critical.

281
00:43:35.610 --> 00:43:39.990
Capstone Coach 1: So you definitely want to try different approaches here.

282
00:43:41.040 --> 00:43:43.350
Capstone Coach 1: The learning rate in particular.

283
00:43:44.880 --> 00:43:56.910
Capstone Coach 1: This is sort of like the regular eyes her that I was talking about, if you have a small learning rate like 001 here 001 something like that.

284
00:43:57.510 --> 00:44:08.040
Capstone Coach 1: Probably the optimizer is going to behave itself, but if you bring it up and make it too large you'll see the solutions can go to heck real quick.

285
00:44:08.580 --> 00:44:17.940
Capstone Coach 1: So you want to play with these try, if you eat, this is a picture on the right hand side of where we are with the time series.

286
00:44:18.300 --> 00:44:27.990
Capstone Coach 1: And the picture of the forecasts right it's really important for me to see this because if you choose the wrong optimizer that picture is going to look horrible.

287
00:44:28.290 --> 00:44:37.230
Capstone Coach 1: The red dots will not fall on top of the blue last things right that's what you want, you want the red dots to track the blue line if it's not doing that.

288
00:44:37.590 --> 00:44:44.250
Capstone Coach 1: And it's why i'm really off the the blue line then your optimizer is not doing its shop.

289
00:44:45.150 --> 00:44:56.940
Capstone Coach 1: And one of the best optimizer is here is that at a delta, but if you don't get the urn and the learning rate tune that will it can give you a horrible horrible forecasts.

290
00:44:57.840 --> 00:45:14.220
Capstone Coach 1: Okay, so if you see your forecasts are wild and like ugly i'm not good look at the optimizer is that for the first thing you want to look at say Okay, let me try a different optimizer let me change the learning rate, though, you can go play with that for a while okay.

291
00:45:15.240 --> 00:45:25.170
Capstone Coach 1: And then under the next one you have them the model compile you have you must have a loss function normally, of course, for classification, is going to be crossing entropy.

292
00:45:25.590 --> 00:45:34.380
Capstone Coach 1: elite either binomial or categorical here we have 10 different digits that we're trying to classify so it's categorical.

293
00:45:34.950 --> 00:45:53.310
Capstone Coach 1: But if it was a binary would have binary cross entropy and if it's neither of those is that so regression type problem where i'm going to forecast, why then you would use mean squared error and it's actually spelled out means on a score squared on the score error okay.

294
00:45:54.360 --> 00:46:04.440
Capstone Coach 1: And then the optimizer well that's The thing that I mentioned that you want to tune that try tuning that, especially if the picture forecast doesn't look good.

295
00:46:05.010 --> 00:46:14.970
Capstone Coach 1: And then on metrics you can spell out the word accuracy as i've done here, and some cases, you can get away with abbreviating that ACC.

296
00:46:15.330 --> 00:46:27.180
Capstone Coach 1: Now why the difference, I am not sure I think this is a version one of tensor versus a version to version one you use the abbreviation ACC and version to spell it out.

297
00:46:27.660 --> 00:46:39.600
Capstone Coach 1: And I think, in most cases the version two siblings still accepts the abbreviation, but I think you're better off just to spell it out so that there's going forward void version three who knows.

298
00:46:40.770 --> 00:46:42.870
Capstone Coach 1: But anyway, accuracy.

299
00:46:43.920 --> 00:46:47.040
Capstone Coach 1: accuracy, by the way, is usually the same thing as last.

300
00:46:48.540 --> 00:46:59.520
Capstone Coach 1: But not always so in case of cross entropy a you if it's binary cross entropy I believe you will see the accuracy and the binary cross entropy are the same.

301
00:47:01.140 --> 00:47:15.990
Capstone Coach 1: Okay, then let's see then we come down, and you can see what we're doing for the rest of this we are actually printing out the results and giving us a graph so let's go up here to where it says true.

302
00:47:17.040 --> 00:47:23.220
Capstone Coach 1: nlp model, make sure this is true, it should say faults on your version right now.

303
00:47:24.300 --> 00:47:25.440
Capstone Coach 1: does it say false.

304
00:47:27.180 --> 00:47:30.060
Capstone Coach 1: I think I uploaded it is false and make sure.

305
00:47:31.620 --> 00:47:43.380
Capstone Coach 1: OK change it to true and make sure the one that says CNN model, make sure that says false because I just want to run at this point, I just want to run the first one, which is the nlp model.

306
00:47:44.460 --> 00:47:49.980
Capstone Coach 1: Okay, so this is true, and then everything else should be.

307
00:47:51.600 --> 00:47:52.410
Capstone Coach 1: False.

308
00:47:53.640 --> 00:47:55.110
Capstone Coach 1: i'm going to close this.

309
00:47:56.520 --> 00:47:57.120
Capstone Coach 1: like that.

310
00:47:58.860 --> 00:48:01.410
Capstone Coach 1: I hope I have the right Program.

311
00:48:05.160 --> 00:48:07.680
Capstone Coach 1: This is, this is a fairly complicated program so.

312
00:48:10.410 --> 00:48:30.210
Capstone Coach 1: Up till now, I haven't had a perfect batting record for this have nothing I get this going, and then you know it didn't work well, so Okay, so this is should say true nlp model and let's click on the little green arrow and kick it off, and we should then get the thing going.

313
00:48:31.320 --> 00:48:37.590
Capstone Coach 1: And you see it's it says that we have a hidden layer with 64% ron's.

314
00:48:38.310 --> 00:48:54.150
Capstone Coach 1: And 550 thousand weights, because those pictures at 784 pixels so basically what we're doing is we're fitting the traditional neural network, and we are flooding the input network network.

315
00:48:54.780 --> 00:49:05.670
Capstone Coach 1: With 784 inputs, one for each of Pickles it pixels this is brute force now imagine if you had a color to the image with three layers.

316
00:49:06.120 --> 00:49:20.700
Capstone Coach 1: So you got 784 times three and usually the images will have several thousand pixels and the width and the height now, so you may have a million pixels easy, and if you use this approach.

317
00:49:22.410 --> 00:49:27.510
Capstone Coach 1: All bets are off because the number of inputs here is going to be huge.

318
00:49:28.590 --> 00:49:43.620
Capstone Coach 1: Here we're not too bad it used to four tenths of a minute, because the, the number of pixels, this is only 784 you can see that it we stopped it at a park number 20.

319
00:49:44.490 --> 00:49:56.970
Capstone Coach 1: And the size that we're using here is 128 we could make that smaller the smaller, it is the longer the the the program will run.

320
00:49:57.360 --> 00:50:05.940
Capstone Coach 1: You can see in this little graph the red dots are the training data and the blue line is the validation so we're doing well pretty well with the training data.

321
00:50:06.210 --> 00:50:12.180
Capstone Coach 1: But the validation data, the accuracy and the loss pretty much plateau and stay there.

322
00:50:12.720 --> 00:50:29.850
Capstone Coach 1: So, if you look at the little there's a little table below the charge that says training point oh five, this is the loss validation point one, so the training losses significantly less than validation which means, what we are overfitting in the day that.

323
00:50:31.830 --> 00:50:39.450
Capstone Coach 1: Most people would look at this and say oh good congratulations you fit the training data really well look at this.

324
00:50:40.920 --> 00:50:45.900
Capstone Coach 1: bad guy 54 observations that were misclassified out of 60,000.

325
00:50:47.010 --> 00:50:54.780
Capstone Coach 1: that's good, and so the last there's showing up as point 9% 54 out of 60,000.

326
00:50:55.290 --> 00:51:11.070
Capstone Coach 1: But then misclassification for it's actually not bad 232 out of 10,000, but it is significantly different hire 2.4% than the training data, so I would say that the nlp classifier is doing well, but.

327
00:51:11.610 --> 00:51:25.500
Capstone Coach 1: it's it's correctly classifying approximately 98% of the images 98% and missing about two per 2.3% of the images so.

328
00:51:27.090 --> 00:51:30.900
Capstone Coach 1: So we hopefully we can do better with maybe a different approach.

329
00:51:32.010 --> 00:51:38.160
Capstone Coach 1: let's go to the next one now okay let's let's turn this from true let's turn this back to false.

330
00:51:39.510 --> 00:51:49.590
Capstone Coach 1: Because we're going to go to CNN right that convolution neural network model and i'm going to change that thoughts to a true.

331
00:51:50.160 --> 00:52:00.990
Capstone Coach 1: And before we run it, though let's look at the code here and see how it's different first of all notice that with CNN, we have to give it an input shape.

332
00:52:01.560 --> 00:52:17.310
Capstone Coach 1: Three dimensional remember I said with height and and number of channels that one says, these are black and white don't worry about it okay that that if we had color That would be a three that last number would be a three.

333
00:52:18.060 --> 00:52:19.080
Capstone Coach 1: But, at any rate.

334
00:52:19.530 --> 00:52:30.720
Capstone Coach 1: If you're using the convolution of basically it's a neural network filter, then this is what you would do you would start the network off using con Tu de.

335
00:52:31.410 --> 00:52:37.620
Capstone Coach 1: You have the number of preceptor ons here they call it filters each filter, by the way, is actually.

336
00:52:38.340 --> 00:52:53.100
Capstone Coach 1: it's like a map at the view the mats rather and this case it's a three by three three pixels by three pixels and what happens is it's laying the this three nine weights in this little three by three.

337
00:52:54.540 --> 00:53:08.790
Capstone Coach 1: overlay filter and it applies the weights to each of the pixel numbers in that grid nine pixels numbers and then it it smooths them out using the certain you know algorithm.

338
00:53:09.420 --> 00:53:20.280
Capstone Coach 1: And the idea here is that at this level, where you're looking at a three by three image of the you're really getting down to the pixels in the image and you want to smooth it out.

339
00:53:20.640 --> 00:53:28.800
Capstone Coach 1: into one pixel so it's going to basically take those three by three by area three area and smooth it out into one pixel.

340
00:53:30.420 --> 00:53:39.570
Capstone Coach 1: So the Colonel size, has it has to do with how big of a grid, do you want to use these are, since these are 28 by 28 three by three makes sense.

341
00:53:40.200 --> 00:53:47.790
Capstone Coach 1: But if we had an image let's say there was 2000 by 2000 maybe would be using 300 by 300 OK.

342
00:53:48.570 --> 00:53:57.300
Capstone Coach 1: So the don't think that this is typical the Colonel size here could be much larger depending upon the size of the image.

343
00:53:58.200 --> 00:54:08.820
Capstone Coach 1: activation is almost always already Liu because it's quick, no, no, God calculations required notice that I have been playing here with the regularization.

344
00:54:09.450 --> 00:54:28.770
Capstone Coach 1: And maybe we'll turn that on in a minute, and then, of course, you have input shape equal to that little to pull this is a typical up here, given the actual shape of the image as well as indication of the number of channels or colors that are in the image.

345
00:54:30.690 --> 00:54:37.200
Capstone Coach 1: I only know of one in three but I guess if you're using thermal imaging you might have you might have more than one over there.

346
00:54:38.370 --> 00:54:42.360
Capstone Coach 1: might be a five or something I don't know I haven't done anything more on the chain so.

347
00:54:43.650 --> 00:54:55.560
Capstone Coach 1: So then below succumb to D we almost always have a pool it's either an average pooling or Max pooling and this me in these images the Max pooling seems to work better than the average blue.

348
00:54:56.220 --> 00:55:13.320
Capstone Coach 1: And Max pooling what happens is it actually averages, or whether it looks at it says full size to it looks at a two by two window and and replaces all four pixels with only a single pixel and that would be the maximum of the four.

349
00:55:14.010 --> 00:55:18.150
Capstone Coach 1: If you use the average pooling then it's going to look at those four pixels and.

350
00:55:18.960 --> 00:55:28.650
Capstone Coach 1: replace them with just a single pixel that's the average of all four I did try averaging on these images, instead of Max and I didn't have any luck.

351
00:55:29.460 --> 00:55:45.330
Capstone Coach 1: By the way, but so maxine to be working okay for these these particular images, I think that could be a function of black and white, where you might have some little white dots on the image, or something like that, and you want to get rid of those.

352
00:55:46.620 --> 00:55:47.190
Capstone Coach 1: The.

353
00:55:48.900 --> 00:55:56.850
Capstone Coach 1: Okay, and then, so the pool is right, right after the COMP 2d that here's another i'm saying Okay, we smooth at one time but smooth it again.

354
00:55:57.720 --> 00:56:05.640
Capstone Coach 1: Another con foodie and we have another Max pool or after that, and then you can have as many of these convolutions as you'd like.

355
00:56:06.420 --> 00:56:15.990
Capstone Coach 1: to smooth out the image and to reduce the actual number of pixels that you want to work with in your s cost allocation or regression network.

356
00:56:16.530 --> 00:56:25.680
Capstone Coach 1: So after you get through convolution if you're going to go to a traditional neural network, you always have to flatten that guy right there is required.

357
00:56:26.370 --> 00:56:35.700
Capstone Coach 1: Because what that does is it says okay i've got some new images here, there are two two dimensional three dimensional I want to make that a single long vector.

358
00:56:36.210 --> 00:56:50.580
Capstone Coach 1: A flat I was that's what the flattened bus, so it just takes all of those pixels that are that are basically organized in the kisser as a two dimensional or three dimensional tense or it puts it into a single dimension, not be vector.

359
00:56:51.210 --> 00:57:00.810
Capstone Coach 1: that's what flattened us and then one with that we can push that into the traditional nlp neural network, and you can see, after that is flatten.

360
00:57:01.230 --> 00:57:18.870
Capstone Coach 1: These guys, these are just nothing more than the traditional hidden layer number one 300 neurons hidden layer number two 100 neurons and of course here's the output layer the 10 neurons soft Max activation.

361
00:57:19.980 --> 00:57:23.040
Capstone Coach 1: No regular nope no regularization Karen.

362
00:57:25.200 --> 00:57:27.840
Capstone Coach 1: notice here that i'm using regularization on.

363
00:57:28.200 --> 00:57:33.870
Capstone Coach 1: The first two hidden layers and not on the output that's this is more, this is more common.

364
00:57:35.850 --> 00:57:55.140
Capstone Coach 1: Okay, all right so let's turn this one on if you could make that a true up front, I would make it a false on the, on the other ones, so you don't rerun that but make it a true here and then we're just going to hit run again, and it should then crank on through and.

365
00:57:56.400 --> 00:58:08.340
Capstone Coach 1: This one this is oh yeah, this is the one we're looking at the number of parameters here five over half a million right, so this actually takes some time to run.

366
00:58:09.510 --> 00:58:19.020
Capstone Coach 1: i'm thinking like 20 or 30 minutes or something like that so here's what i'm going to do it's now seven o'clock I timed this perfectly let's.

367
00:58:20.250 --> 00:58:26.970
Capstone Coach 1: let's take a five or 10 minute break to get something to drink and come on back and i'll show you what what happened here.

368
00:58:29.280 --> 00:58:31.350
Capstone Coach 1: it's seven o'clock did I say five o'clock.

369
00:58:33.450 --> 00:58:36.180
Capstone Coach 1: It says, I must have said five right okay.

370
00:58:36.270 --> 00:58:37.590
Karen Fireman: it's actually what you meant.

371
00:58:38.580 --> 00:58:41.970
Capstone Coach 1: it's seven o'clock okay so.

372
00:58:42.180 --> 00:58:43.140
Karen Fireman: Then come back when.

373
00:58:43.410 --> 00:58:44.640
Capstone Coach 1: It said okay break time.

374
00:58:46.140 --> 00:58:47.130
Karen Fireman: What time are we coming back.

375
00:58:47.910 --> 00:58:50.760
Karen Fireman: 710 perfect Thank you so okay.

376
00:58:53.700 --> 00:59:06.810
Capstone Coach 1: My program your program probably still running so i'm going to hit the red button, because I have the output here and save it on a file, so I could open it up in my.

377
00:59:08.550 --> 00:59:09.180
Capstone Coach 1: browser.

378
00:59:10.740 --> 00:59:17.880
Capstone Coach 1: This is, this is what it looks like so we have 540 8978 weights.

379
00:59:19.140 --> 00:59:19.800
Capstone Coach 1: The.

380
00:59:22.140 --> 00:59:27.660
Capstone Coach 1: we're running this for 4020 bucks and the size of 128 I think it is.

381
00:59:30.570 --> 00:59:41.010
Capstone Coach 1: And the graph actually looks pretty good, you can see the the red dots the training data are converging to a small number the blue.

382
00:59:41.490 --> 01:00:07.680
Capstone Coach 1: The validation data, however, not there plateauing and also small number, so, if you look at the loss it's going to come out to be about 1% and 4%, this is the the Cross entropy error loss, the actual misclassification rate for the train date is 21 out of 60,000.035%.

383
01:00:09.180 --> 01:00:19.560
Capstone Coach 1: I can get I think I can get this to zero with by making one modification, by the way, I think the training data, you can get this to fit a perfectly here.

384
01:00:20.070 --> 01:00:29.580
Capstone Coach 1: What I what I believe, as I recall, you can try this at some point, if you turn on the regularization it's just if you look at line number 247.

385
01:00:30.810 --> 01:00:44.580
Capstone Coach 1: I think that's commented out right now, and also line number 254 he commented out those are the regularization the l one rig utilization for the first and the second convolutions.

386
01:00:45.960 --> 01:00:50.430
Capstone Coach 1: And I believe, if you turn that on you'll get a better fit.

387
01:00:52.800 --> 01:00:56.220
Capstone Coach 1: But, at any rate, this is, this is not that bad really.

388
01:00:57.360 --> 01:01:00.990
Capstone Coach 1: The only thing that's clear here's we do have some level of overfitting.

389
01:01:02.130 --> 01:01:27.000
Capstone Coach 1: The point oh three, five, is more than 10 times less than Point six eight for the validation data, so if I was reporting this, I would say we're doing very well, here we have over 99% correct classification, we are misclassified about this point six 8% of the digits.

390
01:01:30.570 --> 01:01:38.910
Capstone Coach 1: Now this is done using just the convolution followed by a traditional nlp network right.

391
01:01:41.820 --> 01:01:56.370
Capstone Coach 1: Now now we're going to go to rnn and so i'm going to put this off the screen for a moment here, and you may be still running that that's okay let's let's change this from true to false.

392
01:01:59.610 --> 01:02:01.530
Capstone Coach 1: we're going to go to the number three.

393
01:02:02.940 --> 01:02:13.110
Capstone Coach 1: And number three is the recurrent neural network let's see it says fault right now let's make that a true don't don't run it just yet, but.

394
01:02:13.620 --> 01:02:23.670
Capstone Coach 1: We will, in just a moment let's talk about what's going on in here, first of all, there is no convolution so we're going to this, this is similar to.

395
01:02:24.180 --> 01:02:38.190
Capstone Coach 1: The nlp network, there is no smoothing of the images or anything like that, however, instead of bringing the images in as a three dimensional row width and height.

396
01:02:39.660 --> 01:02:53.910
Capstone Coach 1: Sorry row and vector it's basically in the LP solution, the data come in as a single long vector 784 I think it is pixels in one long vector.

397
01:02:54.570 --> 01:03:11.460
Capstone Coach 1: In this case, in the rnn case it accepts it in as two dimensional and and what are in really think so the way rnn is going to act here it thinks that the width dimension is actually time.

398
01:03:13.350 --> 01:03:14.160
Capstone Coach 1: So.

399
01:03:15.240 --> 01:03:23.070
Capstone Coach 1: This is not normally done with images, but we're going to do with anyway to illustrate how rnn works.

400
01:03:24.300 --> 01:03:36.030
Capstone Coach 1: It accepts two dimensional data, where the first dimension is time and the or step and or sequence, and the second dimension is the the actual data itself.

401
01:03:36.810 --> 01:03:49.230
Capstone Coach 1: It could be one column, it could be multiple columns in this case it's the height so we're using the width here as a time we're saying the first.

402
01:03:50.430 --> 01:03:58.560
Capstone Coach 1: column is with one and so that's going to come in as an image one, and then we have the second one, the third and fourth and so on.

403
01:03:59.130 --> 01:04:04.860
Capstone Coach 1: Now you would expect some cereal correlation here of the Jason pixels right, so if they were.

404
01:04:05.550 --> 01:04:17.460
Capstone Coach 1: The line number two would be Jason line number three and you would expect those two lines would maybe have similarities, especially if they're crossing through you know white layer or something like that.

405
01:04:18.810 --> 01:04:25.740
Capstone Coach 1: So your image shape is important here in nlp we wouldn't have this we just have a single number.

406
01:04:26.640 --> 01:04:39.060
Capstone Coach 1: notice, though, then the first layer, the first hidden layer instead of saying dense instead of saying dense for the layer description, it says simple are in n.

407
01:04:40.020 --> 01:04:57.930
Capstone Coach 1: Now it turns out that there are several different kinds of rnn networks, the simple rnn is the easiest one to describe it, this is a network, where all and this particular case, the There we have one hidden layer, the first hidden layer

408
01:04:58.980 --> 01:05:12.690
Capstone Coach 1: We accept inputs into that hidden layer we, as we believe those inputs have two dimensions to them, one of them being the time number one number two number three and then the other one being the actual column of.

409
01:05:13.320 --> 01:05:27.690
Capstone Coach 1: That were absent and forecasting now what happens here is that the image shape then comes in, as an argument argument, excuse me, it is a required argument, right here this guy.

410
01:05:28.260 --> 01:05:35.850
Capstone Coach 1: And then the units here is the number of precepts ron's in the hidden layer up here I just said it equal to 256.

411
01:05:36.780 --> 01:05:48.660
Capstone Coach 1: So i'm really kind of going gung Ho on this one, putting in lots of perceptions here we're images, this is not unusual because we do have 789 I think it is pixels.

412
01:05:49.230 --> 01:06:05.670
Capstone Coach 1: And so I might even use more than 256 The more I use the slower things are going to be, but this is not such so bad, the images are small drop out is an alternative to the.

413
01:06:07.110 --> 01:06:22.350
Capstone Coach 1: regularization you assign a proportion to the dropout in this case it's 2% 20% point two, and they basically it says the the 20% of the weights, are going to be pushed out to zero.

414
01:06:23.700 --> 01:06:35.340
Capstone Coach 1: And i'm assuming basically that 20% of the weights have little or no effect so we'll take the smallest of the of the weights and push them down to zero, and work on the other ones okay so let's drop out.

415
01:06:36.600 --> 01:06:37.170
Capstone Coach 1: The.

416
01:06:38.370 --> 01:06:46.620
Capstone Coach 1: OK so it's very easy, then the dropout is not required, you might replace it with the regularization or nothing at all.

417
01:06:47.580 --> 01:06:59.820
Capstone Coach 1: In the structure than have this little hidden layer is identical to the hidden layer that we had for dense or nlp, the only thing that's different here is the input shape.

418
01:07:00.930 --> 01:07:10.260
Capstone Coach 1: As as as a tool to numbers in that tool and the in an nlp case it's just a single number input dimension.

419
01:07:11.280 --> 01:07:11.790
Capstone Coach 1: So.

420
01:07:13.260 --> 01:07:31.740
Capstone Coach 1: let's see and then we have a dense outer output layer, and you can see here the function is soft Max again because the actual number of outputs is the shape of the the training vector there that's it's it's 10 I could have put them in there.

421
01:07:34.170 --> 01:07:37.890
Capstone Coach 1: And then you see the structure for the optimization routines again.

422
01:07:39.540 --> 01:07:40.740
Capstone Coach 1: you'll want to look at that.

423
01:07:41.910 --> 01:07:51.720
Capstone Coach 1: And everything else is the same so let's get this Oh, by the way, so what this is our in in actually do what it does, is it.

424
01:07:52.350 --> 01:08:04.680
Capstone Coach 1: Not only does he do you have the precept ron's there and it has activation on those precept ron's so all the inputs are coming into the perceptions and then they are creating an output function through activation.

425
01:08:05.370 --> 01:08:14.400
Capstone Coach 1: There is a bias term and their weights involved in that what happens is the add the value of the APP to activation form from the last observation.

426
01:08:14.850 --> 01:08:25.530
Capstone Coach 1: The previous observation comes is fed into the neural into the input layer here, each time, so, in addition to have it all the inputs are coming in.

427
01:08:26.250 --> 01:08:36.450
Capstone Coach 1: My also get the activation from the last observation that came out so that is it's assuming then that the data are in sequence their order in time.

428
01:08:37.020 --> 01:08:44.850
Capstone Coach 1: The first, the first observation is the first one, in time, the second, the second one, in time, and so forth, and so on there's an ordering here.

429
01:08:45.600 --> 01:08:58.200
Capstone Coach 1: So, in the case of the stock prices, for example, you have to actually look at the order that the prices come in and you want to set them up, so that the the the.

430
01:08:58.740 --> 01:09:05.970
Capstone Coach 1: latest one or the oldest one comes into the network, first, followed by the next oldest and so forth, and so on.

431
01:09:06.840 --> 01:09:17.490
Capstone Coach 1: You could reverse that but ordinarily your want to make your forecasts going forward, and so you put the oldest one in first and then the next and so forth, and so on.

432
01:09:18.420 --> 01:09:33.420
Capstone Coach 1: so simple rnn assumes that there isn't a time order or sequence order to things in the case of an image it's just the sequences Well, this is the first word, the first row, the second row, the third row and so forth in this pixels.

433
01:09:34.170 --> 01:09:41.400
Capstone Coach 1: let's run this and see what how long it takes so i'm gonna hit get word meant that I set this to true yeah I did.

434
01:09:45.480 --> 01:09:53.280
Capstone Coach 1: right here mm hmm line number 298 should be true, and you can see the the actual table that comes out is super simple.

435
01:09:54.600 --> 01:10:07.860
Capstone Coach 1: You can see it on my right hand lower right hand side 256% ron's basically units, the number of parameters 75,000 instead of the half a million.

436
01:10:08.460 --> 01:10:26.220
Capstone Coach 1: The nlp approach uses a half a million parameters or weights so But even with that it's still not not zipping along right second about 1917 seconds per epoch on my computer.

437
01:10:26.790 --> 01:10:38.280
Capstone Coach 1: You can see the the validation accuracy is already at 91% the accuracy for the training data is at 90 over 90% right now so it's clicking along.

438
01:10:39.900 --> 01:10:49.980
Capstone Coach 1: sorta let me show you the output that you're going to see i'm going to flip over to the sky, right here so.

439
01:10:52.410 --> 01:11:01.500
Capstone Coach 1: This is what I was referring to there's only 75,000 weights in this in this rnn network, so you have fewer things that are being estimated.

440
01:11:02.550 --> 01:11:03.990
Capstone Coach 1: Maybe that helps a little bit.

441
01:11:05.040 --> 01:11:17.760
Capstone Coach 1: And i'm going to go fast forward here to show you what's what we should come out now of course you're you're on a different type of computer your numbers may not will will very much be different.

442
01:11:18.840 --> 01:11:20.550
Capstone Coach 1: Hopefully they'll be close to this.

443
01:11:21.750 --> 01:11:26.580
Capstone Coach 1: The the validation accuracy that i'm getting here is 97.9%.

444
01:11:27.870 --> 01:11:39.930
Capstone Coach 1: And you could see the graph showing the training deed and the melody and the validation data what's there are a little bit nicer they kind of like agree so I feel less.

445
01:11:40.410 --> 01:11:52.770
Capstone Coach 1: put off by overfitting here I don't see that coming in very loudly, but if we look at this Oh, the bottom number here is the loss right so.

446
01:11:54.090 --> 01:12:03.240
Capstone Coach 1: The misclassification rate on the training data is 1.5% and then on the validation data 2%.

447
01:12:04.260 --> 01:12:16.800
Capstone Coach 1: It takes it took on my computer it took about 4.3 minutes to run this through now, this is not nearly as the forecasts are not as accurate as we got on the previous.

448
01:12:17.940 --> 01:12:24.270
Capstone Coach 1: Network, this is the previous network, right here, this is using convolution.

449
01:12:25.470 --> 01:12:45.180
Capstone Coach 1: So we with convolution we've got 21 out of 60,000 misclassified and then 68 I total misclassification 7888 89 out of 70,000 that's of course that's pretty good.

450
01:12:46.590 --> 01:12:59.160
Capstone Coach 1: But you can still see some people might say, well it's overfitting personally I like that model convolution model rnn well, it does respectable job.

451
01:13:00.270 --> 01:13:06.570
Capstone Coach 1: But it's not doing as well, what I like about the rnn is that you have less overfitting.

452
01:13:07.470 --> 01:13:18.480
Capstone Coach 1: Going on so there's more agreement between the training solution and the validation shoes solution, but both of them are not as good as convolution right.

453
01:13:19.200 --> 01:13:38.700
Capstone Coach 1: These are images combination is made for images, maybe we should do that Okay, so what i'm going to do on my computer is you can keep this running for your seat, will have a record, if you want, but for me i'm going to move ahead and i'm going to change that true to a false.

454
01:13:42.570 --> 01:13:48.720
Capstone Coach 1: This is line number 298 So this is the line that says we're going run running the rnn.

455
01:13:52.950 --> 01:13:55.410
Capstone Coach 1: huh okay there we go all right.

456
01:13:56.460 --> 01:13:57.600
Capstone Coach 1: A line to 98.

457
01:13:59.820 --> 01:14:02.970
Capstone Coach 1: let's scoot down to line number 344.

458
01:14:04.410 --> 01:14:06.450
Capstone Coach 1: Line 344 well.

459
01:14:07.650 --> 01:14:25.830
Capstone Coach 1: Why not put the two of them together Why not use convolution with rnn maybe we can get a better solution convolution that seems as necessary here in order to smooth out the images and get rid of some noise that might be in the images and make them more manageable smaller.

460
01:14:26.970 --> 01:14:28.200
Capstone Coach 1: So what I did was.

461
01:14:30.030 --> 01:14:47.460
Capstone Coach 1: And this solution you'll see that it has the convolution here and I tried to make this exactly like the other one, so I have two major convolution filters both one was 64 and both of them actually was 64.

462
01:14:49.470 --> 01:14:52.830
Capstone Coach 1: nodes or preceptor ons and I think that's what.

463
01:14:53.880 --> 01:14:56.910
Capstone Coach 1: I used in the convolution case, let me go back and see them.

464
01:14:58.650 --> 01:15:10.410
Capstone Coach 1: Right in the convolution solution, you see, I just had to to completion layers 64 and 64 and then this was followed by the nlp layers.

465
01:15:11.100 --> 01:15:23.280
Capstone Coach 1: One with 300% trans and then the second hidden layer with 100 and of course we flattened these before we went into this now what i'm doing i'm using the same convolution.

466
01:15:25.050 --> 01:15:29.010
Capstone Coach 1: right here this code is exactly the same right there.

467
01:15:30.390 --> 01:15:31.020
Capstone Coach 1: and

468
01:15:39.030 --> 01:15:39.270
Capstone Coach 1: Oh.

469
01:15:42.000 --> 01:15:42.570
Capstone Coach 1: Right.

470
01:15:43.740 --> 01:15:49.170
Capstone Coach 1: The long lines 351 two 365 or basically the same.

471
01:15:50.880 --> 01:15:59.880
Capstone Coach 1: And like I probably should have put like us little space after that so different so these lines that i'm highlighting right now, these are the convolution lines.

472
01:16:00.240 --> 01:16:12.060
Capstone Coach 1: And these are coming exactly out there coming out of the other solutions CNN solution exactly the same, then what I did was instead of using pls.

473
01:16:13.230 --> 01:16:31.590
Capstone Coach 1: So peel nlp rather yay Mel P, I had a neural network here with 300 and 100% ron's now i'm using rnn with 50 256 and 10 just just 251 layer with 256.

474
01:16:32.610 --> 01:16:34.440
Capstone Coach 1: preceptor on set of 300.

475
01:16:35.700 --> 01:16:48.960
Capstone Coach 1: notice what I had to do here and and the other solution, I had to flatten the image well if I if you flatten the image shake it just make puts it into one long string of numbers one vector.

476
01:16:50.190 --> 01:16:54.120
Capstone Coach 1: So every pixel is just one one after the other, from the image.

477
01:16:55.260 --> 01:17:14.700
Capstone Coach 1: But rnn doesn't want that it wants two dimensional data it wants the first column to represent the time sequence, and the second column to represent the actual data pixels so what I did was I reshaped into 25 because the image, the image that's coming out of the other.

478
01:17:15.930 --> 01:17:20.400
Capstone Coach 1: The other solution, let me show you what that looks like I can bring that up here.

479
01:17:22.050 --> 01:17:25.110
Capstone Coach 1: So if you looked at the convolution solution which is.

480
01:17:29.730 --> 01:17:33.870
Capstone Coach 1: right here, you see that just before it gets flattened.

481
01:17:35.760 --> 01:17:50.610
Capstone Coach 1: The image, the dimension or the shape of the image is five by five by 64 so that says that we went from 28 by 28 and we've now shrink the image down to five by five.

482
01:17:51.240 --> 01:17:56.310
Capstone Coach 1: Each one of these convolution steps narrows or shrinks the image, a little bit.

483
01:17:56.850 --> 01:18:05.400
Capstone Coach 1: The first 126 Bytes 26 what we're doing is we're getting rid of the two pixels on the edge, so that the one pixel on the edge, so one on.

484
01:18:05.760 --> 01:18:18.570
Capstone Coach 1: One of the left, one on the right that's two pixels so as we went from 28 to 2626 and then we use Max pooling to divide it and a half, two and 26 gives us 13.

485
01:18:19.710 --> 01:18:29.400
Capstone Coach 1: And then we did another convolution to take two more pixels away and smooth it, and then we did it divided 11 by to get 10 five.

486
01:18:30.450 --> 01:18:35.910
Capstone Coach 1: And so, then the last that's where we ended, we have a five by five.

487
01:18:37.380 --> 01:18:45.630
Capstone Coach 1: convoluted image smooth image five by five pixels and we have 64 of those for each image.

488
01:18:46.710 --> 01:19:11.880
Capstone Coach 1: So what I did we if you're using them LP, then you have to flatten those into 16,000 1600 pixels so five times five and 64 gets to 1600 1600 pixels and then that is being fed into the dense him LP neural network, you see down here at the bottom to hidden layers 301 hundred.

489
01:19:13.320 --> 01:19:17.910
Capstone Coach 1: In the case, what we're working on now where we're putting convolution together again.

490
01:19:24.360 --> 01:19:35.100
Capstone Coach 1: This is set right here, so you can see that the convolution we end up with five by five by 64 and I need to shrink that to two dimensions.

491
01:19:35.670 --> 01:19:44.940
Capstone Coach 1: So I took the first two dimensions and put those together that will be a total of 25 five times 525 pixels and then leave the 64 alone.

492
01:19:45.420 --> 01:19:54.360
Capstone Coach 1: So i'm using the height than the width of the image to represent the sequence number so that would be like the first row, the first column.

493
01:19:55.050 --> 01:20:07.620
Capstone Coach 1: second row first, second, third row so forth, and so on, so these are ordered, in a sense, first by by the row number and then it's then by the height through the new column number.

494
01:20:09.570 --> 01:20:23.970
Capstone Coach 1: We could have done this, the other way or ground I could have made 60 these numbers of sequence number and then these is maybe a position number, but I think this is the one the way that makes sense for an image anyway.

495
01:20:25.080 --> 01:20:36.750
Capstone Coach 1: So it says reshape i'm using a new command here the reshaped command and charisse So yes, there is a charisse reshape command and, if you look at the code.

496
01:20:40.410 --> 01:20:49.410
Capstone Coach 1: right here it's super simple look at that right there reshape so that is instead of flatten using that that layers flatten.

497
01:20:50.130 --> 01:21:04.830
Capstone Coach 1: Instead of using that we're using the reshape command from charisse and you just give it two numbers, they have to be calculated, they have to be exact, if they're not you'll get an error message so five times five is 25 and 64 was the last digit.

498
01:21:06.120 --> 01:21:14.280
Capstone Coach 1: And then, then we go from there, we dropped directly into the rnn it's going to be happy to see that this is two dimensional.

499
01:21:15.330 --> 01:21:15.990
Capstone Coach 1: and

500
01:21:17.520 --> 01:21:22.830
Capstone Coach 1: It actually will work just fine so let's see the finish.

501
01:21:24.960 --> 01:21:26.640
Capstone Coach 1: Well, it looks like something happened.

502
01:21:29.430 --> 01:21:31.440
Capstone Coach 1: yep so.

503
01:21:36.000 --> 01:21:41.730
Capstone Coach 1: screw screw screw Sean down here a little bit and we'll see that.

504
01:21:43.980 --> 01:21:45.240
Capstone Coach 1: It did converge.

505
01:21:47.340 --> 01:21:56.790
Capstone Coach 1: that the actual solution of the number of miss classifications is not as low as when we just use convolution followed by nlp.

506
01:21:57.630 --> 01:22:10.170
Capstone Coach 1: You might remember that was a total of 90 I think miss classifications here we're getting more like 150 little bit more than 150 hundred and 56 I guess or.

507
01:22:10.680 --> 01:22:24.900
Capstone Coach 1: Something like that, and so, if the misclassification is here are a little bit heavier a little bit worse than the pure convolution nlp solution, but what I do like about this is.

508
01:22:26.040 --> 01:22:37.230
Capstone Coach 1: Well it's a definition or a reasonable amount of time 13.5 minutes and the difference between training and validation is but it's still too large.

509
01:22:37.590 --> 01:22:45.360
Capstone Coach 1: Of its large of a difference, so we might be able to adjust the solution, a bit to get those closer together and.

510
01:22:46.200 --> 01:22:55.890
Capstone Coach 1: What you know what you might do here is, you could put another layer in between the the rnn layer and then, and then the output layer here.

511
01:22:56.460 --> 01:23:07.380
Capstone Coach 1: You can add another hidden layer as many as you want actually you can increase the number of preset trans that are in the rnn layer notice that we're all the weights are located.

512
01:23:08.370 --> 01:23:20.760
Capstone Coach 1: Before the weights were up here in the convolution go within that area but look at this right here in that first layer there is so there are a lot of weeks there, and the reason for that is because.

513
01:23:21.360 --> 01:23:33.810
Capstone Coach 1: In the reshape case, we now have 25 and 64 at 25 times 64 times 256 is a big number so that's where this 82,000 comes from still have quite a few.

514
01:23:34.350 --> 01:23:51.000
Capstone Coach 1: inputs that are coming into this hidden layer because of that we might want to actually try, increasing the 256 to some other value and maybe putting in a hidden layer In between there let's see how we do that, so if we go back to the code.

515
01:23:52.350 --> 01:23:53.160
Capstone Coach 1: right here.

516
01:23:54.450 --> 01:24:06.720
Capstone Coach 1: And i'm so i'm looking at line number two 368 369 that's that's the simple rnn line so instead of a 256.

517
01:24:07.740 --> 01:24:11.070
Capstone Coach 1: Well let's try 512 let's double it.

518
01:24:12.330 --> 01:24:30.750
Capstone Coach 1: And then, what we're going to do is we're going to put a dense layer as another layer right in the middle right between these two, and for that I mean there are lots of ways, you could you could actually do this and i'm just going to take the output layer you see here is almost.

519
01:24:31.770 --> 01:24:41.160
Capstone Coach 1: The format is almost what we want, so i'm going to copy that the output layer and i'm going to stick it right in between, and like that.

520
01:24:43.770 --> 01:24:48.030
Capstone Coach 1: And instead of naming it output i'm going to name it hidden layer two.

521
01:24:57.810 --> 01:24:58.380
Capstone Coach 1: like that.

522
01:24:59.970 --> 01:25:08.820
Capstone Coach 1: Now activation soft Max no way that's only the soft Max activation is only for output we're going to use Arielle you here.

523
01:25:11.250 --> 01:25:11.880
Capstone Coach 1: and

524
01:25:12.990 --> 01:25:17.070
Capstone Coach 1: For the simple layer we didn't designate the activation I think.

525
01:25:18.390 --> 01:25:25.260
Capstone Coach 1: I think you can do that with RNA and you can see activation equals two or email you.

526
01:25:26.820 --> 01:25:27.510
Capstone Coach 1: like that.

527
01:25:28.950 --> 01:25:35.460
Capstone Coach 1: This is the activation that's going to be applied to the neurons that are in this 512 of them.

528
01:25:36.240 --> 01:25:44.430
Capstone Coach 1: So look quite a few of them, so we want to use an activation it's not too time consuming this is fast one now, the number of neurons 10.

529
01:25:45.090 --> 01:25:56.910
Capstone Coach 1: We ought to crank this up a little bit because we have 512 coming into this this part of the network, as we have coming up here, so I would say let's put 100 in here.

530
01:25:58.290 --> 01:26:03.480
Capstone Coach 1: This is something, of course, you could play with and and see what see what happens.

531
01:26:04.770 --> 01:26:20.850
Capstone Coach 1: We don't have regularization turned on in the convolution area let's do that let's let's turn on the regularization up there, like that let's copy you that Colonel regularization and let's put that.

532
01:26:22.710 --> 01:26:25.140
Capstone Coach 1: In both of our hidden layers.

533
01:26:26.790 --> 01:26:33.990
Capstone Coach 1: So i'm going to put that under the 512 and under writing you know, right after the activation like that.

534
01:26:38.040 --> 01:26:40.500
Capstone Coach 1: Now, if you missed any of this it's still going to run.

535
01:26:42.420 --> 01:26:58.200
Capstone Coach 1: As long as you don't have the comma day and missing at the end of the end of one of these or something like that it's still going to run and if you do have it Oh, I have an error message here uh huh I see you see the Little Red dot here line number 374.

536
01:26:59.400 --> 01:27:04.440
Capstone Coach 1: that's because I have regularization set to none so I need to take that out.

537
01:27:05.490 --> 01:27:08.160
Capstone Coach 1: I need to take that out like that.

538
01:27:10.290 --> 01:27:13.530
Capstone Coach 1: And we should be good to go then well.

539
01:27:16.830 --> 01:27:21.840
Capstone Coach 1: No let's i'm going to make this true so we run this.

540
01:27:23.040 --> 01:27:23.760
Capstone Coach 1: like that.

541
01:27:25.860 --> 01:27:26.460
Capstone Coach 1: and

542
01:27:30.930 --> 01:27:34.350
Capstone Coach 1: I see one of the things here is the sizes equal to 128.

543
01:27:35.490 --> 01:27:48.090
Capstone Coach 1: week you, you know changing this can also affect the quality of the solution if you make it a smaller number not ridiculously small like a two, but if you make this may be 6432 or something like that.

544
01:27:49.170 --> 01:27:58.950
Capstone Coach 1: You can actually improve the the accuracy of the solution now if i'm looking at the chart on the right hand side you see how the the the points.

545
01:27:59.280 --> 01:28:11.850
Capstone Coach 1: Are smooth I don't have them jumping up and around that kind of thing well if they're smooth you're probably pretty good with this number, but if they were jumping around a lot, I might make this a smaller number.

546
01:28:12.390 --> 01:28:19.290
Capstone Coach 1: And then hopefully that would smooth things out, so the converge to to a good solution.

547
01:28:20.460 --> 01:28:26.880
Capstone Coach 1: All right, i'm going to turn this on just click run here and see what happens so it's running.

548
01:28:27.780 --> 01:28:38.430
Capstone Coach 1: I with those changes with i've added that middle layer of 100 You see, I now have 380 5000 parameters so.

549
01:28:39.150 --> 01:28:50.520
Capstone Coach 1: That was because we made two big changes the biggest change instead of 256 neurons we're now five that we're now at 512 neurons on hidden layer number one.

550
01:28:51.150 --> 01:29:08.880
Capstone Coach 1: And hidden layer number two is a new layer that brings in an additional 51,000 weights and so you know, basically, we have a heavier network here, and in fact it's still sitting on epoch number one, I may have messed things up here really bad.

551
01:29:11.520 --> 01:29:15.450
Capstone Coach 1: we'll see if this even gets off of the park number one right.

552
01:29:26.700 --> 01:29:33.180
Capstone Coach 1: Unfortunately I don't have the I don't have a a image of what's going to happen here, this is running.

553
01:29:35.700 --> 01:29:40.290
Capstone Coach 1: we'll get an idea here in a second I thought it would be like one minute, maybe.

554
01:29:48.270 --> 01:29:58.080
Capstone Coach 1: Up I got well Okay, so it looks it looks like it's a minute and a half for one epoch so that means with 20 bucks it's going to take about 30 minutes.

555
01:29:58.680 --> 01:30:12.420
Capstone Coach 1: i'm going to let this run and while it's running we're going to talk about some other things we're going to talk go over to the PowerPoint slide here, so this is basically what we just did we.

556
01:30:13.560 --> 01:30:23.700
Capstone Coach 1: were looking at the nlp model, the CNN, for this is for images and then the Orient in this was new number three is new and number four was there.

557
01:30:24.720 --> 01:30:35.940
Capstone Coach 1: and hopefully got some ideas there now let's go to the next slide and I like Oh, I need to turn this on gets you so you can download it.

558
01:30:47.550 --> 01:30:54.660
Capstone Coach 1: there's another program you want you'll want to download while this other programs running so if you go in the week 13 folder.

559
01:30:56.250 --> 01:30:57.120
Capstone Coach 1: and

560
01:30:59.340 --> 01:31:03.570
Capstone Coach 1: Now you can see a program and it's called time series analysis.

561
01:31:04.620 --> 01:31:14.850
Capstone Coach 1: nlp and rnn So what we did with a with an image we're going to do it again if you Fuchs your patient here with me.

562
01:31:16.020 --> 01:31:19.800
Capstone Coach 1: Except now we're going to look at a time series we're going to look at the stock prices.

563
01:31:20.850 --> 01:31:22.440
Capstone Coach 1: So those of you that have.

564
01:31:23.490 --> 01:31:26.760
Capstone Coach 1: Traditional time series data will want to see how the system.

565
01:31:28.770 --> 01:31:30.840
Capstone Coach 1: turns out to be pretty easy.

566
01:31:32.700 --> 01:31:40.860
Capstone Coach 1: In some some respects easier than what we just did with the images, because you, you have images you like you start with four dimensions right.

567
01:31:41.460 --> 01:31:53.580
Capstone Coach 1: Well, with time series data you usually have the time column, and you have the image, you know the y value or variable maybe the X variables and that's it you don't have four dimensions, you don't have the Channel number.

568
01:31:54.300 --> 01:32:07.770
Capstone Coach 1: Red blue green that kind of thing, so please go ahead and download this file time series analysis nlp or an ad copy why out of the week number 13 folder.

569
01:32:08.640 --> 01:32:23.730
Capstone Coach 1: And then, if you can, this is obviously going to take about 30 minutes, so if you could go ahead and open up this other file, this is called time series analysis nlp.

570
01:32:24.990 --> 01:32:30.180
Capstone Coach 1: And i'm going to make it larger now, so we can see what a little bit better what's going on.

571
01:32:34.170 --> 01:32:42.000
Capstone Coach 1: The nice thing one of the nice things about spider is you can be working on code, while other code is running right so.

572
01:32:43.740 --> 01:32:45.720
Capstone Coach 1: And that's what we'll do here for a while.

573
01:32:53.190 --> 01:32:57.210
Capstone Coach 1: I don't know whether this new solution is going to be better or worse than the old one.

574
01:32:58.050 --> 01:33:12.720
Kilani, Shadi: Jones programming excellent, but in the past i've opened a second console So if I have code running, I will just open a new console and then kind of do some more Code and the second console I don't have that connection issues.

575
01:33:14.520 --> 01:33:17.160
Capstone Coach 1: I mean it is so you're saying you can run to Casa.

576
01:33:17.430 --> 01:33:19.470
Capstone Coach 1: Two programs, at the same time and spider.

577
01:33:21.750 --> 01:33:23.010
Capstone Coach 1: Two separate programs.

578
01:33:23.100 --> 01:33:25.140
Kilani, Shadi: yeah just right click and then you can set.

579
01:33:25.140 --> 01:33:25.980
Capstone Coach 1: Up yeah.

580
01:33:27.300 --> 01:33:28.170
Kilani, Shadi: yeah if you do.

581
01:33:28.230 --> 01:33:31.890
Capstone Coach 1: come up here and say new special console or new console right.

582
01:33:36.330 --> 01:33:39.750
Kilani, Shadi: I just did do new consultants the default One the one on the top.

583
01:33:53.190 --> 01:33:54.690
Capstone Coach 1: yeah I think it was right here.

584
01:33:55.650 --> 01:33:59.730
Kilani, Shadi: You had it So yes, there you go it's it's the first choice Newcastle.

585
01:34:00.450 --> 01:34:01.260
Capstone Coach 1: not like that.

586
01:34:02.460 --> 01:34:04.800
Kilani, Shadi: that's a special constant but I don't know what the difference.

587
01:34:05.880 --> 01:34:09.510
Capstone Coach 1: was actually created another instance of Python right so.

588
01:34:09.810 --> 01:34:22.350
Capstone Coach 1: So this is the in that case, we have the sky still running and the a Python one window and now i've got another one and, but this one is says its.

589
01:34:24.510 --> 01:34:25.680
Capstone Coach 1: peak pie lab.

590
01:34:25.800 --> 01:34:27.870
Kilani, Shadi: it's because you selected special console.

591
01:34:28.680 --> 01:34:33.690
Kilani, Shadi: So just just new Castle new default is the top choice.

592
01:34:37.650 --> 01:34:38.790
Capstone Coach 1: that's, so this is it.

593
01:34:38.850 --> 01:34:39.510
Capstone Coach 1: yeah this is.

594
01:34:40.110 --> 01:34:42.390
Karen Fireman: Where are you pulling this down from I can't wait till.

595
01:34:43.890 --> 01:34:45.150
Capstone Coach 1: Karen is what is what.

596
01:34:45.600 --> 01:34:50.100
Karen Fireman: I was wondering how you're pulling down the new console somehow when I did, and I just eliminated my console.

597
01:34:50.310 --> 01:34:54.840
Capstone Coach 1: Well, what I did was here I wasn't counseled console number one right.

598
01:34:55.170 --> 01:34:58.950
Karen Fireman: yeah you're right right click right click on the tab.

599
01:34:59.550 --> 01:35:00.570
Karen Fireman: Oh, oh.

600
01:35:00.990 --> 01:35:02.010
Capstone Coach 1: And then select.

601
01:35:02.130 --> 01:35:04.800
Capstone Coach 1: select the first item, new console.

602
01:35:05.910 --> 01:35:06.480
Karen Fireman: Okay.

603
01:35:06.540 --> 01:35:07.080
Karen Fireman: And then, what.

604
01:35:07.470 --> 01:35:09.870
Capstone Coach 1: What it does is it opens up a new tab.

605
01:35:10.710 --> 01:35:11.610
Capstone Coach 1: Which is empty.

606
01:35:12.510 --> 01:35:29.220
Capstone Coach 1: And, and this is actually another incident instance of Python running so you can have two programs running at the same time, so that other program that we're still there it's still running and then the first window, you can see it here.

607
01:35:29.880 --> 01:35:32.250
Capstone Coach 1: Yes, so it's like running in the background.

608
01:35:32.520 --> 01:35:33.990
Karen Fireman: got it okay thanks.

609
01:35:34.260 --> 01:35:35.970
Capstone Coach 1: But no, I can click here.

610
01:35:36.720 --> 01:35:39.660
Capstone Coach 1: And I can get a new one, and I can run another Program.

611
01:35:42.000 --> 01:35:42.480
Capstone Coach 1: Okay.

612
01:35:43.050 --> 01:35:45.300
Capstone Coach 1: So this this way I can find Thank you.

613
01:35:45.360 --> 01:35:48.630
Capstone Coach 1: This way, I can still get to see what what that other Program.

614
01:35:49.800 --> 01:35:52.680
Capstone Coach 1: converges to you know what kind of solution it comes up.

615
01:35:52.680 --> 01:35:55.290
Capstone Coach 1: With and here we can go ahead and.

616
01:35:55.560 --> 01:35:56.130
Capstone Coach 1: Work on.

617
01:35:58.170 --> 01:35:59.580
Capstone Coach 1: You know timeshares.

618
01:36:01.200 --> 01:36:01.680
Capstone Coach 1: Okay.

619
01:36:04.680 --> 01:36:05.160
Capstone Coach 1: OK.

620
01:36:12.180 --> 01:36:12.660
Capstone Coach 1: OK.

621
01:36:14.310 --> 01:36:31.260
Capstone Coach 1: So the first thing I want to point out here is line number 30 that is a new import that we haven't seen before, and you notice it says tensorflow charisse callbacks the callback is a special function and cares that lets you.

622
01:36:32.310 --> 01:36:49.440
Capstone Coach 1: control how it handles optimization how it handles the conversions and there's a routine called early stopping you'll see I said import early stopping here, and what that does is that lets you give instructions to cares about.

623
01:36:50.460 --> 01:36:58.620
Capstone Coach 1: when things are going slow or the it's converging or has converged to the right solution the give it permission to stop.

624
01:36:59.100 --> 01:37:10.290
Capstone Coach 1: Otherwise it's going to go all you if you asked for 100 ad packs it's going to run all 100 ad packs, but if you set up an early stopping condition, it may stop at a pocket number 20.

625
01:37:11.580 --> 01:37:27.300
Capstone Coach 1: It may stop there, early and you'll see an example here in a little bit of where i'm using this you can one of the conditions, you can set up for instances says that if the loss function doesn't change for 10 epochs stop.

626
01:37:28.770 --> 01:37:35.250
Capstone Coach 1: You say and that's kind of interesting if it doesn't get smaller over 10 epochs stop.

627
01:37:37.590 --> 01:37:46.020
Capstone Coach 1: It should be getting better not getting better syndication she may want to tweak the optimizer or maybe it means that you've found the best great solution.

628
01:37:48.600 --> 01:38:07.290
Capstone Coach 1: Okay, you also have a new function at the top here it's called convert to matrix that's what it does you send it a data array and you send it what's called a window size and then it converts the data into a proper tensor.

629
01:38:08.430 --> 01:38:09.570
Capstone Coach 1: A proper tensor.

630
01:38:10.710 --> 01:38:19.950
Capstone Coach 1: Now what is windows so long winded size is how much here, we will now we're done we're talking about traditional time series forecasting.

631
01:38:20.610 --> 01:38:37.950
Capstone Coach 1: So we're looking at the stock prices with IBM, there are over 1000 stock prices for the last year or so and the window that we're talking about is how far back in that series, do you want to go in order to forecast what's coming next.

632
01:38:39.240 --> 01:38:51.930
Capstone Coach 1: You can make that small if you don't think that the going back more than, say, a week is important than maybe the window size is one or seven if you have daily data, maybe it's seven.

633
01:38:53.190 --> 01:39:03.660
Capstone Coach 1: And our case we have weekly data, so if you want to go back a month that would be four weeks, so the window size would be for.

634
01:39:04.620 --> 01:39:16.080
Capstone Coach 1: Now, in this particular example i'm going to use a window size of 30 that's probably too much, but, and we can play with that we can see what what's going to happen here.

635
01:39:16.800 --> 01:39:31.950
Capstone Coach 1: Alright, so let's go ahead and go down now, and you have two models in here, you have the model that we use last time nlp and i'm going to make this true will run this.

636
01:39:33.030 --> 01:39:35.910
Capstone Coach 1: The nlp model is the otter aggressive.

637
01:39:36.930 --> 01:40:00.960
Capstone Coach 1: model we create lags for the time series data one one week lag two week like three week lag and so forth, and then we use those as inputs to the network to do our forecasting now in our data if you if you scroll down a little bit you'll see this is going to be happening.

638
01:40:02.040 --> 01:40:03.240
Capstone Coach 1: Actually scroll up.

639
01:40:04.350 --> 01:40:07.410
Capstone Coach 1: If you look at line number 66.

640
01:40:08.460 --> 01:40:17.190
Capstone Coach 1: You see that it says number of legs for i'm going to create four legs and you can see that i'm done that here X one X two X three x four.

641
01:40:18.480 --> 01:40:33.120
Capstone Coach 1: And then I put them together in one using calm stack this is row by row right so NP calm stack says take X want to make it the first column next to the second column, and so forth.

642
01:40:35.550 --> 01:40:36.090
Capstone Coach 1: spam.

643
01:40:40.050 --> 01:40:52.500
Capstone Coach 1: And so the Z, then I basically becomes like a super for column input matrix where each column is the light one X one is the first leg.

644
01:40:53.850 --> 01:40:56.040
Capstone Coach 1: Second lag and so forth, and so on.

645
01:41:01.350 --> 01:41:01.770
Capstone Coach 1: yeah.

646
01:41:03.720 --> 01:41:15.420
Capstone Coach 1: And since I have three four legs rather there will be at least one missing value in the first four observations right.

647
01:41:16.320 --> 01:41:31.680
Capstone Coach 1: For the lag one column, only the first observation is going to be messing lag to Italy, the first two three first three like, for the first four values will be missing because we can't compute the fourth lag for OPS of those observations.

648
01:41:32.880 --> 01:41:46.290
Capstone Coach 1: That makes sense, so if I want the first lag I can't do that until I get the observation to observation, one I don't have the data, before that, so I have to set the first leg for observation one equal to missing.

649
01:41:47.940 --> 01:42:05.220
Capstone Coach 1: That Thun these functions that you see up here lags why 123 that's what they do, they all return I call them that's the same length the number of observations, but the initial values will be missing since we can't compute the legs for those guys.

650
01:42:07.170 --> 01:42:11.580
Capstone Coach 1: These four columns then are put together into a major into an array or matrix.

651
01:42:12.870 --> 01:42:38.190
Capstone Coach 1: And then I said X equal to that matrix but I chop off the first four rows that's why it says in legs colon comma colon basically says chop off the first portal four rows in and put the put the rest and X, so I am giving up the force first for observations in our time series.

652
01:42:39.780 --> 01:42:54.330
Capstone Coach 1: Now why i'm going to do the same thing there i'm going to make sure that it's the same link it has the same number of values, as the number of rosen X, so why is you know it will in lags colon so forth, like that.

653
01:42:54.990 --> 01:42:57.240
Kilani, Shadi: Dr Jones I have a quick question if you don't mind, sir.

654
01:42:57.390 --> 01:42:58.110
Capstone Coach 1: Sure sure.

655
01:42:58.200 --> 01:43:02.310
Kilani, Shadi: honey and you may have answered this one, they may have missed it, so I apologize if that's the case.

656
01:43:03.660 --> 01:43:06.390
Kilani, Shadi: For the number of legs How did we decided having four.

657
01:43:12.900 --> 01:43:16.020
Capstone Coach 1: Ideally, you would want to look at the auto correlation function.

658
01:43:18.330 --> 01:43:18.870
Capstone Coach 1: The.

659
01:43:21.030 --> 01:43:34.470
Capstone Coach 1: In your time series course i'm sure they talked about looking at the auto correlation starting with lag one two so forth, and so on, and what they would advise you is that.

660
01:43:36.450 --> 01:43:44.970
Capstone Coach 1: The first few article relations are probably going to be significantly different from zero now they might be negative, they might be positive, but they're not zero.

661
01:43:46.080 --> 01:43:58.320
Capstone Coach 1: And at some point, those auto correlation should taper off to go towards zero basically saying that if you go far enough, back in time, the observation that you see doesn't influence this one.

662
01:43:59.640 --> 01:44:07.620
Capstone Coach 1: So, like in like in the IBM stock data I don't know what the magic number is i'm using for here, because that represents one month.

663
01:44:08.250 --> 01:44:21.840
Capstone Coach 1: One month of data i'm thinking well if I go more than a month back, probably the actual number doesn't mean too much for the number, we have today, the most important thing, so the last four weeks, however.

664
01:44:23.400 --> 01:44:33.060
Capstone Coach 1: You could you could try this you could do a trial and error here okay use for a week So what if we used a What if we use six you feel free to do that.

665
01:44:33.450 --> 01:44:48.390
Capstone Coach 1: The more you're more labs you take in this case we're doing, for if I do eight you're going to chop eight weeks of data right so there's there's a little bit of a trade off here, and if you have a lot of data doesn't matter too much, but.

666
01:44:48.450 --> 01:44:50.430
Capstone Coach 1: And we do have a good deal data.

667
01:44:50.430 --> 01:45:02.280
Capstone Coach 1: Here, but so chopping off four weeks of data at the that's the oldest four weeks of data, not a big deal I probably could have gone to eight even more fell on it.

668
01:45:03.810 --> 01:45:17.700
Capstone Coach 1: Now that's going to happen is if those weeks if those legs don't matter so let's say that week six lag does not matter than the weights in the neural network for that week will be soon we will go to zero, or nearly so.

669
01:45:20.820 --> 01:45:21.780
Capstone Coach 1: Great question.

670
01:45:22.650 --> 01:45:24.990
Kilani, Shadi: Thank you that's based on diagnostics basically.

671
01:45:25.620 --> 01:45:26.130
Kilani, Shadi: matter.

672
01:45:26.520 --> 01:45:32.940
Kilani, Shadi: But I just had another question, what about seasonality effects or Is that not an issue when you're doing neural networks.

673
01:45:33.990 --> 01:45:40.740
Capstone Coach 1: All it is it so seasonality can always be an issue, and I think again, if you look at the chart you'll you'll be looking for that.

674
01:45:41.400 --> 01:45:51.090
Capstone Coach 1: By knowing what the data is a, for example in the case of IBM till they get did that then set a certain point, and the but in this in the year I don't know.

675
01:45:51.630 --> 01:45:55.290
Capstone Coach 1: about you and want to find out whether there are some cycles.

676
01:45:55.830 --> 01:46:06.120
Capstone Coach 1: They take say that utility stocks always are always cyclical and I don't know I don't know what that means, maybe, that means the first quarter is down in the fourth quarter is up I don't know something like that.

677
01:46:06.690 --> 01:46:17.640
Capstone Coach 1: So if you do have seasonality seasonality, then you would want to do the same thing that you ordinarily do in a time series which is use different saying, for example.

678
01:46:18.360 --> 01:46:25.620
Capstone Coach 1: If you know that there's a monthly cycle, or you might want to take a four week differences.

679
01:46:26.370 --> 01:46:41.520
Capstone Coach 1: You might want to determine the data for a smooth curve to it and look at differences from that all sorts of different things that you can do to see it to normalize the data, but that that's why you have an entire course on this subject.

680
01:46:43.080 --> 01:46:52.830
Capstone Coach 1: If you're doing this approach, though, however it's largely non parametric can you just throw everything in there and try it and see once which one comes to give you the best results.

681
01:46:56.400 --> 01:46:56.940
Kilani, Shadi: Thank you, Sir.

682
01:46:57.240 --> 01:46:58.710
Capstone Coach 1: Good good question yeah.

683
01:47:00.990 --> 01:47:01.620
Capstone Coach 1: alright.

684
01:47:02.850 --> 01:47:03.450
Capstone Coach 1: So.

685
01:47:04.890 --> 01:47:16.710
Capstone Coach 1: Now we did this one last time at this nlp solution, and in fact that was your week 12 assignment was to use this code that you see here for nlp.

686
01:47:17.520 --> 01:47:33.690
Capstone Coach 1: think it was almost identical to this and then to try different things like adding the regularization are taken out of regularization how about changing the optimizer here that's a good one, usually you'll get big differences there.

687
01:47:35.040 --> 01:47:40.140
Capstone Coach 1: Those sort of things, maybe changing the number of perceptions that are in the network, here we have 20.

688
01:47:41.160 --> 01:47:54.090
Capstone Coach 1: In one more hidden layer midweek little more or less you know the you'll get a different solution or maybe better maybe not so here we'll 550 box and four wow okay hit run.

689
01:47:55.410 --> 01:48:00.300
Capstone Coach 1: And let's see what I get here, this should be quick, I think this is a quick one because.

690
01:48:01.980 --> 01:48:02.160
Okay.

691
01:48:04.140 --> 01:48:04.770
Capstone Coach 1: well.

692
01:48:06.900 --> 01:48:08.460
Karen Fireman: that's good question it's Karen.

693
01:48:08.760 --> 01:48:10.650
Capstone Coach 1: Something something didn't go right here.

694
01:48:12.090 --> 01:48:14.370
Capstone Coach 1: let's see hang on So this is the other one still run.

695
01:48:14.580 --> 01:48:17.970
Capstone Coach 1: yeah that's running okay Karen go ahead.

696
01:48:19.470 --> 01:48:29.280
Karen Fireman: If you're in the middle of trying to fix this but I was gonna say since we're talking about what we were doing added another layer in line and it needed flatline the projection um I didn't know why.

697
01:48:29.880 --> 01:48:34.500
Karen Fireman: I wondered if there was some obvious reason that would have happened, I mean there's one of my things I tried I.

698
01:48:34.530 --> 01:48:37.020
Capstone Coach 1: tried, what do you are you referring to this program.

699
01:48:37.440 --> 01:48:41.880
Karen Fireman: No, no, the one from from the homework which, which turned out payments this program but yeah.

700
01:48:42.000 --> 01:48:54.030
Karen Fireman: yeah so something commented out and I tried to add it back in um I think it was the thought it was like another layer, I think it was layer two, and when I added it back in and commented that will.

701
01:48:55.260 --> 01:48:56.790
Swanson J, Charles: change the name of the layer

702
01:48:57.570 --> 01:48:58.620
Karen Fireman: No did I need to.

703
01:48:58.950 --> 01:48:59.580
Capstone Coach 1: Oh yeah.

704
01:49:00.300 --> 01:49:02.340
Karen Fireman: Oh gotcha Okay, thank you.

705
01:49:02.940 --> 01:49:03.390
Capstone Coach 1: yeah that's.

706
01:49:03.720 --> 01:49:05.010
Capstone Coach 1: that's one of those no nose.

707
01:49:05.970 --> 01:49:16.260
Capstone Coach 1: So the the charisse uses the name of that you see like a hidden one and output it when you get and that's okay now when you hit summary right here.

708
01:49:16.890 --> 01:49:28.230
Capstone Coach 1: It uses those names to create that that fancy table that describes what the network looks like and so without those names that complains and stuff and shuts down basically.

709
01:49:29.040 --> 01:49:30.570
Karen Fireman: Only did okay i'll try.

710
01:49:33.060 --> 01:49:45.660
Capstone Coach 1: But that's that's a good one i've done that so many times I you know I cut and paste a layer right put it in there, forget to change the name turn it on boom it'll it'll go South now this error that i'm getting right here.

711
01:49:46.110 --> 01:49:52.620
Capstone Coach 1: Okay, this is another one of those kind of mistakes it's looking for the data file and I didn't download the data file.

712
01:49:54.630 --> 01:49:57.870
Capstone Coach 1: You may already see where's it reading the data file.

713
01:49:59.070 --> 01:50:01.230
Swanson J, Charles: it's from last week we just come.

714
01:50:01.290 --> 01:50:10.410
Capstone Coach 1: out line number 56 you need to you need to find this file right here, IBM weekly price stocks csv.

715
01:50:12.090 --> 01:50:14.520
Capstone Coach 1: And it is from Edison week 12 I think.

716
01:50:16.470 --> 01:50:17.160
Capstone Coach 1: That file.

717
01:50:17.700 --> 01:50:25.140
Capstone Coach 1: should be in week 12 and so, if I go this is in, week 13 i'm going to back up and go to week 12 right here.

718
01:50:26.610 --> 01:50:35.100
Capstone Coach 1: And there it is right there so that's the file I needed to download you may already have it downloaded so.

719
01:50:36.690 --> 01:50:37.410
Capstone Coach 1: that's fine.

720
01:50:39.690 --> 01:50:41.430
Capstone Coach 1: And I just need to move it over.

721
01:50:43.860 --> 01:50:48.390
Capstone Coach 1: You into the area where my program is located right here.

722
01:50:50.340 --> 01:50:51.030
Capstone Coach 1: copy.

723
01:50:53.670 --> 01:50:54.390
Capstone Coach 1: and

724
01:51:00.720 --> 01:51:01.920
Capstone Coach 1: paste right in here.

725
01:51:03.330 --> 01:51:04.500
Capstone Coach 1: Week 13.

726
01:51:07.110 --> 01:51:07.500
Capstone Coach 1: We go.

727
01:51:09.030 --> 01:51:11.370
Capstone Coach 1: Now, going back.

728
01:51:13.350 --> 01:51:13.680
Capstone Coach 1: here.

729
01:51:15.000 --> 01:51:19.200
Capstone Coach 1: This time I had run it should work there, we go.

730
01:51:24.060 --> 01:51:24.630
Capstone Coach 1: and

731
01:51:26.160 --> 01:51:33.090
Capstone Coach 1: This one I turned the the output is not verbose is set to zero, so, if you look in the code.

732
01:51:35.640 --> 01:51:45.060
Capstone Coach 1: On the fit area, you see, where it says model fit, there is a verbose option I set that to zero because it's just ripping on the you know, through this.

733
01:51:45.570 --> 01:52:07.890
Capstone Coach 1: And here's what I got out from the fit the the square root of ASC is 1.00212 the red dots are the forecasts are the ansari there the predicted values and the blue is the actual and so you can see that we are there are a meter squared error is not zero.

734
01:52:09.030 --> 01:52:16.620
Capstone Coach 1: The red dots are not exactly on top of the blue, but, honestly, is this doing this actually very good fit.

735
01:52:18.480 --> 01:52:24.810
Capstone Coach 1: The msc of one says that i'm i'm forecasting within about plus or minus $2.

736
01:52:25.920 --> 01:52:30.510
Capstone Coach 1: And the sales price here is is 100.

737
01:52:34.380 --> 01:52:38.880
Capstone Coach 1: yeah 100 and some dollars and 35.

738
01:52:40.470 --> 01:52:44.130
Capstone Coach 1: actually took the log so so that.

739
01:52:45.390 --> 01:52:48.240
Capstone Coach 1: The log actually improved the forecasts, a bit.

740
01:52:50.070 --> 01:53:03.810
Capstone Coach 1: And you can see up here, where I took right here is where i'm doing the log line 72 and 73 taking the log of the X and the y values, using the non P log function.

741
01:53:05.130 --> 01:53:26.160
Capstone Coach 1: pretty easy to do so that's that's what we get if we use nlp, this is a otter aggressive model of order for, so this is a fourth order otter aggressive model if we go back further than for ar we might get a little bit of an improvement, but I, you know, maybe.

742
01:53:28.140 --> 01:53:34.710
Capstone Coach 1: If you have seasonality let's say it's a monthly thing then you probably want to go ar for ar ar.

743
01:53:36.360 --> 01:53:51.990
Capstone Coach 1: ar you know 1216 you know that kind of thing and actually bring those in as inputs, so I would have a legs 1234 and then i'd have a leg eight like 12 like 16 that kind of thing.

744
01:53:56.190 --> 01:54:02.730
Capstone Coach 1: Now the weights will be set to zero if those legs are not important and that's fine let's go to.

745
01:54:04.020 --> 01:54:12.270
Capstone Coach 1: we're still oh we're almost done it says epoch i'm on this is going back to the other console and.

746
01:54:14.100 --> 01:54:19.500
Capstone Coach 1: Here we go so we're not we're not going to get perfect fit here.

747
01:54:20.520 --> 01:54:30.990
Capstone Coach 1: The training accuracy is sitting at about 99.85%, which is good validation accuracy 99.1%.

748
01:54:32.130 --> 01:54:32.760
Capstone Coach 1: and

749
01:54:34.020 --> 01:54:42.840
Capstone Coach 1: So the accuracy on this is pretty good we're using now, this is the solution that uses convolution married with.

750
01:54:45.030 --> 01:54:45.780
Capstone Coach 1: rnn.

751
01:54:46.860 --> 01:54:59.280
Capstone Coach 1: And there, there comes out, so we have 100 misclassification is in the training data and 90 in the validation data, so it looks like we're actually doing worse than we did before.

752
01:54:59.850 --> 01:55:13.170
Capstone Coach 1: I think it was we had about 156 total now we're we're sitting at 190 what's interesting here, though, is the misclassification in the trainee bit is actually higher than it is in the validation data.

753
01:55:14.310 --> 01:55:16.320
Capstone Coach 1: That you don't see that happen every day.

754
01:55:19.710 --> 01:55:20.400
Capstone Coach 1: So.

755
01:55:22.380 --> 01:55:25.440
Capstone Coach 1: yeah plots kind of look okay.

756
01:55:26.850 --> 01:55:30.870
Capstone Coach 1: that's what happened when we added all those neurons so we have.

757
01:55:32.340 --> 01:55:38.400
Capstone Coach 1: Instead of fitting with the 200 I think it was 256 we are now sitting with what.

758
01:55:39.480 --> 01:55:40.080
Capstone Coach 1: let's say.

759
01:55:43.770 --> 01:55:44.190
Capstone Coach 1: Women.

760
01:55:46.200 --> 01:55:49.320
Capstone Coach 1: In my others oh i'm in the different program yeah.

761
01:56:04.500 --> 01:56:05.400
Swanson J, Charles: Yes, this is it.

762
01:56:06.090 --> 01:56:07.980
Capstone Coach 1: This is, this is it Okay, thank you.

763
01:56:09.150 --> 01:56:20.100
Capstone Coach 1: yeah we we jacked up to 512 and the rnn part and then an additional 100 and really it made things worse, so this is not worth it right.

764
01:56:21.600 --> 01:56:36.960
Capstone Coach 1: We did we did that some regularization and maybe that contributed to it as well, but it certainly seems that we don't need to have we're not benefiting from all those extra neurons that we're putting in there, so we probably cut back on and be happy with what we had.

765
01:56:38.340 --> 01:56:43.440
Capstone Coach 1: But you have you ever get the idea, though it's just lot of this is just trial and error.

766
01:56:44.730 --> 01:56:45.870
Capstone Coach 1: We do work it that way.

767
01:56:47.670 --> 01:56:51.840
Capstone Coach 1: Okay let's go back to the time serious now let's cure.

768
01:56:53.070 --> 01:56:57.870
Capstone Coach 1: and go back to that code well that's not it oh yeah you said.

769
01:57:00.390 --> 01:57:00.780
Capstone Coach 1: No.

770
01:57:05.130 --> 01:57:06.060
Capstone Coach 1: This is it yeah.

771
01:57:09.150 --> 01:57:29.100
Capstone Coach 1: So let's let's turn it turn the nlp model from true why wouldn't leave it is true, this thing runs pretty fast right it took point four minutes to run that let's go to rnn and see what's different on there so there's second model is the rnn model and we're going to make this true.

772
01:57:30.450 --> 01:57:31.050
Capstone Coach 1: like that.

773
01:57:32.520 --> 01:57:50.730
Capstone Coach 1: And let's see what's different well there's quite a bit going on here actually there's a lot of gymnastics involved in the front part and these gymnastics you see now, this is lions basically from the start of the rnn code to down to about line number 144.

774
01:57:51.840 --> 01:57:55.020
Capstone Coach 1: You can see there is some reshaping going on.

775
01:57:57.750 --> 01:57:58.260
Capstone Coach 1: That.

776
01:57:59.550 --> 01:58:05.670
Capstone Coach 1: Basically, is designed to get the data into the format that rnn likes to see.

777
01:58:07.110 --> 01:58:08.070
Capstone Coach 1: So.

778
01:58:10.740 --> 01:58:19.380
Capstone Coach 1: What happened let's see what happened here, so the training and the validation data they're coming in, as one long vector it was an x.

779
01:58:20.280 --> 01:58:33.870
Capstone Coach 1: What what what I did was I took I set the number Now this is somewhat arbitrary I said the, the number of validation points is 30 that's what I use 30 now that's what.

780
01:58:35.220 --> 01:58:37.350
Capstone Coach 1: A little less than six months.

781
01:58:38.760 --> 01:58:49.950
Capstone Coach 1: of data for validation and then the training will be all else, so I took the tail end of the data, basically, the last 30 weeks that are in the data.

782
01:58:50.490 --> 01:59:00.510
Capstone Coach 1: And i'm going to call that the validation data now, we could change that we could change, maybe the total number of weeks here is like a 1118 or one on one for.

783
01:59:01.200 --> 01:59:14.160
Capstone Coach 1: A little over 1000 so maybe you know 10% would be 120% will be 200 right so maybe we want to do 200 or something like that, but I took 30.

784
01:59:15.510 --> 01:59:25.350
Capstone Coach 1: And set that, as my the number of validation points i'm going to use, and I also decided to set the window size to 30 and now that's.

785
01:59:25.950 --> 01:59:42.660
Capstone Coach 1: How far back i'm willing to go and i'm going to go back to 30 weeks and use the previous 30 weeks data to forecast so next you months the next week's data, right now, this is necessary because.

786
01:59:43.680 --> 01:59:54.510
Capstone Coach 1: Already rnn remember is cycling back how long did you do, the last time well in this case it's not only the last time, the last 30 weeks.

787
01:59:55.140 --> 02:00:04.800
Capstone Coach 1: How well, did you do so, it retains that information in the in the analysis in the in the optimization and.

788
02:00:05.520 --> 02:00:17.640
Capstone Coach 1: If you don't set that that it's going to try to ring to remain retain all of the data, and if you have thousands of data points that could then cause the computer to basically really go out to lunch.

789
02:00:18.240 --> 02:00:26.010
Capstone Coach 1: So you ordinarily you set the one decides to something smaller than all of the data something more reasonable, but this is.

790
02:00:26.700 --> 02:00:44.580
Capstone Coach 1: there's no clear guidelines on how to do this, and this data data dependent so it's something that when you get this solution, you may want to tweak it a bit you might want to change the window size and make it smaller make it larger and see how much that affects the solution.

791
02:00:45.840 --> 02:01:03.690
Capstone Coach 1: The the code, you see here the first few lines it's just basically setting up the numbers to make sure they're they're in the right shape and, as I said, do you want the first column, to be the the time sequence in the second column or the columns after that to be the data.

792
02:01:04.710 --> 02:01:20.640
Capstone Coach 1: Our rnn does allow multi dimensional inputs, so you can have one target vector and then you can also have additional X values that are used to help forecast that target factor.

793
02:01:21.120 --> 02:01:30.780
Capstone Coach 1: it's still going to use the legs on the target factor to help forecast what's coming next, but if you have other data that you don't want to lag.

794
02:01:31.320 --> 02:01:40.290
Capstone Coach 1: These this would be let's say the actual numbers related to something else, like these, this data, the Dow Jones average, for example.

795
02:01:41.160 --> 02:01:51.870
Capstone Coach 1: If you're predicting stock prices it's usually a good idea to include that and maybe from yesterday, maybe today, but most likely from yesterday but.

796
02:01:52.680 --> 02:02:07.620
Capstone Coach 1: yeah So here we were setting up three layers the first layer is always the rnn layer right that's the one that you would feed that it doesn't have to be the one of the first layer

797
02:02:08.100 --> 02:02:18.990
Capstone Coach 1: But normally that that's the easiest thing to do, because you can prepare the the setup the dimensions of the data properly for all of the data coming into that aren't in layer

798
02:02:19.530 --> 02:02:31.350
Capstone Coach 1: After the rnn layer the format of the data is going to be the same, that is ordinarily used by nlp which means is basically one big vector and.

799
02:02:32.280 --> 02:02:47.520
Capstone Coach 1: You can see here that I did what I did was after the rnn layer I put in a dense layer of 10% fronts, and then the output is just one because i'm forecasting the single number, which is stock price for tomorrow for the next week.

800
02:02:48.990 --> 02:02:55.080
Capstone Coach 1: we're not doing classification here we're just forecasting a single number for next week.

801
02:02:56.280 --> 02:03:15.090
Capstone Coach 1: You can use organization, it sets a nun here and the activation on the output side is always none if you're not using classification, the only time it's not none is when you're doing binary classification or categorical classification, in which case it's going to be sigmoid or soft Max.

802
02:03:16.440 --> 02:03:18.720
Capstone Coach 1: So, setting up the output layer so it's pretty.

803
02:03:19.830 --> 02:03:27.780
Capstone Coach 1: Not many options there and you'll get some rude messages if it's if you deviate from what's expected.

804
02:03:29.070 --> 02:03:35.160
Capstone Coach 1: The intermediate layers the hidden layers, on the other hand, well, you of course you have to give them names.

805
02:03:35.760 --> 02:03:43.230
Capstone Coach 1: But the activation could be Arielle you were could be 10 H or sigmoid even if you wanted to try that.

806
02:03:44.100 --> 02:03:53.070
Capstone Coach 1: If you change the activation normally you don't get a big change in the forecast, so I reserve that for the tail end of the job and other words.

807
02:03:53.460 --> 02:04:03.150
Capstone Coach 1: And the beginning of fitting a network like this year you go after the things that make a big difference, one of them being the optimists optimizer that you using.

808
02:04:03.720 --> 02:04:12.750
Capstone Coach 1: And the other would be like the number of perceptions, that you have and whether you have an intermediate layer or just one single layer

809
02:04:14.160 --> 02:04:16.800
Capstone Coach 1: Here we have one in intermediate layer

810
02:04:19.410 --> 02:04:25.830
Capstone Coach 1: So this is a three layer network rnn first and then we have an LP after that.

811
02:04:27.060 --> 02:04:35.130
Capstone Coach 1: We have to optimize users that were being used here at them and add a delta and they also usually is an improvement over at them, but they're.

812
02:04:35.700 --> 02:04:46.950
Capstone Coach 1: very similar very close to each other at a delta is extremely sensitive to the learning rate yeah you can change this to be let's say seven or eight.

813
02:04:47.820 --> 02:05:06.210
Capstone Coach 1: And you can drive it all the way down to like point 0001 and usually at those extremes you'll see a very bad fit and instead of getting the Nice red lines lay in over the blue you'll get a solid line of red, although straight across the the diagram doesn't do well.

814
02:05:07.350 --> 02:05:11.400
Capstone Coach 1: So i'm going to turn this on and and let's see what happens here.

815
02:05:12.750 --> 02:05:17.100
Capstone Coach 1: I think already set that to true So here we go.

816
02:05:19.590 --> 02:05:35.070
Capstone Coach 1: Wait oh yeah so we're gonna i'm decided to just rerun the first nlp solution it's already done and now we're starting off with the rnn solution coming up here.

817
02:05:37.500 --> 02:05:43.500
Capstone Coach 1: And one of the things that you will see in the er and in solution is.

818
02:05:46.260 --> 02:05:47.550
Capstone Coach 1: let's see where is it.

819
02:05:58.230 --> 02:06:16.590
Capstone Coach 1: Here, this in the fifth and the model fits statements you'll see callbacks equal to early stopping monitor vow loss patients 10 okay what this says it says, for I want you to monitor the data for early stopping.

820
02:06:17.700 --> 02:06:22.440
Capstone Coach 1: And particular I want you to look at dollar loss, now that you can see in the output here.

821
02:06:23.100 --> 02:06:34.620
Capstone Coach 1: it's the value loss is one of the numbers that that appears in the output, if you have verbose equal to one or two they'll see dollar loss coming out you I could have said valley messy.

822
02:06:35.520 --> 02:06:48.420
Capstone Coach 1: However, notices same thing in this case because it's a regression problem that loss and balance messy or the same thing, so this is the main squared error or we used to call it ase and.

823
02:06:49.920 --> 02:06:52.890
Capstone Coach 1: SAS, and this would be the validation ase.

824
02:06:54.270 --> 02:06:59.310
Capstone Coach 1: Then, on the left hand side you have loss and msc this is for the training data.

825
02:07:00.720 --> 02:07:09.630
Capstone Coach 1: And so what this is saying it says, if you notice that the valve loss does not change for 10 epochs.

826
02:07:10.350 --> 02:07:17.130
Capstone Coach 1: It doesn't go down much for 10 years and I don't know what they mean by much look closer at the documentation.

827
02:07:18.060 --> 02:07:34.320
Capstone Coach 1: But i'm going to give you 10 a box and if it doesn't really go down much over those 10 a box shut down stop notice in this case the output yeah I actually made it to the park 100, which means that this condition did not occur.

828
02:07:35.550 --> 02:07:38.730
Capstone Coach 1: The validation loss apparently was was changing.

829
02:07:39.900 --> 02:07:50.610
Capstone Coach 1: Over those 10 over each set of 10, and so it didn't see that condition come up, so it traveled and continued on to epoch number 100.

830
02:07:51.720 --> 02:07:58.770
Capstone Coach 1: Since this was running pretty quickly, the whole thing ran in three tenths of a minute it doesn't much matter anyway.

831
02:08:00.330 --> 02:08:10.290
Capstone Coach 1: But this condition the early stopping condition is very helpful if you have a situation where things are not running that fast right so.

832
02:08:12.060 --> 02:08:26.190
Capstone Coach 1: Okay, there we go now let's see did this do any better than the other solution well let's see the square root of the afc is 1.00202 you see the number right here.

833
02:08:27.240 --> 02:08:44.610
Capstone Coach 1: that's the criteria that we normally use to decide who is the winner and now, if you look at the previous solution, what is the value there 1.0 or 212, so I think it's a tie.

834
02:08:46.020 --> 02:08:54.810
Capstone Coach 1: I think it's a tie, but I prefer to use the rnn solution because normally you have better protection there against.

835
02:08:55.230 --> 02:09:05.970
Capstone Coach 1: overfitting overfitting because it's using the previous result to adjust the results so it's going to make it's going to try to do well there and not over fit the data.

836
02:09:06.690 --> 02:09:18.570
Capstone Coach 1: Now, if you look at the actual red dots you can see it doesn't fall exactly on the blue lines, but I would say, I would say that this is very reasonable.

837
02:09:19.440 --> 02:09:35.430
Capstone Coach 1: forecasts that you're seeing here it's not going to high it's not going to low I don't see like a forecast down in the blue, and the one that this these big white zones, you see there I don't see any podcasts going out there now.

838
02:09:36.870 --> 02:09:45.330
Capstone Coach 1: If you look on the far right hand side you see this little end here that's the validation data right the last 30 points, right here.

839
02:09:46.440 --> 02:10:09.150
Capstone Coach 1: So maybe we should have done a little validation going back further see how well we really did in here, so let me go do that on school into the code and let's go to the value it says windows went no invalid invalidation here Okay, what the heck let's make it 300.

840
02:10:10.560 --> 02:10:16.680
Capstone Coach 1: Okay let's make a 300 the window size, so we can keep that at 38 does that's that's okay.

841
02:10:17.490 --> 02:10:27.480
Capstone Coach 1: I just saying I go back 33 oh wait a minute let's make it 60 just for the heck of it okay Now let me blow this code may blow up on the I don't know I haven't tried these.

842
02:10:27.870 --> 02:10:38.640
Capstone Coach 1: So we're going to make invalidation 300 we're going to make the one decide 60 and we're going to cross your fingers and hit go and see what happens okay there's go.

843
02:10:39.390 --> 02:10:48.300
Capstone Coach 1: Okay, so far, so good, no, no red lights coming on nothing saying Oh well, yeah oh wait a minute i'm running the other code, first the other solution.

844
02:10:48.810 --> 02:11:07.440
Capstone Coach 1: Okay, the emotional piece Russia or here goes before we wrap up boom there it is okay well, what do you think the ASC 00171 Okay, what do you think Charles are you here yeah you're here.

845
02:11:07.740 --> 02:11:08.850
Capstone Coach 1: I can always count on you.

846
02:11:08.880 --> 02:11:11.910
Swanson J, Charles: i'm here, but it kind of looks worse to me.

847
02:11:12.540 --> 02:11:17.850
Capstone Coach 1: yeah in some respects, if you look at the dots they seem to be further away from the blue line okay.

848
02:11:18.420 --> 02:11:22.890
Capstone Coach 1: yeah well, then why is the ASC the square where the sc smaller.

849
02:11:25.110 --> 02:11:28.530
Swanson J, Charles: It looks like it's exactly a lag of the.

850
02:11:30.660 --> 02:11:31.590
Swanson J, Charles: Of the POP.

851
02:11:34.200 --> 02:11:34.830
Capstone Coach 1: yeah.

852
02:11:39.180 --> 02:11:44.280
Karen Fireman: The data at the end, the validation data that's really close to that line what's the deal with that on the right.

853
02:11:45.960 --> 02:11:46.290
Capstone Coach 1: here.

854
02:11:47.010 --> 02:11:48.210
Karen Fireman: Over there let's kind of good.

855
02:11:50.580 --> 02:11:57.450
Capstone Coach 1: yeah, this is actually pretty pretty that's pretty pretty good, but I think was Charles was thinking about is right in here.

856
02:11:58.380 --> 02:12:09.900
Capstone Coach 1: But that's the silly area as a matter of fact, if you look at the forecast over this period, right here, where they're going up the almost all of the red points are below the line below the blue.

857
02:12:10.560 --> 02:12:21.630
Swanson J, Charles: yeah it seems solanas there isn't a trend and say the price is holding steady well it's predicting it because it's just taking a lag.

858
02:12:22.290 --> 02:12:23.340
Capstone Coach 1: yeah yeah.

859
02:12:23.490 --> 02:12:24.480
Karen Fireman: He said turn coven.

860
02:12:24.780 --> 02:12:26.040
Capstone Coach 1: What you know.

861
02:12:27.480 --> 02:12:31.860
Capstone Coach 1: For sure the mathematically they se is smaller.

862
02:12:33.990 --> 02:12:45.090
Capstone Coach 1: You get one one or two verses 1017 it's a little bit small little bit difference, but not of course it's not enough to to.

863
02:12:47.700 --> 02:12:49.080
Capstone Coach 1: really get excited about.

864
02:12:50.400 --> 02:13:00.030
Capstone Coach 1: I would think maybe we can make this better let's let's look at this again and think about how we could make this fit better even still more, better than this.

865
02:13:01.320 --> 02:13:01.920
Capstone Coach 1: What do you think.

866
02:13:05.220 --> 02:13:07.500
Swanson J, Charles: Any suggestions give it a try i'm.

867
02:13:10.320 --> 02:13:26.820
Karen Fireman: wondering something i'm go back to your validation at the end there, why is the actual just a straight line, as well as the mean is that a little weird for the stock price to just stay at one level just totally straight on that blue line is that does it make sense.

868
02:13:29.160 --> 02:13:33.060
Karen Fireman: Your last data points or whatever they are bunch of data points.

869
02:13:33.510 --> 02:13:38.370
Capstone Coach 1: You know, is it is, it does look a bizarre because if you look at the other.

870
02:13:38.700 --> 02:13:40.170
Karen Fireman: And not straight yeah.

871
02:13:41.250 --> 02:13:47.220
Capstone Coach 1: If you look at the other plot it's so there's there's something going on here, I think the.

872
02:13:51.030 --> 02:14:06.150
Capstone Coach 1: Think what is it is it just ignoring the end of the end of the data and forecasting is flat that's what that what that you know, I have to look into this, but it does look like for some weirdness going on here at the end.

873
02:14:07.980 --> 02:14:09.750
Capstone Coach 1: That blue line is not the data.

874
02:14:11.640 --> 02:14:13.230
Capstone Coach 1: that's what i'm thinking right.

875
02:14:13.740 --> 02:14:15.000
Karen Fireman: Now yeah it's bothering me.

876
02:14:15.660 --> 02:14:16.020
yeah.

877
02:14:17.430 --> 02:14:22.710
Capstone Coach 1: Okay, it could be the plot routine I don't know but there's something bizarre about that.

878
02:14:23.820 --> 02:14:31.980
Capstone Coach 1: And there's something bizarre about this number right here being better considering what you can like Charles was saying, I don't think so.

879
02:14:33.000 --> 02:14:33.900
Capstone Coach 1: Then look that good.

880
02:14:35.730 --> 02:14:49.080
Capstone Coach 1: But what I would probably do is say well let's instead of using a window size of 30 let's use one decide let's make it smaller than 30 We made it bigger so let's let's go to.

881
02:14:50.430 --> 02:14:55.980
Capstone Coach 1: For a 16 which would be for you know, four months right.

882
02:14:57.450 --> 02:15:10.350
Capstone Coach 1: For this, for 16 four months of data and let's try it again and see what happens I think if you make the window too big, then it doesn't respond to the ups and the Downs as well.

883
02:15:11.490 --> 02:15:14.280
Capstone Coach 1: But that's My guess would say here.

884
02:15:21.810 --> 02:15:22.620
Capstone Coach 1: Okay.

885
02:15:24.120 --> 02:15:40.860
Capstone Coach 1: Well that's interesting Okay, so I made you all we did there was make the window from 60 down to 16 very small and so now the red curve is more as smooth right it looks like it's more of a smooth solution to the forecast problem.

886
02:15:43.470 --> 02:15:45.930
Capstone Coach 1: But actually for.

887
02:15:47.190 --> 02:15:51.750
Capstone Coach 1: This is not a bad thing, because many times those ups and downs are just.

888
02:15:52.110 --> 02:16:04.590
Capstone Coach 1: really big daily swings right, so this might actually be a reasonable, if you look at the the average squared error it's pretty close to the other values, a little bit higher.

889
02:16:06.180 --> 02:16:06.720
Capstone Coach 1: So.

890
02:16:08.100 --> 02:16:10.710
Capstone Coach 1: Obviously the if you change the window size.

891
02:16:12.150 --> 02:16:22.350
Capstone Coach 1: let's see 2320 if you change the window size you're going to get quite a bit different picture of the club, the forecast and the predictive values.

892
02:16:23.400 --> 02:16:24.090
Capstone Coach 1: and

893
02:16:26.820 --> 02:16:30.210
Capstone Coach 1: I would think that smooth one actually might be more appealing to people.

894
02:16:34.290 --> 02:16:37.830
Capstone Coach 1: Because you this, these are weekly.

895
02:16:39.090 --> 02:16:41.070
Capstone Coach 1: values and it's pretty hard to.

896
02:16:42.420 --> 02:16:49.260
Capstone Coach 1: Actually guesstimate what through there Okay, this is also somewhat smooth not quite as much but.

897
02:16:50.820 --> 02:17:03.510
Capstone Coach 1: yeah it's it's not responding as well to the the the really issues like here there's this huge drop and it it tried to follow that but it didn't respond to that.

898
02:17:04.650 --> 02:17:18.030
Capstone Coach 1: As deeply as you might or might have wanted, but looking over in here where it's wiggling around and trying to decide whether to go up or down finally decided to go down this is not bad behavior I think.

899
02:17:19.260 --> 02:17:25.560
Swanson J, Charles: i'm Dr Jones when I drop the window size down to five.

900
02:17:27.000 --> 02:17:27.600
Swanson J, Charles: The.

901
02:17:28.800 --> 02:17:32.490
Swanson J, Charles: AC or the square root got really small.

902
02:17:32.700 --> 02:17:33.180
mm.

903
02:17:35.730 --> 02:17:36.030
Capstone Coach 1: hmm.

904
02:17:36.570 --> 02:17:40.350
Swanson J, Charles: It got down to 1.00123.

905
02:17:42.990 --> 02:18:01.980
Capstone Coach 1: I see okay yeah anyway so obviously the one size is going to affect things if you make it large you're going to get more of jumping around, I guess, and the smaller, it is the smoother it's going to be, and then the invalidation here.

906
02:18:03.000 --> 02:18:06.930
Capstone Coach 1: i'm not sure what's going on with that, but the end of the curve here so.

907
02:18:07.920 --> 02:18:08.340
anyway.

908
02:18:09.870 --> 02:18:23.160
Swanson J, Charles: Dr Johnson when I cut the window size down to five that straight line went away and so like when I had the window size 30 there's a straight line at the end of the hour.

909
02:18:24.240 --> 02:18:24.420
Swanson J, Charles: But.

910
02:18:24.990 --> 02:18:26.700
Capstone Coach 1: Well i'm gonna run now you got a cure.

911
02:18:26.700 --> 02:18:30.090
Karen Fireman: Think what it's I think what it's doing is it's assuming that the.

912
02:18:30.630 --> 02:18:40.950
Karen Fireman: Data would be straight out from the thing and then it's doing its predictions and comparison when you cut it down from 30 to five now you don't see it because you're on 30 of them there's only five of them.

913
02:18:41.820 --> 02:18:54.180
Karen Fireman: it's not very visible when it's still just assuming it equals the last I mean isn't that the assumption of the random walk that the next invalid next value is the same as last value that's random walk assumption.

914
02:18:54.570 --> 02:18:58.800
Capstone Coach 1: Well, this is what you see this is with a window of five I like this.

915
02:18:58.860 --> 02:19:05.640
Capstone Coach 1: Right right that's pretty cool and here you can see it actually did respond very well to that.

916
02:19:06.690 --> 02:19:12.480
Capstone Coach 1: That big drop that you see here and over here as well, that's good I like five.

917
02:19:13.530 --> 02:19:21.810
Capstone Coach 1: and going back to become it and most time series software, the forecasts beyond the data or flat.

918
02:19:23.280 --> 02:19:29.520
Capstone Coach 1: They they're usually they usually because the the assumption in a time series is that it's.

919
02:19:30.720 --> 02:19:37.770
Capstone Coach 1: heterogeneous, which means that, on the long term it's gonna it's it's just up noise around an average right.

920
02:19:38.250 --> 02:19:46.350
Capstone Coach 1: So we took time series course they said, well, one of the assumptions you're making an arena model is that the long term average the static.

921
02:19:46.890 --> 02:19:53.130
Capstone Coach 1: it's not changing it's not going up continuously are going down continuously it's you know it's it's.

922
02:19:53.700 --> 02:20:05.550
Capstone Coach 1: it's static and so that assumption means that drives this the end of the curve production predictions will be flat and that's true of all remote software.

923
02:20:05.940 --> 02:20:20.430
Capstone Coach 1: Unless you incorporate things like seasonality, or you allow the software to adjust for trends, but if you're looking at a straight a remodel the forecast off the end of the series is always just a flat static.

924
02:20:23.520 --> 02:20:26.640
Capstone Coach 1: Because that's what's required for the assumptions.

925
02:20:27.870 --> 02:20:43.770
Capstone Coach 1: However, this kind of thing it doesn't know anything about arena it's just all we're doing here is checking the data as it is and then train the computer to bed best as it can react to that data up or down.

926
02:20:48.000 --> 02:21:01.050
Capstone Coach 1: And I can, I think this is, I like this solution here, and we might might want to tweak the number of perceptions, or even the tell you what she really needed to look at is the.

927
02:21:02.190 --> 02:21:23.130
Capstone Coach 1: optimization the optimizer square Oh, here we go yeah so watch what's going to happen here's right now the way this is working is it's using a Adam Adam delta right and we've set the learning rate at waco one watch what happens if I change this to say 5.0 like that.

928
02:21:24.300 --> 02:21:24.690
Capstone Coach 1: Oh.

929
02:21:26.250 --> 02:21:38.970
Capstone Coach 1: By the way, that's that optimize your set of 5.0 in some data, this is a good solution, the Ad a delta will work better with the high number like this and the small number.

930
02:21:40.170 --> 02:22:05.040
Capstone Coach 1: And now it's kicking in bones see that not good, so the red dots are all below the green the blue lines and that's because we set the learning rate to a very high value now if I said it to a really small body let's say point 0001 like that OK, and now run it again.

931
02:22:09.450 --> 02:22:11.070
Capstone Coach 1: You see what's going to happen here.

932
02:22:16.290 --> 02:22:18.510
Capstone Coach 1: Ah, no that's no that's the other one.

933
02:22:21.330 --> 02:22:22.140
Capstone Coach 1: This is funny.

934
02:22:23.970 --> 02:22:25.110
Capstone Coach 1: Okay.

935
02:22:26.070 --> 02:22:27.450
Swanson J, Charles: it's not learning anything.

936
02:22:27.600 --> 02:22:28.800
Capstone Coach 1: This is really bad as.

937
02:22:30.060 --> 02:22:31.230
Capstone Coach 1: You can see there you go.

938
02:22:32.670 --> 02:22:35.220
Capstone Coach 1: I think we can do better than that yeah.

939
02:22:36.660 --> 02:22:37.830
Capstone Coach 1: How did this happen.

940
02:22:39.330 --> 02:22:45.870
Capstone Coach 1: You know how could you look at the ESC up here the square root of the SS huge dumber ridiculous.

941
02:22:46.350 --> 02:22:58.740
Capstone Coach 1: So if you see this stuff coming out yeah you need to switch on optimizer so say well okay so much RAD adele that I tried a big number, I tried, a small number they both crap so let's try Adam.

942
02:22:59.970 --> 02:23:09.990
Capstone Coach 1: see what happens with Adam Adam I got a point oh 1am I trade oh one with that a delta gotta be a bad.

943
02:23:11.070 --> 02:23:14.430
Capstone Coach 1: bad solution so let's see what we get here.

944
02:23:15.600 --> 02:23:18.720
Capstone Coach 1: Oh, this is a little more reasonable a lot more reasonable.

945
02:23:20.160 --> 02:23:20.730
Capstone Coach 1: There we go.

946
02:23:21.930 --> 02:23:24.570
Capstone Coach 1: Oh okay actually i'm not bad at all.

947
02:23:25.710 --> 02:23:41.670
Capstone Coach 1: 1.013 sides, one of the better better examples, and we do have some cases here we're above the blue, but then we have an equal number of more or less score below the below so that kind of added average out, I suppose that's bad okay.

948
02:23:42.720 --> 02:23:44.610
Capstone Coach 1: Well that's about it for tonight folks.

949
02:23:45.690 --> 02:24:02.280
Capstone Coach 1: we're will be ending officially a little early, but if you have questions that you want to sit here and chat with me about that's fine some of you may want to ask questions about your project or other things like where to find a beer in the city Center.

950
02:24:04.140 --> 02:24:10.770
Capstone Coach 1: I don't know I do know there's some places across the way that we've gone to other classes said con to.

951
02:24:12.300 --> 02:24:16.830
Capstone Coach 1: Would we did graduation before so okay.

952
02:24:18.180 --> 02:24:23.880
Capstone Coach 1: that's it for tonight, thank you very much, and she whoever's doing the recording you can stop the recording.

953
02:24:25.500 --> 02:24:28.140
Capstone Coach 1: This what goes on next is off the record.

954
02:24:28.680 --> 02:24:28.890
Capstone Coach 1: yeah.

955
02:24:29.250 --> 02:24:30.870
Swanson J, Charles: Thank you, Dr Jones good night.

956
02:24:31.170 --> 02:24:31.770
Capstone Coach 1: good night, no.

957
02:24:32.160 --> 02:24:33.930
Kilani, Shadi: Thank you, Dr Jones great classes.

958
02:24:34.920 --> 02:24:36.870
Karen Fireman: yeah it is great, I would like to save.

959
02:24:37.950 --> 02:24:38.550
Capstone Coach 1: Okay sure.

960
02:24:42.840 --> 02:24:50.490
Karen Fireman: Thank you um the thing I was going to ask you is to go back what I was starting to ask before where you said I had to put in the.

961
02:24:52.500 --> 02:24:54.960
Karen Fireman: name of the layer

962
02:24:56.040 --> 02:24:57.750
Capstone Coach 1: The layer oh you mean which.

963
02:24:58.920 --> 02:25:02.760
Karen Fireman: Is in the one that we did for homework this that we turned in, and I know what.

964
02:25:04.140 --> 02:25:08.370
Karen Fireman: I know, Charles said that I didn't name the layer but I thought the layer was already named.

